{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import itertools\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utility_functions import (calculate_model_performance,\n",
    "                               plot_ROC,\n",
    "                               one_hot_encode,\n",
    "                               split_data_as,\n",
    "                               grid_search,\n",
    "                               shuffled,\n",
    "                               timeit)\n",
    "\n",
    "EPSILON = 10e-08\n",
    "\n",
    "\n",
    "def get_shapes(any_):\n",
    "    for array in any_:\n",
    "        try:\n",
    "            print(array.shape)\n",
    "        except:\n",
    "            print(\"NONE\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# ============= ACTIVATION FUNCTIONS ===============#\n",
    "\n",
    "def sigmoid(Z, prime=False):\n",
    "    # np.\n",
    "    if prime:\n",
    "        return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "\n",
    "def linear(Z, prime=False):\n",
    "    if prime:\n",
    "        return np.ones_like(Z)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def relu(Z, alpha=0.01, prime=False):\n",
    "    if prime:\n",
    "        Z_relu = np.ones_like(Z, dtype=np.float64)\n",
    "        Z_relu[Z < 0] = alpha\n",
    "        return Z_relu\n",
    "    return np.where(Z < 0, alpha * Z, Z)\n",
    "\n",
    "\n",
    "def tanh(Z, prime=False):\n",
    "    # np.tanh() could be used directly to speed this up\n",
    "    if prime:\n",
    "        return 1 - np.power(tanh(Z), 2)\n",
    "    return (2 / (1 + np.exp(-2 * Z))) - 1\n",
    "\n",
    "\n",
    "def elu(Z, prime=False):\n",
    "    # https://mlfromscratch.com/activation-functions-explained/#/\n",
    "    alpha = 0.2\n",
    "    if prime:\n",
    "        return np.where(Z < 0, alpha * (np.exp(Z)), 1)\n",
    "    return np.where(Z < 0, alpha * (np.exp(Z) - 1), Z)\n",
    "\n",
    "\n",
    "def softmax(Z, prime=False):\n",
    "    # https://deepnotes.io/softmax-crossentropy\n",
    "    # max(Z) term is added to stabilise the function.\n",
    "    exps = np.exp(Z - np.max(Z))\n",
    "    return exps / np.sum(exps, axis=0)\n",
    "\n",
    "\n",
    "# References\n",
    "# https://mc.ai/multilayered-neural-network-from-scratch-using-python/\n",
    "# https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "# https://www.coursera.org/learn/machine-learning/home/week/5\n",
    "# https://www.coursera.org/specializations/deep-learning\n",
    "# https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py\n",
    "# https://github.com/JWarmenhoven/Coursera-Machine-Learning\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_layer: tuple,\n",
    "            hidden_layer: list,  # list of tuples\n",
    "            output_layer: int,\n",
    "            batch_size=16,\n",
    "            alpha=0.01,\n",
    "            optimizer=\"SGD\",\n",
    "            weight_initialisation = \"he_uniform\",\n",
    "            penalty=None,\n",
    "            lambd=0.01,\n",
    "            keep_prob = None,\n",
    "            epoch=500,\n",
    "            random_state=42,\n",
    "            verbose=True,\n",
    "            metrics=\"accuracy\"\n",
    "    ):\n",
    "        self.input_layer = input_layer\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.output_layer = output_layer\n",
    "        self.mini_batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.optimizer = optimizer\n",
    "        self.weight_initialisation = weight_initialisation\n",
    "        self.penalty = penalty\n",
    "        self.lambd = lambd\n",
    "        self.keep_prob = keep_prob\n",
    "        # dropout: http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf\n",
    "        self.dropout = True if isinstance(self.keep_prob, float) else False\n",
    "        self.epoch = epoch\n",
    "        self.seed = random_state\n",
    "        self.verbose = verbose\n",
    "        self.metrics = metrics\n",
    "        self.layers = len(self.weight_set_dimensions) + 1\n",
    "        self.EPSILON = 10e-10\n",
    "        self.best_score = 0\n",
    "        self.best_W = np.empty_like(range(self.layers), dtype=object) if verbose else None\n",
    "        self.best_B = np.empty_like(range(self.layers), dtype=object) if verbose else None\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        parameters = (\n",
    "            \"Input layer: {0}\\n\"\n",
    "            \"Hidden layer: {1}\\n\"\n",
    "            \"Output layer: {2}\\n\"\n",
    "            \"Batch size: {3}\\n\"\n",
    "            \"Learning rate: {4}\\n\"\n",
    "            \"Epoch: {5}\\n\"\n",
    "            \"Seed: {6}\\n\"\n",
    "            \"Verbose: {7}\\n\"\n",
    "            \"Metric: {8}\"\n",
    "        ).format(\n",
    "            self.input_layer,\n",
    "            \" - \".join(map(str, self.hidden_layer)),\n",
    "            self.output_layer,\n",
    "            self.mini_batch_size,\n",
    "            self.alpha,\n",
    "            self.epoch,\n",
    "            self.seed,\n",
    "            self.verbose,\n",
    "            self.metrics\n",
    "        )\n",
    "        return parameters\n",
    "\n",
    "    def get_A(self, X, predict=True):\n",
    "        A, _ = self.forwardpass(X, predict=predict)\n",
    "        return A\n",
    "\n",
    "    def get_Z(self, X, predict=True):\n",
    "        _, Z = self.forwardpass(X, predict=predict)\n",
    "        return Z\n",
    "    \n",
    "    # ============== LOSS FUNCTIONS ===============#\n",
    "\n",
    "    # https://deepnotes.io/softmax-crossentropy\n",
    "\n",
    "    def calculate_error(self, Y, Y_hat):\n",
    "        # Y and Y_hat should be in the form of (no_of_classes, no_of_training_examples)\n",
    "        cost = -np.sum(Y * np.log(Y_hat + EPSILON)) / self.m\n",
    "        if self.penalty == \"l1\":\n",
    "            for layer in range(1, self.layers):\n",
    "                cost += np.sum(np.abs(self.W[layer])) * self.lambd / (2 * self.m)\n",
    "        elif self.penalty == \"l2\":\n",
    "            for layer in range(1, self.layers):\n",
    "                cost += np.sum(np.square(self.W[layer])) * self.lambd / (2 * self.m)\n",
    "        return cost\n",
    "\n",
    "\n",
    "    def display_information(self, X, Y, X_test, Y_test, epoch_no):\n",
    "        model_performance_metrics_train = calculate_model_performance(\n",
    "            np.argmax(Y, axis=0),\n",
    "            self.predict(X)\n",
    "        )\n",
    "        \n",
    "        model_performance_metrics_test = calculate_model_performance(\n",
    "            np.argmax(Y_test, axis=0),\n",
    "            self.predict(X_test)\n",
    "        )\n",
    "        print(\"train_{0}: {1} ____ test_{0}: {2}  - epoch {3}    iteration {4} - loss {5}\".format(\n",
    "            self.metrics,\n",
    "            np.round(model_performance_metrics_train[self.metrics], 5),\n",
    "            np.round(model_performance_metrics_test[self.metrics], 5),\n",
    "            epoch_no,\n",
    "            self.no_of_iterations,\n",
    "            np.round(self.calculate_error(Y,\n",
    "                            self.get_A(X)[-1]), 10)\n",
    "        ))\n",
    "\n",
    "        self.get_best_scoring_weight(model_performance_metrics_test)\n",
    "        \n",
    "    def get_best_scoring_weight(self, test_performance):\n",
    "        score = test_performance[self.metrics]\n",
    "        \n",
    "        # self.best_score is initialised with 0 upon the construction of the class\n",
    "        if score > self.best_score:\n",
    "            # start from the first element since first element is None, deepcopy\n",
    "            # throws an error\n",
    "            self.best_W[1:] = copy.deepcopy(self.W[1:])\n",
    "            self.best_B[1:] = copy.deepcopy(self.B[1:])\n",
    "            self.best_score = score\n",
    "            # print(self.best_score)\n",
    "\n",
    "    def get_dimensions_and_activations(self):\n",
    "        self.dimensions = []\n",
    "        self.activation_functions = []\n",
    "\n",
    "        self.dimensions.append(self.input_layer[0])\n",
    "        self.activation_functions.append(self.input_layer[1])\n",
    "\n",
    "        for dim, act_func in self.hidden_layer:\n",
    "            self.dimensions.append(dim)\n",
    "            self.activation_functions.append(act_func)\n",
    "\n",
    "        self.dimensions.append(self.output_layer)\n",
    "\n",
    "    @property\n",
    "    def weight_set_dimensions(self):\n",
    "        self.get_dimensions_and_activations()\n",
    "        a, b = itertools.tee(self.dimensions[::-1])\n",
    "        next(b, None)\n",
    "        weight_set_dimensions = list(zip(a, b))[::-1]\n",
    "        return weight_set_dimensions\n",
    "\n",
    "    def initialise_weights(self, layer=None):\n",
    "        self.W = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.B = np.empty_like(range(self.layers), dtype=object)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        for layer, (y, x) in zip(range(1, self.layers), self.weight_set_dimensions):\n",
    "            # x = (layer - 1) *** fan_in\n",
    "            # y = (layer)  *** fan_out\n",
    "            \n",
    "            # difference between randn and normal:\n",
    "            # https://stackoverflow.com/questions/21738383/python-difference-between-randn-and-normal\n",
    "\n",
    "            if isinstance(self.weight_initialisation, dict):\n",
    "                if self.weight_initialisation[\"method\"] == \"random_normal\":\n",
    "                    mean = self.weight_initialisation[\"mean\"]\n",
    "                    standard_deviation = self.weight_initialisation[\"standard_deviation\"]\n",
    "\n",
    "                    self.W[layer] = np.random.normal(mean, standard_deviation, size=(y, x))\n",
    "                    self.B[layer] = np.random.normal(mean, standard_deviation, size=(y, 1))\n",
    "\n",
    "                elif self.weight_initialisation[\"method\"] == \"random_uniform\":\n",
    "                    min_ = self.weight_initialisation[\"min\"]\n",
    "                    max_ = self.weight_initialisation[\"max\"]\n",
    "\n",
    "                    self.W[layer] = np.random.uniform(min_, max_, size=(y, x))\n",
    "                    self.B[layer] = np.random.uniform(min_, max_, size=(y, 1))\n",
    "\n",
    "            elif self.weight_initialisation == \"zeros\":\n",
    "                self.W[layer] = np.zeros((y, x))\n",
    "                self.B[layer] = np.zeros((y, 1))\n",
    "\n",
    "            elif self.weight_initialisation == \"ones\":\n",
    "                self.W[layer] = np.ones((y, x))\n",
    "                self.B[layer] = np.ones((y, 1))\n",
    "\n",
    "            elif self.weight_initialisation == \"he_normal\":\n",
    "                # https://arxiv.org/abs/1502.01852\n",
    "                # https://keras.rstudio.com/reference/initializer_he_normal.html\n",
    "                self.W[layer] = np.random.normal(\n",
    "                    loc=0,\n",
    "                    scale=np.sqrt(2 / x),\n",
    "                    size=(y, x)\n",
    "                )\n",
    "                self.B[layer] = np.zeros((y, 1))\n",
    "                \n",
    "            elif self.weight_initialisation == \"he_uniform\":\n",
    "                # https://docs.chainer.org/en/stable/reference/generated/chainer.initializers.HeUniform.html\n",
    "                # https://keras.rstudio.com/reference/initializer_he_uniform.html\n",
    "                limit = np.sqrt(6 / x)\n",
    "\n",
    "                self.W[layer] = np.random.uniform(\n",
    "                    low=-limit,\n",
    "                    high=limit,\n",
    "                    size=(y, x)\n",
    "                )\n",
    "                self.B[layer] = np.zeros((y, 1))\n",
    "            \n",
    "            elif self.weight_initialisation == \"xavier_normal\":\n",
    "                # Glorot normal initializer, also called Xavier normal initializer.\n",
    "                # https://keras.rstudio.com/reference/initializer_glorot_normal.html\n",
    "                self.W[layer] = np.random.normal(\n",
    "                    loc=0,\n",
    "                    scale=np.sqrt(2 / (x + y)),\n",
    "                    size=(y, x)\n",
    "                )\n",
    "                self.B[layer] = np.zeros((y, 1))\n",
    "                \n",
    "            elif self.weight_initialisation == \"xavier_uniform\":\n",
    "                # https://keras.rstudio.com/reference/initializer_glorot_uniform.html\n",
    "                limit = np.sqrt(6 / (x + y))\n",
    "\n",
    "                self.W[layer] = np.random.uniform(\n",
    "                    low=-limit,\n",
    "                    high=limit,\n",
    "                    size=(y, x)\n",
    "                )                \n",
    "                self.B[layer] = np.zeros((y, 1))\n",
    "\n",
    "    def forwardpass(self, X, predict=False):\n",
    "        Z = np.empty_like(range(self.layers), dtype=object)\n",
    "        A = np.empty_like(range(self.layers), dtype=object)\n",
    "        A[0] = X\n",
    "\n",
    "        for layer in range(1, self.layers):\n",
    "            # activation_function starts from 0 whereas layer starts from 1\n",
    "            active_function = self.activation_functions[layer - 1]\n",
    "            arg_to_pass_to_eval = \"(Z[layer])\"\n",
    "\n",
    "            Z[layer] = self.W[layer] @ A[layer - 1] + self.B[layer]\n",
    "            A[layer] = eval(active_function + arg_to_pass_to_eval)\n",
    "            \n",
    "            # dropout is only applied to first hidden layer\n",
    "            # https://www.kaggle.com/mtax687/dropout-regularization-of-neural-net-using-numpy\n",
    "            if self.dropout and layer == 1 and not predict:\n",
    "                self.D = np.random.randn(A[layer].shape[0], A[layer].shape[1])\n",
    "                self.D = (self.D < self.keep_prob)\n",
    "                A[layer] = np.multiply(A[layer], self.D) / self.keep_prob\n",
    "\n",
    "        return A, Z\n",
    "\n",
    "    def backpropagation(self, Y, A, Z):\n",
    "        self.delta = np.empty_like(range(self.layers), dtype=object)\n",
    "\n",
    "        self.gradient_W = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.gradient_B = np.empty_like(range(self.layers), dtype=object)\n",
    "\n",
    "        self.delta[-1] = A[-1] - Y\n",
    "\n",
    "        # We substract 1 here as delta_final is calculated seperately above\n",
    "        for layer in reversed(range(1, self.layers - 1)):\n",
    "            # 1 is substracted from layer as activation_functions start indexing from 0\n",
    "            active_function = self.activation_functions[layer - 1]\n",
    "            arg_to_pass_to_eval = \"(Z[layer], prime=True)\"\n",
    "\n",
    "            DA = self.W[layer + 1].T @ self.delta[layer + 1]\n",
    "\n",
    "            # If dropout is applied\n",
    "            if self.dropout and layer == 1:\n",
    "                DA = np.multiply(DA, self.D) / self.keep_prob\n",
    "\n",
    "\n",
    "            self.delta[layer] = (\n",
    "                    DA *\n",
    "                    eval(active_function + arg_to_pass_to_eval)\n",
    "            )\n",
    "\n",
    "        for layer in range(1, self.layers):\n",
    "            self.gradient_W[layer] = (self.delta[layer] @ A[layer - 1].T) / self.m\n",
    "            self.gradient_B[layer] = np.sum(self.delta[layer], axis=1, keepdims=True) / self.m\n",
    "            \n",
    "            if self.penalty == \"l1\":\n",
    "                # https://towardsdatascience.com/only-numpy-implementing-different-combination-of-l1-norm-l2-norm-l1-regularization-and-14b01a9773b\n",
    "                self.gradient_W[layer] += np.where(self.W[layer] < 0, -1, 1) * (self.lambd / self.m)\n",
    "            elif self.penalty == \"l2\":\n",
    "                self.gradient_W[layer] += self.W[layer] * (self.lambd / self.m)\n",
    "\n",
    "        self.update_weights()\n",
    "\n",
    "    def update_weights(self):\n",
    "        if self.optimizer == \"SGD\":\n",
    "            for layer in range(1, self.layers):\n",
    "                self.W[layer] -= self.alpha * self.gradient_W[layer]\n",
    "                self.B[layer] -= self.alpha * self.gradient_B[layer]\n",
    "\n",
    "        elif self.optimizer[\"method\"] == \"SGDM\":\n",
    "            for layer in range(1, self.layers):\n",
    "                beta = self.optimizer[\"beta\"]\n",
    "                self.v_dw[layer] = beta * self.v_dw[layer] + (1 - beta) * self.gradient_W[layer]\n",
    "                self.v_db[layer] = beta * self.v_db[layer] + (1 - beta) * self.gradient_B[layer]\n",
    "\n",
    "                self.W[layer] -= self.alpha * self.v_dw[layer]\n",
    "                self.B[layer] -= self.alpha * self.v_db[layer]\n",
    "\n",
    "        elif self.optimizer[\"method\"] == \"RMSP\":\n",
    "            for layer in range(1, self.layers):\n",
    "                beta = self.optimizer[\"beta\"]\n",
    "                self.s_dw[layer] = beta * self.s_dw[layer] + (1 - beta) * np.square(self.gradient_W[layer])\n",
    "                self.s_db[layer] = beta * self.s_db[layer] + (1 - beta) * np.square(self.gradient_B[layer])\n",
    "\n",
    "                w_rms_grad = self.gradient_W[layer] / (np.sqrt(self.s_dw[layer]) + self.EPSILON)\n",
    "                b_rms_grad = self.gradient_B[layer] / (np.sqrt(self.s_db[layer]) + self.EPSILON)\n",
    "\n",
    "                self.W[layer] -= self.alpha * w_rms_grad\n",
    "                self.B[layer] -= self.alpha * b_rms_grad\n",
    "\n",
    "        elif self.optimizer[\"method\"] == \"ADAM\":\n",
    "            # EWA: Exponential weighted average\n",
    "            # ToDo: Check if bias correction is necessary. The EWA will be inaccurate initially,\n",
    "            # but it shouldn't take many iterations to compute correct EWA.\n",
    "            for layer in range(1, self.layers):\n",
    "                beta1 = self.optimizer[\"beta1\"]\n",
    "                beta2 = self.optimizer[\"beta2\"]\n",
    "                self.v_dw[layer] = beta1 * self.v_dw[layer] + (1 - beta1) * self.gradient_W[layer]\n",
    "                self.v_db[layer] = beta1 * self.v_db[layer] + (1 - beta1) * self.gradient_B[layer]\n",
    "\n",
    "                self.s_dw[layer] = beta2 * self.s_dw[layer] + (1 - beta2) * np.square(self.gradient_W[layer])\n",
    "                self.s_db[layer] = beta2 * self.s_db[layer] + (1 - beta2) * np.square(self.gradient_B[layer])\n",
    "\n",
    "                v_dw_corrected = self.v_dw[layer] / (1 - beta1 ** self.no_of_iterations)\n",
    "                s_dw_corrected = self.s_dw[layer] / (1 - beta2 ** self.no_of_iterations)\n",
    "\n",
    "                v_db_corrected = self.v_db[layer] / (1 - beta1 ** self.no_of_iterations)\n",
    "                s_db_corrected = self.s_db[layer] / (1 - beta2 ** self.no_of_iterations)\n",
    "\n",
    "                self.W[layer] -= self.alpha * (v_dw_corrected / (np.sqrt(s_dw_corrected) + self.EPSILON))\n",
    "                self.B[layer] -= self.alpha * (v_db_corrected / (np.sqrt(s_db_corrected) + self.EPSILON))\n",
    "\n",
    "\n",
    "    def initialise_cache(self):\n",
    "        self.v_dw = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.v_db = np.empty_like(range(self.layers), dtype=object)\n",
    "    \n",
    "        self.s_dw = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.s_db = np.empty_like(range(self.layers), dtype=object)\n",
    "\n",
    "        for layer, (y, x) in zip(range(1, self.layers), self.weight_set_dimensions):\n",
    "            self.v_dw[layer] = np.zeros((y, x))\n",
    "            self.v_db[layer] = np.zeros((y, 1))\n",
    "            \n",
    "            self.s_dw[layer] = np.zeros((y, x))\n",
    "            self.s_db[layer] = np.zeros((y, 1))\n",
    "\n",
    "    @timeit\n",
    "    def fit(self, X, Y, X_test, Y_test):\n",
    "        self.m = X.shape[1] # where (no_of_features, no_of_training_examples)\n",
    "        self.initialise_weights()\n",
    "        self.initialise_cache()\n",
    "\n",
    "        # By default the method is SGD(Stochastic Gradient Descent) if one wishes to use\n",
    "        # the whole batch, simply pass the number of traning examples available as the\n",
    "        # batch size when instantiating the class\n",
    "        self.no_of_iterations = 0\n",
    "        shuffled = np.arange(self.m)\n",
    "        if self.verbose:\n",
    "            print(\"Initialising weights...\")\n",
    "            print(\"Starting the training...\")\n",
    "            print(\"Initial cost: %.10f\\n\" % self.calculate_error(Y, self.get_A(X)[-1]))\n",
    "        for epoch_no in range(1, self.epoch + 1):\n",
    "            np.random.shuffle(shuffled)\n",
    "            X_shuffled = X[:, shuffled]\n",
    "            Y_shuffled = Y[:, shuffled]\n",
    "            for i in range(0, self.m, self.mini_batch_size):\n",
    "                self.no_of_iterations += 1\n",
    "                X_mini_batch = X_shuffled[:, i: i + self.mini_batch_size]\n",
    "                Y_mini_batch = Y_shuffled[:, i: i + self.mini_batch_size]\n",
    "\n",
    "                A, Z = self.forwardpass(X_mini_batch)\n",
    "                self.backpropagation(Y_mini_batch, A, Z)\n",
    "                if self.no_of_iterations % 100 == 0 and self.verbose:\n",
    "                    self.display_information(X, Y, X_test, Y_test, epoch_no)\n",
    "\n",
    "    def predict(\n",
    "            self,\n",
    "            X: np.ndarray,\n",
    "            apply_dropout=False,\n",
    "            return_prob_matrix=False,\n",
    "    ):\n",
    "        \"\"\"Predict the output given the training data.\n",
    "\n",
    "            Returns the predicted values in two forms:\n",
    "\n",
    "            1.either by picking up the highest value along the columns for every row,\n",
    "                i.e. \"np.argmax(self.A[-1].T, axis=1)\"\n",
    "            2.or by returning a matrix that is in the shape of Y.T where each column\n",
    "                represents the probability of the instance belonging to that class.\n",
    "                Please note that every column in Y.T represents a class. To be able to\n",
    "                return the probability matrix, the final activation function must be\n",
    "                softmax!\n",
    "                i.e. \"array([0.9650488423, 0.0354737543, 0.0005225966])\"\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Training set in the shape of\n",
    "                (no_of_features, no_of_training examples).\n",
    "            return_prob_matrix (bool, optional): Returns the probability matrix if True.\n",
    "                Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray:\n",
    "\n",
    "            if return_prob_matrix is False, the output is in the shape of\n",
    "                (no_of_training_examples, 1)\n",
    "            if return_prob_matrix is True, the output is in the shape of\n",
    "                (no_of_training_examples, no_of_features)\n",
    "        \"\"\"\n",
    "        # here predict means if forwardpass is called from predict method\n",
    "        # if so, there is an option to apply or not apply dropout\n",
    "        predict = np.invert(apply_dropout)\n",
    "        A, Z = self.forwardpass(X, predict=predict)\n",
    "        if return_prob_matrix:\n",
    "            np.set_printoptions(precision=10, suppress=True)\n",
    "            return A[-1].T\n",
    "        return np.argmax(A[-1].T, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "x = data.data[:,[0,2]]\n",
    "y = data.target\n",
    "\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.layers import Dropout\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model_keras = Sequential()\n",
    "model_keras.add(Dense(4, input_dim=2, activation='relu'))\n",
    "model_keras.add(Dropout(0.5))\n",
    "model_keras.add(Dense(4, activation='relu'))\n",
    "model_keras.add(Dense(3, activation='softmax'))\n",
    "adam = optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model_keras.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model_keras.fit(X.T,Y.T, batch_size=10, epochs=100)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with benchmark datasets\n",
    "\n",
    "## 1.Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the dataset as test and train...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "x = data.data[:,[0,2]]\n",
    "y = data.target\n",
    "\n",
    "dataset_train, dataset_test = split_data_as(x, y, train=0.8, test=0.2)\n",
    "\n",
    "X_train = dataset_train[:, :-1].T\n",
    "Y_train = one_hot_encode(dataset_train[:, -1]).T\n",
    "\n",
    "X_test = dataset_test[:, :-1].T\n",
    "Y_test = one_hot_encode(dataset_test[:, -1]).T\n",
    "\n",
    "\n",
    "# X = x.T\n",
    "# Y = one_hot_encode(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 30)\n",
      "(3, 30)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.0962094968\n",
      "\n",
      "train_accuracy: 73.33333 ____ test_accuracy: 75.0  - epoch 34    iteration 100 - loss 0.4243553343\n",
      "train_accuracy: 96.66667 ____ test_accuracy: 93.33333  - epoch 67    iteration 200 - loss 0.1436908921\n",
      "train_accuracy: 96.66667 ____ test_accuracy: 92.5  - epoch 100    iteration 300 - loss 0.1047364436\n",
      "func:'fit' -- took: 0.1407 sec\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(2, 'relu'),\n",
    "    hidden_layer=[(4,'relu'),(4,'softmax')],\n",
    "    output_layer=3,\n",
    "    batch_size=10,\n",
    "    optimizer={\"method\": \"ADAM\", \"beta1\": 0.9, \"beta2\": 0.999},\n",
    "    weight_initialisation = \"xavier_normal\",\n",
    "    keep_prob=1,\n",
    "    penalty=None,\n",
    "    epoch=100,\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "model.fit(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F1': 89.15662650337757,\n",
       " 'accuracy': 92.49999999922916,\n",
       " 'false_positive_rate': 8.641975308535283,\n",
       " 'precision': 84.09090908899795,\n",
       " 'prevalence': 32.49999999972917,\n",
       " 'sensitivity/recall': 94.87179486936228,\n",
       " 'specificity': 91.35802469023014}"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_model_performance(np.argmax(Y_test, axis=0),\n",
    "                           model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results_dict_all_models, results_average_dict, models = grid_search(\n",
    "#     x,\n",
    "#     y,\n",
    "#     clf=NeuralNetwork,\n",
    "#     lst_metrics=[\"F1\", \"accuracy\"],\n",
    "#     sort_by = \"accuracy\",\n",
    "#     n_folds=5,\n",
    "#     dict_param_grid={\n",
    "#         'batch_size': [8, 16, 32],\n",
    "#         'input_layer': [(2, 'relu')],\n",
    "#         'hidden_layer': [\n",
    "#             [(4,'relu'), (4,'softmax')],\n",
    "#             [(4,'sigmoid'),(4,'softmax')]\n",
    "#         ],\n",
    "#         'optimizer': [\n",
    "#             {\n",
    "#                 \"method\": \"RMSP\",\n",
    "#                 \"beta\": 0.9\n",
    "#             }\n",
    "#         ],\n",
    "#         'output_layer': [3],\n",
    "#         'alpha': [0.001],\n",
    "#         'verbose': [False],\n",
    "#         'epoch': [1000]\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\ma\\core.py:6461: MaskedArrayFutureWarning: In the future the default for ma.maximum.reduce will be axis=0, not the current None, to match np.maximum.reduce. Explicitly pass 0 or None to silence this warning.\n",
      "  return self.reduce(a)\n",
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\ma\\core.py:6461: MaskedArrayFutureWarning: In the future the default for ma.minimum.reduce will be axis=0, not the current None, to match np.minimum.reduce. Explicitly pass 0 or None to silence this warning.\n",
      "  return self.reduce(a)\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "dt = data.data[:,[0,2]]\n",
    "x_min, x_max = dt[:, 0].min() - 1, dt[:, 0].max() + 1\n",
    "y_min, y_max = dt[:, 1].min() - 1, dt[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()].T, apply_dropout=False)\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx, yy, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "\n",
    "plt.scatter(dt[:, 0], dt[:, 1], c=y,s=20, edgecolor='k')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('petal length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Make Moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "x,y =make_moons(n_samples=1500, noise=.05)\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(X.shape[0], 'relu'),\n",
    "    hidden_layer=[(10,'relu'), (4,'softmax')],\n",
    "    output_layer=Y.shape[0],\n",
    "    batch_size=8,\n",
    "    optimizer=\n",
    "    {\n",
    "        \"method\": \"ADAM\",\n",
    "        \"beta1\": 0.9,\n",
    "        \"beta2\": 0.999\n",
    "    },\n",
    "    penalty = \"l2\",\n",
    "    keep_prob=0.5,\n",
    "    lambd=0.001,\n",
    "    epoch=250,\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Decision Boundaries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "dt = x\n",
    "x_min, x_max = dt[:, 0].min() - 0.5, dt[:, 0].max() + 0.5\n",
    "y_min, y_max = dt[:, 1].min() - 0.5, dt[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()].T, apply_dropout=False) \n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx, yy, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "plt.scatter(dt[:, 0], dt[:, 1], c=y, s=20, edgecolor='k')\n",
    "plt.title('Decision Boundaries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Andrew NG Assignment 2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex2data2 = np.loadtxt(\"../ex2/data/ex2data2.txt\", delimiter=\",\")\n",
    "\n",
    "x = ex2data2[:, :-1]\n",
    "y = ex2data2[:, -1]\n",
    "\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 118)\n",
      "(2, 118)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(X.shape[0], 'relu'),\n",
    "    hidden_layer=[(8,'relu'), (4,'softmax')],\n",
    "    output_layer=Y.shape[0],\n",
    "    batch_size=8,\n",
    "    optimizer={\"method\": \"ADAM\", \"beta1\": 0.9, \"beta2\": 0.999},\n",
    "    keep_prob=0.8,\n",
    "    epoch=1500,\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results_dict_all_models, results_average_dict, models = grid_search_stratified(\n",
    "#     x,\n",
    "#     y,\n",
    "#     clf=NeuralNetwork,\n",
    "#     metrics=[\"F1\", \"accuracy\"],\n",
    "#     sort_by = \"accuracy\",\n",
    "#     n_fold=6,\n",
    "#     param_grid_dict={\n",
    "#         'batch_size': [16, 32],\n",
    "#         'input_layer': [(2, 'relu')],\n",
    "#         'hidden_layer': [\n",
    "#             [(4,'relu'), (4,'relu'), (4,'softmax')],\n",
    "#             [(4,'sigmoid'),(4,'softmax')]\n",
    "#         ],\n",
    "#         'output_layer': [2],\n",
    "#         'alpha': [2, 4],\n",
    "#         'verbose': [False],\n",
    "#         'epoch': [5000]\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_average_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(models[\"model_5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Decision Boundaries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "data = ex2data2\n",
    "\n",
    "x1_min, x1_max = data[:, 0].min() - 0.3, data[:, 0].max() + 0.3,\n",
    "x2_min, x2_max = data[:, 1].min() - 0.3, data[:, 1].max() + 0.3,\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx1.ravel(), xx2.ravel()].T, apply_dropout=False) \n",
    "\n",
    "negatives = ex2data2[ex2data2[:, -1] == 0]\n",
    "positives = ex2data2[ex2data2[:, -1] == 1]\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx1.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx1, xx2, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "plt.scatter(negatives[:, 0], negatives[:, 1],s=50, color='k')\n",
    "plt.scatter(positives[:, 0], positives[:, 1],s=50, color='r')\n",
    "plt.title('Decision Boundaries')\n",
    "\n",
    "plt.contour(xx1, xx2, Z, [0.5], linewidths=2, colors=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X', '__header__', '__version__', '__globals__', 'y'])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "data = loadmat('../ex3/data/ex3data1.mat')\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = data[\"X\"]\n",
    "y = data[\"y\"]\n",
    "y[y==10] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the dataset as test, validation and train...\n"
     ]
    }
   ],
   "source": [
    "dataset_test, dataset_validation, dataset_train = split_data_as(x, y, train=0.8, test=0.1, validation=0.1)\n",
    "X_train = dataset_train[:, :-1].T\n",
    "Y_train = one_hot_encode(dataset_train[:, -1]).T\n",
    "\n",
    "X_test = dataset_test[:, :-1].T\n",
    "Y_test = one_hot_encode(dataset_test[:, -1]).T\n",
    "\n",
    "X_validation = dataset_validation[:, :-1].T\n",
    "Y_validation = one_hot_encode(dataset_validation[:, -1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "sample = np.random.choice(data[\"X\"].shape[0], 20)\n",
    "ax.imshow(data[\"X\"][sample,1:].reshape(-1,20).T)\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.3007645699\n",
      "\n",
      "train_accuracy: 55.325 ____ test_accuracy: 53.2  - epoch 2    iteration 100 - loss 1.4388698328\n",
      "train_accuracy: 78.125 ____ test_accuracy: 72.8  - epoch 3    iteration 200 - loss 0.7803600256\n",
      "train_accuracy: 86.2 ____ test_accuracy: 80.6  - epoch 4    iteration 300 - loss 0.4899953164\n",
      "train_accuracy: 88.425 ____ test_accuracy: 83.8  - epoch 5    iteration 400 - loss 0.410429103\n",
      "train_accuracy: 93.4 ____ test_accuracy: 89.0  - epoch 7    iteration 500 - loss 0.2715323505\n",
      "train_accuracy: 94.95 ____ test_accuracy: 88.4  - epoch 8    iteration 600 - loss 0.1948008794\n",
      "train_accuracy: 95.275 ____ test_accuracy: 89.2  - epoch 9    iteration 700 - loss 0.2017770758\n",
      "train_accuracy: 96.45 ____ test_accuracy: 90.6  - epoch 10    iteration 800 - loss 0.1300057845\n",
      "train_accuracy: 96.875 ____ test_accuracy: 91.6  - epoch 12    iteration 900 - loss 0.111093967\n",
      "train_accuracy: 97.55 ____ test_accuracy: 91.6  - epoch 13    iteration 1000 - loss 0.0981078696\n",
      "train_accuracy: 98.225 ____ test_accuracy: 90.8  - epoch 14    iteration 1100 - loss 0.0682947004\n",
      "train_accuracy: 97.25 ____ test_accuracy: 89.0  - epoch 15    iteration 1200 - loss 0.1017462831\n",
      "train_accuracy: 98.275 ____ test_accuracy: 91.8  - epoch 17    iteration 1300 - loss 0.0628365898\n",
      "train_accuracy: 98.125 ____ test_accuracy: 91.2  - epoch 18    iteration 1400 - loss 0.0740074703\n",
      "train_accuracy: 98.4 ____ test_accuracy: 92.2  - epoch 19    iteration 1500 - loss 0.063613689\n",
      "train_accuracy: 99.05 ____ test_accuracy: 91.4  - epoch 20    iteration 1600 - loss 0.0387617001\n",
      "train_accuracy: 97.475 ____ test_accuracy: 89.2  - epoch 22    iteration 1700 - loss 0.1529869164\n",
      "train_accuracy: 96.675 ____ test_accuracy: 89.2  - epoch 23    iteration 1800 - loss 0.1106186267\n",
      "train_accuracy: 98.875 ____ test_accuracy: 91.6  - epoch 24    iteration 1900 - loss 0.0427005801\n",
      "train_accuracy: 98.925 ____ test_accuracy: 92.2  - epoch 25    iteration 2000 - loss 0.0333145099\n",
      "train_accuracy: 99.375 ____ test_accuracy: 93.4  - epoch 27    iteration 2100 - loss 0.0196278837\n",
      "train_accuracy: 96.975 ____ test_accuracy: 89.2  - epoch 28    iteration 2200 - loss 0.1123703941\n",
      "train_accuracy: 97.95 ____ test_accuracy: 90.6  - epoch 29    iteration 2300 - loss 0.0851064088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:72: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy: 71.075 ____ test_accuracy: 90.0  - epoch 30    iteration 2400 - loss nan\n",
      "train_accuracy: 99.125 ____ test_accuracy: 91.8  - epoch 32    iteration 2500 - loss 0.0328457264\n",
      "train_accuracy: 99.625 ____ test_accuracy: 92.6  - epoch 33    iteration 2600 - loss 0.0137026421\n",
      "train_accuracy: 99.675 ____ test_accuracy: 91.8  - epoch 34    iteration 2700 - loss 0.0125033163\n",
      "train_accuracy: 99.75 ____ test_accuracy: 92.4  - epoch 35    iteration 2800 - loss 0.0091953845\n",
      "train_accuracy: 99.725 ____ test_accuracy: 92.2  - epoch 37    iteration 2900 - loss 0.0064074401\n",
      "train_accuracy: 99.775 ____ test_accuracy: 92.0  - epoch 38    iteration 3000 - loss 0.0048296968\n",
      "train_accuracy: 99.825 ____ test_accuracy: 91.8  - epoch 39    iteration 3100 - loss 0.0041846097\n",
      "train_accuracy: 99.9 ____ test_accuracy: 91.6  - epoch 40    iteration 3200 - loss 0.0039059598\n",
      "train_accuracy: 99.95 ____ test_accuracy: 91.8  - epoch 42    iteration 3300 - loss 0.0031302233\n",
      "train_accuracy: 99.95 ____ test_accuracy: 91.8  - epoch 43    iteration 3400 - loss 0.0028138476\n",
      "train_accuracy: 99.95 ____ test_accuracy: 91.8  - epoch 44    iteration 3500 - loss 0.0026542049\n",
      "train_accuracy: 99.95 ____ test_accuracy: 91.8  - epoch 45    iteration 3600 - loss 0.0023892474\n",
      "train_accuracy: 99.925 ____ test_accuracy: 91.8  - epoch 47    iteration 3700 - loss 0.0025425507\n",
      "train_accuracy: 99.925 ____ test_accuracy: 91.6  - epoch 48    iteration 3800 - loss 0.0023039682\n",
      "train_accuracy: 94.775 ____ test_accuracy: 87.4  - epoch 49    iteration 3900 - loss 0.207474255\n",
      "train_accuracy: 96.475 ____ test_accuracy: 91.0  - epoch 50    iteration 4000 - loss 0.1618947793\n",
      "train_accuracy: 98.7 ____ test_accuracy: 93.0  - epoch 52    iteration 4100 - loss 0.0508552965\n",
      "train_accuracy: 98.925 ____ test_accuracy: 92.4  - epoch 53    iteration 4200 - loss 0.0384353276\n",
      "train_accuracy: 99.575 ____ test_accuracy: 93.0  - epoch 54    iteration 4300 - loss 0.0149806717\n",
      "train_accuracy: 99.775 ____ test_accuracy: 92.8  - epoch 55    iteration 4400 - loss 0.0085845504\n",
      "train_accuracy: 99.8 ____ test_accuracy: 92.8  - epoch 57    iteration 4500 - loss 0.0068101295\n",
      "train_accuracy: 99.925 ____ test_accuracy: 92.8  - epoch 58    iteration 4600 - loss 0.0036355761\n",
      "train_accuracy: 99.65 ____ test_accuracy: 91.8  - epoch 59    iteration 4700 - loss 0.018045584\n",
      "train_accuracy: 99.625 ____ test_accuracy: 92.4  - epoch 60    iteration 4800 - loss 0.0109122492\n",
      "train_accuracy: 99.875 ____ test_accuracy: 92.6  - epoch 62    iteration 4900 - loss 0.0048658319\n",
      "train_accuracy: 99.925 ____ test_accuracy: 92.4  - epoch 63    iteration 5000 - loss 0.0036821488\n",
      "train_accuracy: 98.825 ____ test_accuracy: 91.6  - epoch 64    iteration 5100 - loss 0.0611402678\n",
      "train_accuracy: 97.475 ____ test_accuracy: 90.8  - epoch 65    iteration 5200 - loss 0.1247059595\n",
      "train_accuracy: 97.95 ____ test_accuracy: 91.0  - epoch 67    iteration 5300 - loss 0.0788032333\n",
      "train_accuracy: 98.975 ____ test_accuracy: 91.2  - epoch 68    iteration 5400 - loss 0.0334437842\n",
      "train_accuracy: 99.625 ____ test_accuracy: 91.6  - epoch 69    iteration 5500 - loss 0.0159121988\n",
      "train_accuracy: 99.4 ____ test_accuracy: 91.6  - epoch 70    iteration 5600 - loss 0.0351368161\n",
      "train_accuracy: 99.75 ____ test_accuracy: 93.0  - epoch 72    iteration 5700 - loss 0.0110334405\n",
      "train_accuracy: 99.775 ____ test_accuracy: 91.8  - epoch 73    iteration 5800 - loss 0.0090117076\n",
      "train_accuracy: 99.95 ____ test_accuracy: 92.2  - epoch 74    iteration 5900 - loss 0.0029254075\n",
      "train_accuracy: 99.925 ____ test_accuracy: 92.0  - epoch 75    iteration 6000 - loss 0.0023460153\n",
      "train_accuracy: 99.95 ____ test_accuracy: 91.8  - epoch 77    iteration 6100 - loss 0.001796778\n",
      "train_accuracy: 99.95 ____ test_accuracy: 91.8  - epoch 78    iteration 6200 - loss 0.0016074534\n",
      "train_accuracy: 99.95 ____ test_accuracy: 91.8  - epoch 79    iteration 6300 - loss 0.0013789474\n",
      "train_accuracy: 99.95 ____ test_accuracy: 92.0  - epoch 80    iteration 6400 - loss 0.0015657735\n",
      "train_accuracy: 99.125 ____ test_accuracy: 90.4  - epoch 82    iteration 6500 - loss 0.038918503\n",
      "train_accuracy: 99.775 ____ test_accuracy: 90.8  - epoch 83    iteration 6600 - loss 0.0097388658\n",
      "train_accuracy: 99.475 ____ test_accuracy: 90.8  - epoch 84    iteration 6700 - loss 0.0265005014\n",
      "train_accuracy: 99.925 ____ test_accuracy: 91.6  - epoch 85    iteration 6800 - loss 0.0031663936\n",
      "train_accuracy: 99.875 ____ test_accuracy: 92.0  - epoch 87    iteration 6900 - loss 0.0071763868\n",
      "train_accuracy: 99.25 ____ test_accuracy: 92.4  - epoch 88    iteration 7000 - loss 0.045273899\n",
      "train_accuracy: 99.925 ____ test_accuracy: 93.2  - epoch 89    iteration 7100 - loss 0.0031612106\n",
      "train_accuracy: 99.95 ____ test_accuracy: 93.6  - epoch 90    iteration 7200 - loss 0.0015114726\n",
      "train_accuracy: 99.95 ____ test_accuracy: 92.8  - epoch 92    iteration 7300 - loss 0.002611049\n",
      "train_accuracy: 99.95 ____ test_accuracy: 93.0  - epoch 93    iteration 7400 - loss 0.0027538322\n",
      "train_accuracy: 99.95 ____ test_accuracy: 93.0  - epoch 94    iteration 7500 - loss 0.0014228431\n",
      "train_accuracy: 99.95 ____ test_accuracy: 93.2  - epoch 95    iteration 7600 - loss 0.0011786467\n",
      "train_accuracy: 99.95 ____ test_accuracy: 93.0  - epoch 97    iteration 7700 - loss 0.0009858271\n",
      "train_accuracy: 99.95 ____ test_accuracy: 93.0  - epoch 98    iteration 7800 - loss 0.0008660236\n",
      "train_accuracy: 99.975 ____ test_accuracy: 92.8  - epoch 99    iteration 7900 - loss 0.0008970515\n",
      "train_accuracy: 99.975 ____ test_accuracy: 92.6  - epoch 100    iteration 8000 - loss 0.0005850444\n",
      "func:'fit' -- took: 93.1897 sec\n"
     ]
    }
   ],
   "source": [
    "# http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354\n",
    "\n",
    "model = NeuralNetwork(\n",
    "    input_layer=(X_train.shape[0], 'relu'),\n",
    "    hidden_layer=[(200,'relu'),(100,'relu'), (50,'relu'), (25, 'relu'), (4,'softmax')],\n",
    "    output_layer=Y_train.shape[0],\n",
    "    batch_size=50,\n",
    "    optimizer={\"method\": \"ADAM\", \"beta1\": 0.9, \"beta2\": 0.999},\n",
    "    weight_initialisation = \"xavier_uniform\",\n",
    "    penalty=None,\n",
    "    metrics=\"accuracy\",\n",
    "    epoch=100,\n",
    "    alpha=0.01,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.39999999980921"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F1': 76.27118643890951,\n",
       " 'accuracy': 94.3999999998112,\n",
       " 'false_positive_rate': 5.947136563863553,\n",
       " 'precision': 62.49999999913194,\n",
       " 'prevalence': 9.1999999999816,\n",
       " 'sensitivity/recall': 97.8260869543951,\n",
       " 'specificity': 94.0528634359162}"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_model_performance(np.argmax(Y_test, axis=0),\n",
    "                           model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F1': 78.0141843955807,\n",
       " 'accuracy': 93.7999999998124,\n",
       " 'false_positive_rate': 6.966292134815806,\n",
       " 'precision': 63.95348837134938,\n",
       " 'prevalence': 10.999999999978,\n",
       " 'sensitivity/recall': 99.99999999818182,\n",
       " 'specificity': 93.03370786495948}"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_model_performance(np.argmax(Y_validation, axis=0),\n",
    "                           model.predict(X_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0393697009874934"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_W[1][0][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11189225771617269"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.W[1][0][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.W = model.best_W\n",
    "model.B = model.best_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F1': 79.64601769722266,\n",
       " 'accuracy': 95.39999999980921,\n",
       " 'false_positive_rate': 4.845814977962895,\n",
       " 'precision': 67.16417910347516,\n",
       " 'prevalence': 9.1999999999816,\n",
       " 'sensitivity/recall': 97.8260869543951,\n",
       " 'specificity': 95.15418502181684}"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_model_performance(np.argmax(Y_test, axis=0),\n",
    "                           model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F1': 72.8476821177773,\n",
       " 'accuracy': 91.79999999981641,\n",
       " 'false_positive_rate': 9.21348314604671,\n",
       " 'precision': 57.29166666606987,\n",
       " 'prevalence': 10.999999999978,\n",
       " 'sensitivity/recall': 99.99999999818182,\n",
       " 'specificity': 90.78651685372857}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_model_performance(np.argmax(Y_validation, axis=0),\n",
    "                           model.predict(X_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results_dict_all_models, results_average_dict, models = grid_search(\n",
    "#     x,\n",
    "#     y,\n",
    "#     clf=NeuralNetwork,\n",
    "#     lst_metrics=[\"F1\", \"accuracy\"],\n",
    "#     sort_by = \"F1\",\n",
    "#     n_folds=10,\n",
    "#     dict_param_grid={\n",
    "#         'batch_size': [64, 128, 256],\n",
    "#         'input_layer': [(x.shape[1], 'relu')],\n",
    "#         'hidden_layer': [\n",
    "#             [(50, 'relu'), (25, 'relu'), (10,'softmax')],\n",
    "#             [(200, 'relu'), (100, 'relu'), (50, 'relu'), (10,'softmax')]\n",
    "#         ],\n",
    "#         'optimizer':[\n",
    "#             {\n",
    "#                 \"method\": \"ADAM\",\n",
    "#                 \"beta1\": 0.9,\n",
    "#                 \"beta2\": 0.999\n",
    "#             }\n",
    "#         ],\n",
    "#         'output_layer': [10],\n",
    "#         'alpha': [0.0001, 0.001],\n",
    "#         'verbose': [False],\n",
    "#         'epoch': [250]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# results_average_dict\n",
    "# print(models[\"model_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_miss_clasifications(model, digits_to_display):\n",
    "    count = 0\n",
    "    for index, (act, predicted) in enumerate(zip(np.argmax(Y,axis=0), model.predict(X))):\n",
    "        if act != predicted:\n",
    "            fig, ax = plt.subplots(figsize = (2,2))\n",
    "            ax.set_title(\"%s: act %s --- predicted %s\" %(index, act, predicted))\n",
    "            ax.imshow(X[:, index].reshape(-1,20).T)\n",
    "            ax.axis('off');\n",
    "            count += 1\n",
    "        if count == digits_to_display:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'display_miss_clasifications' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-2f44cc775398>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdisplay_miss_clasifications\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'display_miss_clasifications' is not defined"
     ]
    }
   ],
   "source": [
    "display_miss_clasifications(model, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = np.array([1,2,3])\n",
    "# keep = weights[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.delete(weights, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep = np.copy(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.delete(weights, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
