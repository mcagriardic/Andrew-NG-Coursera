{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utility_functions import (calculate_model_performance,\n",
    "                               plot_ROC,\n",
    "                               one_hot_encode,\n",
    "                               split_data_as,\n",
    "                               grid_search_stratified,\n",
    "                               shuffle)\n",
    "\n",
    "\n",
    "def get_shapes(any_):\n",
    "    for array in any_:\n",
    "        try:\n",
    "            print(array.shape)\n",
    "        except:\n",
    "            print(\"NONE\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# ============= ACTIVATION FUNCTIONS ===============#\n",
    "\n",
    "def sigmoid(Z, prime=False):\n",
    "    # np.\n",
    "    if prime:\n",
    "        return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "\n",
    "def linear(Z, prime=False):\n",
    "    if prime:\n",
    "        return np.ones_like(Z)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def relu(Z, alpha=0.01, prime=False):\n",
    "    if prime:\n",
    "        Z_relu = np.ones_like(Z, dtype=np.float64)\n",
    "        Z_relu[Z < 0] = alpha\n",
    "        return Z_relu\n",
    "    return np.where(Z < 0, alpha * Z, Z)\n",
    "\n",
    "\n",
    "def tanh(Z, prime=False):\n",
    "    # np.tanh() could be used directly to speed this up\n",
    "    if prime:\n",
    "        return 1 - np.power(tanh(Z), 2)\n",
    "    return (2 / (1 + np.exp(-2 * Z))) - 1\n",
    "\n",
    "\n",
    "def elu(Z, prime=False):\n",
    "    # https://mlfromscratch.com/activation-functions-explained/#/\n",
    "    alpha = 0.2\n",
    "    if prime:\n",
    "        return np.where(Z < 0, alpha * (np.exp(Z)), 1)\n",
    "    return np.where(Z < 0, alpha * (np.exp(Z) - 1), Z)\n",
    "\n",
    "\n",
    "def softmax(Z, prime=False):\n",
    "    # https://deepnotes.io/softmax-crossentropy\n",
    "    # max(Z) term is added to stabilise the function.\n",
    "    exps = np.exp(Z - np.max(Z))\n",
    "    return exps / np.sum(exps, axis=0)\n",
    "\n",
    "\n",
    "# ============== LOSS FUNCTIONS ===============#\n",
    "\n",
    "# https://deepnotes.io/softmax-crossentropy\n",
    "EPSILON = 1e-8\n",
    "\n",
    "\n",
    "def calculate_error(Y, Y_hat):\n",
    "    # Y and Y_hat should be in the form of (no_of_classes, no_of_training_examples)\n",
    "    m = Y.shape[1]\n",
    "    return -np.sum(Y * np.log(Y_hat + EPSILON)) / m\n",
    "\n",
    "\n",
    "# References\n",
    "# https://mc.ai/multilayered-neural-network-from-scratch-using-python/\n",
    "# https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "# https://www.coursera.org/learn/machine-learning/home/week/5\n",
    "# https://www.coursera.org/specializations/deep-learning\n",
    "# https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py\n",
    "# https://github.com/JWarmenhoven/Coursera-Machine-Learning\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_layer: tuple,\n",
    "            hidden_layer: list,  # list of tuples\n",
    "            output_layer: int,\n",
    "            batch_size=16,\n",
    "            alpha=1,\n",
    "            epoch=500,\n",
    "            random_state=42,\n",
    "            verbose=True,\n",
    "            metrics=\"accuracy\"\n",
    "    ):\n",
    "        self.input_layer = input_layer\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.output_layer = output_layer\n",
    "        self.mini_batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.epoch = epoch\n",
    "        self.seed = random_state\n",
    "        self.verbose = verbose\n",
    "        self.metrics = metrics\n",
    "\n",
    "        self.layers = len(self.weight_set_dimensions) + 1\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        parameters = (\n",
    "            \"Input layer: {0}\\n\"\n",
    "            \"Hidden layer: {1}\\n\"\n",
    "            \"Output layer: {2}\\n\"\n",
    "            \"Batch size: {3}\\n\"\n",
    "            \"Learning rate: {4}\\n\"\n",
    "            \"Epoch: {5}\\n\"\n",
    "            \"Seed: {6}\\n\"\n",
    "            \"Verbose: {7}\\n\"\n",
    "            \"Metric: {8}\"\n",
    "        ).format(\n",
    "            self.input_layer,\n",
    "            \" - \".join(map(str, self.hidden_layer)),\n",
    "            self.output_layer,\n",
    "            self.mini_batch_size,\n",
    "            self.alpha,\n",
    "            self.epoch,\n",
    "            self.seed,\n",
    "            self.verbose,\n",
    "            self.metrics\n",
    "        )\n",
    "        return parameters\n",
    "\n",
    "    def get_A(self, X):\n",
    "        A, _ = self.forwardpass(X)\n",
    "        return A\n",
    "\n",
    "    def get_Z(self, X):\n",
    "        _, Z = self.forwardpass(X)\n",
    "        return Z\n",
    "\n",
    "    def display_information(self, X, Y, epoch_no):\n",
    "        model_performance_metrics = calculate_model_performance(\n",
    "            np.argmax(Y, axis=0),\n",
    "            self.predict(X)\n",
    "        )\n",
    "        print(\"%s: %.10f - epoch %s    iteration %s - loss %.20f\" % (\n",
    "            self.metrics,\n",
    "            model_performance_metrics[self.metrics],\n",
    "            epoch_no,\n",
    "            self.no_of_iterations,\n",
    "            calculate_error(Y,\n",
    "                            self.get_A(X)[-1])\n",
    "        )\n",
    "              )\n",
    "\n",
    "    def get_dimensions_and_activations(self):\n",
    "        self.dimensions = []\n",
    "        self.activation_functions = []\n",
    "\n",
    "        self.dimensions.append(self.input_layer[0])\n",
    "        self.activation_functions.append(self.input_layer[1])\n",
    "\n",
    "        for dim, act_func in self.hidden_layer:\n",
    "            self.dimensions.append(dim)\n",
    "            self.activation_functions.append(act_func)\n",
    "\n",
    "        self.dimensions.append(self.output_layer)\n",
    "\n",
    "    @property\n",
    "    def weight_set_dimensions(self):\n",
    "        self.get_dimensions_and_activations()\n",
    "        a, b = itertools.tee(self.dimensions[::-1])\n",
    "        next(b, None)\n",
    "        weight_set_dimensions = list(zip(a, b))[::-1]\n",
    "        return weight_set_dimensions\n",
    "\n",
    "    def initialise_weights(self, layer=None):\n",
    "        self.W = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.B = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.W[0] = None\n",
    "        self.B[0] = None\n",
    "        for layer, (y, x) in zip(range(1, self.layers), self.weight_set_dimensions):\n",
    "            np.random.seed(self.seed)\n",
    "            self.W[layer] = np.random.rand(y, x) / np.sqrt(self.dimensions[layer - 1])\n",
    "            self.B[layer] = np.random.rand(y, 1)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        Z = np.empty_like(range(self.layers), dtype=object)\n",
    "        A = np.empty_like(range(self.layers), dtype=object)\n",
    "        A[0] = X\n",
    "        Z[0] = None\n",
    "        for layer in range(1, self.layers):\n",
    "            # activation_function starts from 0 whereas layer starts from 1\n",
    "            active_function = self.activation_functions[layer - 1]\n",
    "            arg_to_pass_to_eval = \"(Z[layer])\"\n",
    "\n",
    "            Z[layer] = self.W[layer] @ A[layer - 1] + self.B[layer]\n",
    "            A[layer] = eval(active_function + arg_to_pass_to_eval)\n",
    "        return A, Z\n",
    "\n",
    "    def backpropagation(self, Y, A, Z):\n",
    "        self.delta = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.delta[0] = None\n",
    "\n",
    "        self.gradient_W = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.gradient_B = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.gradient_W[0] = None\n",
    "        self.gradient_B[0] = None\n",
    "\n",
    "        self.delta[-1] = A[-1] - Y\n",
    "\n",
    "        # We substract 1 here as delta_final is calculated seperately above\n",
    "        for layer in reversed(range(1, self.layers - 1)):\n",
    "            # 1 is substracted from layer as activation_functions start indexing from 0\n",
    "            active_function = self.activation_functions[layer - 1]\n",
    "            arg_to_pass_to_eval = \"(Z[layer], prime=True)\"\n",
    "\n",
    "            self.delta[layer] = (\n",
    "                    self.W[layer + 1].T @ self.delta[layer + 1] *\n",
    "                    eval(active_function + arg_to_pass_to_eval)\n",
    "            )\n",
    "\n",
    "            # calculate the gradient\n",
    "\n",
    "        for layer in range(1, self.layers):\n",
    "            self.gradient_W[layer] = (self.delta[layer] @ A[layer - 1].T) / self.m\n",
    "            self.gradient_B[layer] = np.sum(self.delta[layer], axis=1, keepdims=True) / self.m\n",
    "\n",
    "        # update the weights\n",
    "        for layer in range(1, self.layers):\n",
    "            self.W[layer] -= self.alpha * self.gradient_W[layer]\n",
    "            self.B[layer] -= self.alpha * self.gradient_B[layer]\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.m = X.shape[1] # where (no_of_features, no_of_training_examples)\n",
    "        self.initialise_weights()\n",
    "\n",
    "        # By default the method is SGD(Stochastic Gradient Descent) if one wishes to use\n",
    "        # the whole batch, simply pass the number of traning examples available as the\n",
    "        # batch size when instantiating the class\n",
    "        self.no_of_iterations = 0\n",
    "        shuffled = np.arange(self.m)\n",
    "        if self.verbose:\n",
    "            print(\"Initialising weights...\")\n",
    "            print(\"Starting the training...\")\n",
    "            print(\"Initial cost: %.10f\\n\" % calculate_error(Y, self.get_A(X)[-1]))\n",
    "        for epoch_no in range(1, self.epoch + 1):\n",
    "            np.random.shuffle(shuffled)\n",
    "            X_shuffled = X[:, shuffled]\n",
    "            Y_shuffled = Y[:, shuffled]\n",
    "            for i in range(0, self.m, self.mini_batch_size):\n",
    "                self.no_of_iterations += 1\n",
    "                X_mini_batch = X_shuffled[:, i: i + self.mini_batch_size]\n",
    "                Y_mini_batch = Y_shuffled[:, i: i + self.mini_batch_size]\n",
    "\n",
    "                A, Z = self.forwardpass(X_mini_batch)\n",
    "                self.backpropagation(Y_mini_batch, A, Z)\n",
    "                if self.no_of_iterations % 5000 == 0 and self.verbose:\n",
    "                    self.display_information(X, Y, epoch_no)\n",
    "\n",
    "    def predict(\n",
    "            self,\n",
    "            X: np.ndarray,\n",
    "            return_prob_matrix=False\n",
    "    ):\n",
    "        \"\"\"Predict the output given the training data.\n",
    "\n",
    "            Returns the predicted values in two forms:\n",
    "\n",
    "            1.either by picking up the highest value along the columns for every row,\n",
    "                i.e. \"np.argmax(self.A[-1].T, axis=1)\"\n",
    "            2.or by returning a matrix that is in the shape of Y.T where each column\n",
    "                represents the probability of the instance belonging to that class.\n",
    "                Please note that every column in Y.T represents a class. To be able to\n",
    "                return the probability matrix, the final activation function must be\n",
    "                softmax!\n",
    "                i.e. \"array([0.9650488423, 0.0354737543, 0.0005225966])\"\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Training set in the shape of\n",
    "                (no_of_features, no_of_training examples).\n",
    "            return_prob_matrix (bool, optional): Returns the probability matrix if True.\n",
    "                Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray:\n",
    "\n",
    "            if return_prob_matrix is False, the output is in the shape of\n",
    "                (no_of_training_examples, 1)\n",
    "            if return_prob_matrix is True, the output is in the shape of\n",
    "                (no_of_training_examples, no_of_features)\n",
    "        \"\"\"\n",
    "        A, Z = self.forwardpass(X)\n",
    "        if return_prob_matrix:\n",
    "            np.set_printoptions(precision=10, suppress=True)\n",
    "            return A[-1].T\n",
    "        return np.argmax(A[-1].T, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with benchmark datasets\n",
    "\n",
    "## 1.Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "x = data.data[:,[0,2]]\n",
    "y = data.target\n",
    "\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T\n",
    "\n",
    "# test, train = split_data_as(x, y, train=0.8, test=0.2)\n",
    "\n",
    "# X_train = train[:, :-1]\n",
    "# Y_train = train[:, -1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 150)\n",
      "(3, 150)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "# print(\"\\n\")\n",
    "# print(X_test.shape)\n",
    "# print(Y_test.shape)\n",
    "# print(\"\\n\")\n",
    "# print(X_validation.shape)\n",
    "# print(Y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(\n",
    "    [\n",
    "        [1,2,3],\n",
    "        [4,5,6],\n",
    "        [7,8,9],\n",
    "        [11,12,13],\n",
    "        [14,15,16],\n",
    "        [17,18,19],\n",
    "        [20,21,22],\n",
    "        [23,24,25],\n",
    "        [26,27,28],\n",
    "        [29,30,31],\n",
    "        [32,33,34],\n",
    "        [35,36,37]\n",
    "    ]\n",
    ")\n",
    "\n",
    "y = np.array(\n",
    "    [\n",
    "        [1],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1],\n",
    "        [0],\n",
    "        [0],\n",
    "        [0],\n",
    "        [0],\n",
    "        [2],\n",
    "        [2],\n",
    "        [2],\n",
    "        [2]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X, y):\n",
    "    dataset = np.c_[X,y]\n",
    "    shuffled = np.arange(len(dataset))\n",
    "    np.random.shuffle(shuffled)\n",
    "    return dataset[shuffled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shuffled = np.c_[x,y]\n",
    "classes, counts = np.unique(y, return_counts=True)\n",
    "classes = np.int_(classes)\n",
    "counts_perc = counts / y.shape[0]\n",
    "\n",
    "classes_holder_dict = {}\n",
    "for class_ in classes:\n",
    "    classes_holder_dict[class_] = dataset_shuffled[dataset_shuffled[:, -1] == class_]\n",
    "\n",
    "fold_size = y.shape[0] / n_fold\n",
    "\n",
    "train_dataset = {}\n",
    "array_training = np.empty_like(classes, dtype=object)\n",
    "test_dataset = {}\n",
    "array_test = np.empty_like(classes, dtype=object)\n",
    "for fold in range(n_fold):\n",
    "    for class_ in classes:\n",
    "        grab = np.int(fold_size * counts_perc[class_])\n",
    "        array_test[class_] = classes_holder_dict[class_][fold * grab:(fold + 1) * grab ,:]\n",
    "        array_training[class_] = np.delete(\n",
    "            classes_holder_dict[class_],\n",
    "            (range(fold * grab, (fold + 1) * grab)),\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "    train_dataset[fold] = np.concatenate(array_training)\n",
    "    test_dataset[fold] = np.concatenate(array_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  1],\n",
       "       [ 4,  5,  6,  1],\n",
       "       [ 7,  8,  9,  1],\n",
       "       [11, 12, 13,  1],\n",
       "       [14, 15, 16,  0],\n",
       "       [17, 18, 19,  0],\n",
       "       [20, 21, 22,  0],\n",
       "       [23, 24, 25,  0],\n",
       "       [26, 27, 28,  2],\n",
       "       [29, 30, 31,  2],\n",
       "       [32, 33, 34,  2],\n",
       "       [35, 36, 37,  2]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14, 15, 16,  0],\n",
       "       [17, 18, 19,  0],\n",
       "       [23, 24, 25,  0],\n",
       "       [ 1,  2,  3,  1],\n",
       "       [ 4,  5,  6,  1],\n",
       "       [11, 12, 13,  1],\n",
       "       [26, 27, 28,  2],\n",
       "       [29, 30, 31,  2],\n",
       "       [35, 36, 37,  2]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20, 21, 22,  0],\n",
       "       [ 7,  8,  9,  1],\n",
       "       [32, 33, 34,  2]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [4.9 1.4 0. ]\n",
      "1 [5.  1.2 0. ]\n",
      "2 [4.6 1.4 0. ]\n",
      "3 [5.1 1.5 0. ]\n",
      "4 [5.  1.6 0. ]\n",
      "5 [5.  1.4 0. ]\n",
      "6 [4.6 1.5 0. ]\n",
      "7 [5.  1.6 0. ]\n",
      "8 [5.7 1.7 0. ]\n",
      "9 [4.6 1.  0. ]\n",
      "10 [4.9 1.4 0. ]\n",
      "11 [5.  1.4 0. ]\n",
      "12 [5.1 1.9 0. ]\n",
      "13 [4.6 1.4 0. ]\n",
      "14 [5.4 1.3 0. ]\n",
      "15 [5.1 1.7 0. ]\n",
      "16 [5.2 1.4 0. ]\n",
      "17 [5.4 1.7 0. ]\n",
      "18 [5.3 1.5 0. ]\n",
      "19 [5.1 1.5 0. ]\n",
      "20 [5.5 1.4 0. ]\n",
      "21 [4.8 1.9 0. ]\n",
      "22 [5.  1.3 0. ]\n",
      "23 [4.3 1.1 0. ]\n",
      "24 [4.8 1.4 0. ]\n",
      "25 [5.6 4.2 1. ]\n",
      "26 [6.2 4.5 1. ]\n",
      "27 [5.5 3.7 1. ]\n",
      "28 [5.7 4.2 1. ]\n",
      "29 [5.5 4.4 1. ]\n",
      "30 [6.5 4.6 1. ]\n",
      "31 [7.  4.7 1. ]\n",
      "32 [6.7 4.4 1. ]\n",
      "33 [5.6 4.5 1. ]\n",
      "34 [5.7 4.5 1. ]\n",
      "35 [5.4 4.5 1. ]\n",
      "36 [6.7 4.7 1. ]\n",
      "37 [5.5 4.  1. ]\n",
      "38 [5.9 4.8 1. ]\n",
      "39 [5.7 4.1 1. ]\n",
      "40 [5.9 4.2 1. ]\n",
      "41 [6.  5.1 1. ]\n",
      "42 [5.7 3.5 1. ]\n",
      "43 [6.3 4.9 1. ]\n",
      "44 [4.9 3.3 1. ]\n",
      "45 [6.  4.5 1. ]\n",
      "46 [6.7 5.  1. ]\n",
      "47 [5.5 4.  1. ]\n",
      "48 [5.2 3.9 1. ]\n",
      "49 [5.8 4.1 1. ]\n",
      "50 [6.8 5.5 2. ]\n",
      "51 [4.9 4.5 2. ]\n",
      "52 [6.5 5.8 2. ]\n",
      "53 [6.3 5.6 2. ]\n",
      "54 [7.7 6.7 2. ]\n",
      "55 [6.9 5.1 2. ]\n",
      "56 [6.5 5.5 2. ]\n",
      "57 [7.7 6.9 2. ]\n",
      "58 [6.7 5.6 2. ]\n",
      "59 [6.9 5.7 2. ]\n",
      "60 [6.4 5.3 2. ]\n",
      "61 [6.7 5.7 2. ]\n",
      "62 [6.  4.8 2. ]\n",
      "63 [6.7 5.7 2. ]\n",
      "64 [6.5 5.1 2. ]\n",
      "65 [7.2 6.1 2. ]\n",
      "66 [7.2 5.8 2. ]\n",
      "67 [6.1 5.6 2. ]\n",
      "68 [6.3 6.  2. ]\n",
      "69 [6.3 5.6 2. ]\n",
      "70 [5.8 5.1 2. ]\n",
      "71 [7.2 6.  2. ]\n",
      "72 [6.1 4.9 2. ]\n",
      "73 [7.7 6.7 2. ]\n",
      "74 [6.4 5.5 2. ]\n",
      "NEXT--\n",
      "0 [5.2 1.5 0. ]\n",
      "1 [4.9 1.5 0. ]\n",
      "2 [4.8 1.4 0. ]\n",
      "3 [5.5 1.3 0. ]\n",
      "4 [4.5 1.3 0. ]\n",
      "5 [5.1 1.4 0. ]\n",
      "6 [5.1 1.6 0. ]\n",
      "7 [4.4 1.3 0. ]\n",
      "8 [5.4 1.7 0. ]\n",
      "9 [4.9 1.5 0. ]\n",
      "10 [5.4 1.5 0. ]\n",
      "11 [5.7 1.5 0. ]\n",
      "12 [4.8 1.6 0. ]\n",
      "13 [5.  1.6 0. ]\n",
      "14 [4.7 1.6 0. ]\n",
      "15 [5.1 1.4 0. ]\n",
      "16 [4.7 1.3 0. ]\n",
      "17 [5.8 1.2 0. ]\n",
      "18 [4.4 1.4 0. ]\n",
      "19 [5.1 1.5 0. ]\n",
      "20 [4.4 1.3 0. ]\n",
      "21 [4.8 1.6 0. ]\n",
      "22 [5.2 1.5 0. ]\n",
      "23 [5.  1.5 0. ]\n",
      "24 [5.4 1.5 0. ]\n",
      "25 [6.3 4.4 1. ]\n",
      "26 [5.  3.3 1. ]\n",
      "27 [6.4 4.3 1. ]\n",
      "28 [6.6 4.4 1. ]\n",
      "29 [6.6 4.6 1. ]\n",
      "30 [5.7 4.2 1. ]\n",
      "31 [5.1 3.  1. ]\n",
      "32 [6.1 4.7 1. ]\n",
      "33 [6.3 4.7 1. ]\n",
      "34 [6.2 4.3 1. ]\n",
      "35 [5.6 4.1 1. ]\n",
      "36 [6.9 4.9 1. ]\n",
      "37 [6.1 4.6 1. ]\n",
      "38 [5.  3.5 1. ]\n",
      "39 [5.6 3.9 1. ]\n",
      "40 [5.8 3.9 1. ]\n",
      "41 [6.8 4.8 1. ]\n",
      "42 [6.1 4.  1. ]\n",
      "43 [6.  4.5 1. ]\n",
      "44 [5.8 4.  1. ]\n",
      "45 [6.4 4.5 1. ]\n",
      "46 [6. 4. 1.]\n",
      "47 [5.5 3.8 1. ]\n",
      "48 [6.1 4.7 1. ]\n",
      "49 [5.6 3.6 1. ]\n",
      "50 [6.5 5.2 2. ]\n",
      "51 [6.8 5.9 2. ]\n",
      "52 [6.9 5.4 2. ]\n",
      "53 [6.3 4.9 2. ]\n",
      "54 [7.3 6.3 2. ]\n",
      "55 [6.2 4.8 2. ]\n",
      "56 [6.2 5.4 2. ]\n",
      "57 [7.9 6.4 2. ]\n",
      "58 [5.7 5.  2. ]\n",
      "59 [6.4 5.6 2. ]\n",
      "60 [6.7 5.2 2. ]\n",
      "61 [6.3 5.  2. ]\n",
      "62 [6.3 5.1 2. ]\n",
      "63 [6.4 5.6 2. ]\n",
      "64 [5.8 5.1 2. ]\n",
      "65 [7.6 6.6 2. ]\n",
      "66 [5.8 5.1 2. ]\n",
      "67 [6.7 5.8 2. ]\n",
      "68 [7.4 6.1 2. ]\n",
      "69 [6. 5. 2.]\n",
      "70 [5.6 4.9 2. ]\n",
      "71 [6.4 5.3 2. ]\n",
      "72 [5.9 5.1 2. ]\n",
      "73 [7.7 6.1 2. ]\n",
      "74 [7.1 5.9 2. ]\n",
      "NEXT--\n"
     ]
    }
   ],
   "source": [
    "for fold in train_dataset:\n",
    "    for index, row in enumerate(train_dataset[fold]):\n",
    "        print(index, row)\n",
    "    print(\"NEXT--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 1.4],\n",
       "       [4.9, 1.4],\n",
       "       [4.7, 1.3],\n",
       "       [4.6, 1.5],\n",
       "       [5. , 1.4],\n",
       "       [5.4, 1.7],\n",
       "       [4.6, 1.4],\n",
       "       [5. , 1.5],\n",
       "       [4.4, 1.4],\n",
       "       [4.9, 1.5],\n",
       "       [5.4, 1.5],\n",
       "       [4.8, 1.6],\n",
       "       [4.8, 1.4],\n",
       "       [4.3, 1.1],\n",
       "       [5.8, 1.2],\n",
       "       [5.7, 1.5],\n",
       "       [5.4, 1.3],\n",
       "       [5.1, 1.4],\n",
       "       [5.7, 1.7],\n",
       "       [5.1, 1.5],\n",
       "       [5.4, 1.7],\n",
       "       [5.1, 1.5],\n",
       "       [4.6, 1. ],\n",
       "       [5.1, 1.7],\n",
       "       [4.8, 1.9],\n",
       "       [5. , 1.6],\n",
       "       [5. , 1.6],\n",
       "       [5.2, 1.5],\n",
       "       [5.2, 1.4],\n",
       "       [4.7, 1.6],\n",
       "       [4.8, 1.6],\n",
       "       [5.4, 1.5],\n",
       "       [5.2, 1.5],\n",
       "       [5.5, 1.4],\n",
       "       [4.9, 1.5],\n",
       "       [5. , 1.2],\n",
       "       [5.5, 1.3],\n",
       "       [4.9, 1.4],\n",
       "       [4.4, 1.3],\n",
       "       [5.1, 1.5],\n",
       "       [5. , 1.3],\n",
       "       [4.5, 1.3],\n",
       "       [4.4, 1.3],\n",
       "       [5. , 1.6],\n",
       "       [5.1, 1.9],\n",
       "       [4.8, 1.4],\n",
       "       [5.1, 1.6],\n",
       "       [4.6, 1.4],\n",
       "       [5.3, 1.5],\n",
       "       [5. , 1.4],\n",
       "       [7. , 4.7],\n",
       "       [6.4, 4.5],\n",
       "       [6.9, 4.9],\n",
       "       [5.5, 4. ],\n",
       "       [6.5, 4.6],\n",
       "       [5.7, 4.5],\n",
       "       [6.3, 4.7],\n",
       "       [4.9, 3.3],\n",
       "       [6.6, 4.6],\n",
       "       [5.2, 3.9],\n",
       "       [5. , 3.5],\n",
       "       [5.9, 4.2],\n",
       "       [6. , 4. ],\n",
       "       [6.1, 4.7],\n",
       "       [5.6, 3.6],\n",
       "       [6.7, 4.4],\n",
       "       [5.6, 4.5],\n",
       "       [5.8, 4.1],\n",
       "       [6.2, 4.5],\n",
       "       [5.6, 3.9],\n",
       "       [5.9, 4.8],\n",
       "       [6.1, 4. ],\n",
       "       [6.3, 4.9],\n",
       "       [6.1, 4.7],\n",
       "       [6.4, 4.3],\n",
       "       [6.6, 4.4],\n",
       "       [6.8, 4.8],\n",
       "       [6.7, 5. ],\n",
       "       [6. , 4.5],\n",
       "       [5.7, 3.5],\n",
       "       [5.5, 3.8],\n",
       "       [5.5, 3.7],\n",
       "       [5.8, 3.9],\n",
       "       [6. , 5.1],\n",
       "       [5.4, 4.5],\n",
       "       [6. , 4.5],\n",
       "       [6.7, 4.7],\n",
       "       [6.3, 4.4],\n",
       "       [5.6, 4.1],\n",
       "       [5.5, 4. ],\n",
       "       [5.5, 4.4],\n",
       "       [6.1, 4.6],\n",
       "       [5.8, 4. ],\n",
       "       [5. , 3.3],\n",
       "       [5.6, 4.2],\n",
       "       [5.7, 4.2],\n",
       "       [5.7, 4.2],\n",
       "       [6.2, 4.3],\n",
       "       [5.1, 3. ],\n",
       "       [5.7, 4.1],\n",
       "       [6.3, 6. ],\n",
       "       [5.8, 5.1],\n",
       "       [7.1, 5.9],\n",
       "       [6.3, 5.6],\n",
       "       [6.5, 5.8],\n",
       "       [7.6, 6.6],\n",
       "       [4.9, 4.5],\n",
       "       [7.3, 6.3],\n",
       "       [6.7, 5.8],\n",
       "       [7.2, 6.1],\n",
       "       [6.5, 5.1],\n",
       "       [6.4, 5.3],\n",
       "       [6.8, 5.5],\n",
       "       [5.7, 5. ],\n",
       "       [5.8, 5.1],\n",
       "       [6.4, 5.3],\n",
       "       [6.5, 5.5],\n",
       "       [7.7, 6.7],\n",
       "       [7.7, 6.9],\n",
       "       [6. , 5. ],\n",
       "       [6.9, 5.7],\n",
       "       [5.6, 4.9],\n",
       "       [7.7, 6.7],\n",
       "       [6.3, 4.9],\n",
       "       [6.7, 5.7],\n",
       "       [7.2, 6. ],\n",
       "       [6.2, 4.8],\n",
       "       [6.1, 4.9],\n",
       "       [6.4, 5.6],\n",
       "       [7.2, 5.8],\n",
       "       [7.4, 6.1],\n",
       "       [7.9, 6.4],\n",
       "       [6.4, 5.6],\n",
       "       [6.3, 5.1],\n",
       "       [6.1, 5.6],\n",
       "       [7.7, 6.1],\n",
       "       [6.3, 5.6],\n",
       "       [6.4, 5.5],\n",
       "       [6. , 4.8],\n",
       "       [6.9, 5.4],\n",
       "       [6.7, 5.6],\n",
       "       [6.9, 5.1],\n",
       "       [5.8, 5.1],\n",
       "       [6.8, 5.9],\n",
       "       [6.7, 5.7],\n",
       "       [6.7, 5.2],\n",
       "       [6.3, 5. ],\n",
       "       [6.5, 5.2],\n",
       "       [6.2, 5.4],\n",
       "       [5.9, 5.1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.2, 1.5, 0. ],\n",
       "       [5.4, 1.5, 0. ],\n",
       "       [5.3, 1.5, 0. ],\n",
       "       [5.8, 1.2, 0. ],\n",
       "       [4.8, 1.4, 0. ],\n",
       "       [4.4, 1.3, 0. ],\n",
       "       [4.5, 1.3, 0. ],\n",
       "       [5.5, 1.3, 0. ],\n",
       "       [5.4, 1.5, 0. ],\n",
       "       [5.1, 1.5, 0. ],\n",
       "       [5. , 1.4, 0. ],\n",
       "       [4.7, 1.3, 0. ],\n",
       "       [5.5, 1.4, 0. ],\n",
       "       [5.4, 1.3, 0. ],\n",
       "       [4.6, 1. , 0. ],\n",
       "       [5. , 1.5, 0. ],\n",
       "       [4.9, 1.4, 0. ],\n",
       "       [4.3, 1.1, 0. ],\n",
       "       [4.9, 1.5, 0. ],\n",
       "       [4.6, 1.5, 0. ],\n",
       "       [5. , 1.3, 0. ],\n",
       "       [4.8, 1.6, 0. ],\n",
       "       [5.4, 1.7, 0. ],\n",
       "       [5.1, 1.6, 0. ],\n",
       "       [5.1, 1.5, 0. ],\n",
       "       [5. , 1.6, 0. ],\n",
       "       [5.1, 1.5, 0. ],\n",
       "       [5.1, 1.4, 0. ],\n",
       "       [5. , 1.2, 0. ],\n",
       "       [4.7, 1.6, 0. ],\n",
       "       [4.8, 1.9, 0. ],\n",
       "       [5.4, 1.7, 0. ],\n",
       "       [4.6, 1.4, 0. ],\n",
       "       [4.8, 1.6, 0. ],\n",
       "       [5.7, 1.5, 0. ],\n",
       "       [5.2, 1.4, 0. ],\n",
       "       [4.4, 1.3, 0. ],\n",
       "       [4.4, 1.4, 0. ],\n",
       "       [4.9, 1.5, 0. ],\n",
       "       [5.2, 1.5, 0. ],\n",
       "       [7. , 4.7, 1. ],\n",
       "       [5.8, 4.1, 1. ],\n",
       "       [6.4, 4.3, 1. ],\n",
       "       [6.1, 4.7, 1. ],\n",
       "       [5.4, 4.5, 1. ],\n",
       "       [6.5, 4.6, 1. ],\n",
       "       [5.7, 4.1, 1. ],\n",
       "       [5. , 3.3, 1. ],\n",
       "       [5.9, 4.2, 1. ],\n",
       "       [6.8, 4.8, 1. ],\n",
       "       [6.6, 4.4, 1. ],\n",
       "       [5. , 3.5, 1. ],\n",
       "       [5.2, 3.9, 1. ],\n",
       "       [6. , 5.1, 1. ],\n",
       "       [6. , 4.5, 1. ],\n",
       "       [5.5, 4. , 1. ],\n",
       "       [5.5, 3.7, 1. ],\n",
       "       [6.2, 4.5, 1. ],\n",
       "       [6.1, 4. , 1. ],\n",
       "       [4.9, 3.3, 1. ],\n",
       "       [6.3, 4.9, 1. ],\n",
       "       [5.5, 3.8, 1. ],\n",
       "       [5.6, 4.2, 1. ],\n",
       "       [6.4, 4.5, 1. ],\n",
       "       [6.3, 4.7, 1. ],\n",
       "       [5.6, 4.5, 1. ],\n",
       "       [5.5, 4.4, 1. ],\n",
       "       [6.9, 4.9, 1. ],\n",
       "       [5.9, 4.8, 1. ],\n",
       "       [5.6, 3.9, 1. ],\n",
       "       [6. , 4. , 1. ],\n",
       "       [5.8, 3.9, 1. ],\n",
       "       [6.3, 4.4, 1. ],\n",
       "       [5.8, 4. , 1. ],\n",
       "       [6.7, 5. , 1. ],\n",
       "       [6.1, 4.6, 1. ],\n",
       "       [5.7, 4.2, 1. ],\n",
       "       [5.6, 3.6, 1. ],\n",
       "       [5.6, 4.1, 1. ],\n",
       "       [5.1, 3. , 1. ],\n",
       "       [5.8, 5.1, 2. ],\n",
       "       [5.7, 5. , 2. ],\n",
       "       [6. , 4.8, 2. ],\n",
       "       [6.3, 4.9, 2. ],\n",
       "       [7.7, 6.1, 2. ],\n",
       "       [6.4, 5.5, 2. ],\n",
       "       [5.6, 4.9, 2. ],\n",
       "       [6.2, 4.8, 2. ],\n",
       "       [7.1, 5.9, 2. ],\n",
       "       [7.7, 6.7, 2. ],\n",
       "       [5.9, 5.1, 2. ],\n",
       "       [6.7, 5.8, 2. ],\n",
       "       [6.8, 5.9, 2. ],\n",
       "       [6.1, 5.6, 2. ],\n",
       "       [6.5, 5.2, 2. ],\n",
       "       [6.3, 6. , 2. ],\n",
       "       [6.2, 5.4, 2. ],\n",
       "       [7.6, 6.6, 2. ],\n",
       "       [6.7, 5.2, 2. ],\n",
       "       [7.2, 6.1, 2. ],\n",
       "       [6.9, 5.4, 2. ],\n",
       "       [6.4, 5.6, 2. ],\n",
       "       [6.3, 5.6, 2. ],\n",
       "       [6.3, 5.6, 2. ],\n",
       "       [6.9, 5.7, 2. ],\n",
       "       [5.8, 5.1, 2. ],\n",
       "       [6.7, 5.7, 2. ],\n",
       "       [6.5, 5.1, 2. ],\n",
       "       [7.7, 6.9, 2. ],\n",
       "       [7.9, 6.4, 2. ],\n",
       "       [7.7, 6.7, 2. ],\n",
       "       [6.7, 5.7, 2. ],\n",
       "       [6.8, 5.5, 2. ],\n",
       "       [7.2, 6. , 2. ],\n",
       "       [7.3, 6.3, 2. ],\n",
       "       [6.4, 5.3, 2. ],\n",
       "       [4.9, 4.5, 2. ],\n",
       "       [6.3, 5.1, 2. ],\n",
       "       [6.9, 5.1, 2. ],\n",
       "       [7.4, 6.1, 2. ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dict_all_models, results_average_dict, models = grid_search_stratified(\n",
    "    x,\n",
    "    y,\n",
    "    clf=NeuralNetwork,\n",
    "    metrics=[\"F1\", \"accuracy\"],\n",
    "    sort_by = \"accuracy\",\n",
    "    n_fold=10,\n",
    "    param_grid_dict={\n",
    "        'batch_size': [8, 16, 32],\n",
    "        'input_layer': [(2, 'relu'), (2, 'tanh')],\n",
    "        'hidden_layer': [\n",
    "            [(4,'relu'), (4,'relu'), (4,'softmax')],\n",
    "            [(4,'sigmoid'),(4,'softmax')]\n",
    "        ],\n",
    "        'output_layer': [3],\n",
    "        'alpha': [1, 2, 4],\n",
    "        'verbose': [False],\n",
    "        'epoch': [100]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_average_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dict_all_models[\"model_17\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(models[\"model_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "dt = data.data[:,[0,2]]\n",
    "x_min, x_max = dt[:, 0].min() - 1, dt[:, 0].max() + 1\n",
    "y_min, y_max = dt[:, 1].min() - 1, dt[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = models[\"model_1\"].predict(np.c_[xx.ravel(), yy.ravel()].T) \n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx, yy, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "\n",
    "plt.scatter(dt[:, 0], dt[:, 1], c=y,s=20, edgecolor='k')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('petal length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Make Moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "x,y =make_moons(n_samples=1500, noise=.05)\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(2, \"sigmoid\"),\n",
    "    hidden_layer=[(8, \"tanh\"),\n",
    "                  (6, \"relu\"),\n",
    "                  (4, \"softmax\")],\n",
    "    output_layer=2,\n",
    "    batch_size=64,\n",
    "    alpha=0.5,\n",
    "    epoch=2500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Decision Boundaries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "dt = x\n",
    "x_min, x_max = dt[:, 0].min() - 0.5, dt[:, 0].max() + 0.5\n",
    "y_min, y_max = dt[:, 1].min() - 0.5, dt[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()].T) \n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx, yy, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "plt.scatter(dt[:, 0], dt[:, 1], c=y, s=20, edgecolor='k')\n",
    "plt.title('Decision Boundaries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Andrew NG Assignment 2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex2data2 = np.loadtxt(\"../ex2/data/ex2data2.txt\", delimiter=\",\")\n",
    "\n",
    "x = ex2data2[:, :-1]\n",
    "y = ex2data2[:, -1]\n",
    "\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(X.shape[0], \"relu\"),\n",
    "    hidden_layer=[(4, \"relu\"),\n",
    "                  (4, \"softmax\")],\n",
    "    output_layer=Y.shape[0],\n",
    "    batch_size=16,\n",
    "    alpha=4,\n",
    "    epoch=5000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dict_all_models, results_average_dict, models = grid_search_stratified(\n",
    "    x,\n",
    "    y,\n",
    "    clf=NeuralNetwork,\n",
    "    metrics=[\"F1\", \"accuracy\"],\n",
    "    sort_by = \"accuracy\",\n",
    "    n_fold=6,\n",
    "    param_grid_dict={\n",
    "        'batch_size': [16, 32],\n",
    "        'input_layer': [(2, 'relu')],\n",
    "        'hidden_layer': [\n",
    "            [(4,'relu'), (4,'relu'), (4,'softmax')],\n",
    "            [(4,'sigmoid'),(4,'softmax')]\n",
    "        ],\n",
    "        'output_layer': [2],\n",
    "        'alpha': [2, 4],\n",
    "        'verbose': [False],\n",
    "        'epoch': [5000]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_average_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models[\"model_5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decision Boundaries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "X = ex2data2\n",
    "\n",
    "x1_min, x1_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3,\n",
    "x2_min, x2_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3,\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = models[\"model_5\"].predict(np.c_[xx1.ravel(), xx2.ravel()].T) \n",
    "\n",
    "negatives = ex2data2[ex2data2[:, -1] == 0]\n",
    "positives = ex2data2[ex2data2[:, -1] == 1]\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx1.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx1, xx2, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "plt.scatter(negatives[:, 0], negatives[:, 1],s=50, color='k')\n",
    "plt.scatter(positives[:, 0], positives[:, 1],s=50, color='r')\n",
    "plt.title('Decision Boundaries')\n",
    "\n",
    "plt.contour(xx1, xx2, Z, [0.5], linewidths=2, colors=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "data = loadmat('../ex3/data/ex3data1.mat')\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"X\"]\n",
    "y = data[\"y\"]\n",
    "y[y==10] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.T\n",
    "Y = one_hot_encode(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "sample = np.random.choice(data[\"X\"].shape[0], 20)\n",
    "ax.imshow(data[\"X\"][sample,1:].reshape(-1,20).T)\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dict_all_models, results_average_dict, models = grid_search_stratified(\n",
    "    x,\n",
    "    y,\n",
    "    clf=NeuralNetwork,\n",
    "    metrics=[\"F1\", \"accuracy\"],\n",
    "    sort_by = \"F1\",\n",
    "    n_fold=5,\n",
    "    param_grid_dict={\n",
    "        'batch_size': [16, 32, 64],\n",
    "        'input_layer': [(x.shape[1], 'relu')],\n",
    "        'hidden_layer': [\n",
    "            [(50,'relu'), (10,'softmax')],\n",
    "            [(25,'relu'), (10,'sigmoid'), (10,'softmax')],\n",
    "            [(25,'relu'), (10,'relu'), (10,'softmax')]\n",
    "        ],\n",
    "        'output_layer': [10],\n",
    "        'alpha': [3,4,6],\n",
    "        'verbose': [False],\n",
    "        'epoch': [500]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_average_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dict_all_models[\"model_21\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models[\"model_21\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, (act, predicted) in enumerate(zip(np.argmax(Y,axis=0), models[\"model_21\"].predict(X))):\n",
    "    if act != predicted:\n",
    "        fig, ax = plt.subplots(figsize = (2,2))\n",
    "        ax.set_title(\"%s: act %s --- predicted %s\" %(index, act, predicted))\n",
    "        ax.imshow(X[:, index].reshape(-1,20).T)\n",
    "        ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
