{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utility_functions import (calculate_model_performance,\n",
    "                               plot_ROC,\n",
    "                               one_hot_encode,\n",
    "                               split_data_as,\n",
    "                               random_shuffle)\n",
    "\n",
    "\n",
    "def get_shapes(any_):\n",
    "    for array in any_:\n",
    "        try:\n",
    "            print(array.shape)\n",
    "        except:\n",
    "            print(\"NONE\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# ============= ACTIVATION FUNCTIONS ===============#\n",
    "\n",
    "def sigmoid(Z, prime=False):\n",
    "    # np.\n",
    "    if prime:\n",
    "        return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "\n",
    "def linear(Z, prime=False):\n",
    "    if prime:\n",
    "        return np.ones_like(Z)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def relu(Z, alpha=0.01, prime=False):\n",
    "    if prime:\n",
    "        Z_relu = np.ones_like(Z, dtype=np.float64)\n",
    "        Z_relu[Z < 0] = alpha\n",
    "        return Z_relu\n",
    "    return np.where(Z < 0, alpha * Z, Z)\n",
    "\n",
    "\n",
    "def tanh(Z, prime=False):\n",
    "    # np.tanh() could be used directly to speed this up\n",
    "    if prime:\n",
    "        return 1 - np.power(tanh(Z), 2)\n",
    "    return (2 / (1 + np.exp(-2 * Z))) - 1\n",
    "\n",
    "\n",
    "def elu(Z, prime=False):\n",
    "    # https://mlfromscratch.com/activation-functions-explained/#/\n",
    "    alpha = 0.2\n",
    "    if prime:\n",
    "        return np.where(Z < 0, alpha * (np.exp(Z)), 1)\n",
    "    return np.where(Z < 0, alpha * (np.exp(Z) - 1), Z)\n",
    "\n",
    "\n",
    "def softmax(Z, prime=False):\n",
    "    # https://deepnotes.io/softmax-crossentropy\n",
    "    # max(Z) term is added to stabilise the function.\n",
    "    exps = np.exp(Z - np.max(Z))\n",
    "    return exps / np.sum(exps, axis=0)\n",
    "\n",
    "\n",
    "# ============== LOSS FUNCTIONS ===============#\n",
    "\n",
    "# https://deepnotes.io/softmax-crossentropy\n",
    "EPSILON = 1e-8\n",
    "\n",
    "\n",
    "def calculate_error(Y, Y_hat):\n",
    "    # Y and Y_hat should be in the form of (no_of_classes, no_of_training_examples)\n",
    "    m = Y.shape[1]\n",
    "    return -np.sum(Y * np.log(Y_hat + EPSILON)) / m\n",
    "\n",
    "\n",
    "# References\n",
    "# https://mc.ai/multilayered-neural-network-from-scratch-using-python/\n",
    "# https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "# https://www.coursera.org/learn/machine-learning/home/week/5\n",
    "# https://www.coursera.org/specializations/deep-learning\n",
    "# https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py\n",
    "# https://github.com/JWarmenhoven/Coursera-Machine-Learning\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_layer: tuple,\n",
    "            hidden_layer: list,  # list of tuples\n",
    "            output_layer: int,\n",
    "            batch_size=16,\n",
    "            alpha=1,\n",
    "            epoch=500,\n",
    "            random_state=42,\n",
    "            verbose=True,\n",
    "            metrics=\"accuracy\"\n",
    "    ):\n",
    "        self.input_layer = input_layer\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.output_layer = output_layer\n",
    "        self.mini_batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.epoch = epoch\n",
    "        self.seed = random_state\n",
    "        self.verbose = verbose\n",
    "        self.metrics = metrics\n",
    "\n",
    "        self.layers = len(self.weight_set_dimensions) + 1\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        parameters = (\n",
    "            \"Input layer: {0}\\n\"\n",
    "            \"Hidden layer: {1}\\n\"\n",
    "            \"Output layer: {2}\\n\"\n",
    "            \"Batch size: {3}\\n\"\n",
    "            \"Learning rate: {4}\\n\"\n",
    "            \"Epoch: {5}\\n\"\n",
    "            \"Seed: {6}\\n\"\n",
    "            \"Verbose: {7}\\n\"\n",
    "            \"Metric: {8}\"\n",
    "        ).format(\n",
    "            self.input_layer,\n",
    "            self.hidden_layer,\n",
    "            self.output_layer,\n",
    "            self.mini_batch_size,\n",
    "            self.alpha,\n",
    "            self.epoch,\n",
    "            self.seed,\n",
    "            self.verbose,\n",
    "            self.metrics\n",
    "        )\n",
    "        return parameters\n",
    "\n",
    "    def get_A(self, X):\n",
    "        A, _ = self.forwardpass(X)\n",
    "        return A\n",
    "\n",
    "    def get_Z(self, X):\n",
    "        _, Z = self.forwardpass(X)\n",
    "        return Z\n",
    "\n",
    "    def display_information(self, X, Y, epoch_no):\n",
    "        model_performance_metrics = calculate_model_performance(\n",
    "            np.argmax(Y, axis=0),\n",
    "            self.predict(X)\n",
    "        )\n",
    "        print(\"%s: %.10f - epoch %s    iteration %s - loss %.20f\" % (\n",
    "            self.metrics,\n",
    "            model_performance_metrics[self.metrics],\n",
    "            epoch_no,\n",
    "            self.no_of_iterations,\n",
    "            calculate_error(Y,\n",
    "                            self.get_A(X)[-1])\n",
    "        )\n",
    "              )\n",
    "\n",
    "    def get_dimensions_and_activations(self):\n",
    "        self.dimensions = []\n",
    "        self.activation_functions = []\n",
    "\n",
    "        self.dimensions.append(self.input_layer[0])\n",
    "        self.activation_functions.append(self.input_layer[1])\n",
    "\n",
    "        for dim, act_func in self.hidden_layer:\n",
    "            self.dimensions.append(dim)\n",
    "            self.activation_functions.append(act_func)\n",
    "\n",
    "        self.dimensions.append(self.output_layer)\n",
    "\n",
    "    @property\n",
    "    def weight_set_dimensions(self):\n",
    "        self.get_dimensions_and_activations()\n",
    "        a, b = itertools.tee(self.dimensions[::-1])\n",
    "        next(b, None)\n",
    "        weight_set_dimensions = list(zip(a, b))[::-1]\n",
    "        return weight_set_dimensions\n",
    "\n",
    "    def initialise_weights(self, layer=None):\n",
    "        self.W = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.B = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.W[0] = None\n",
    "        self.B[0] = None\n",
    "        for layer, (y, x) in zip(range(1, self.layers), self.weight_set_dimensions):\n",
    "            np.random.seed(self.seed)\n",
    "            self.W[layer] = np.random.rand(y, x) / np.sqrt(self.dimensions[layer - 1])\n",
    "            self.B[layer] = np.random.rand(y, 1)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        Z = np.empty_like(range(self.layers), dtype=object)\n",
    "        A = np.empty_like(range(self.layers), dtype=object)\n",
    "        A[0] = X\n",
    "        Z[0] = None\n",
    "        for layer in range(1, self.layers):\n",
    "            # activation_function starts from 0 whereas layer starts from 1\n",
    "            active_function = self.activation_functions[layer - 1]\n",
    "            arg_to_pass_to_eval = \"(Z[layer])\"\n",
    "\n",
    "            Z[layer] = self.W[layer] @ A[layer - 1] + self.B[layer]\n",
    "            A[layer] = eval(active_function + arg_to_pass_to_eval)\n",
    "        return A, Z\n",
    "\n",
    "    def backpropagation(self, Y, A, Z):\n",
    "        self.delta = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.delta[0] = None\n",
    "\n",
    "        self.gradient_W = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.gradient_B = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.gradient_W[0] = None\n",
    "        self.gradient_B[0] = None\n",
    "\n",
    "        self.delta[-1] = A[-1] - Y\n",
    "\n",
    "        # We substract 1 here as delta_final is calculated seperately above\n",
    "        for layer in reversed(range(1, self.layers - 1)):\n",
    "            # 1 is substracted from layer as activations_functions start indexing from 0\n",
    "            active_function = self.activation_functions[layer - 1]\n",
    "            arg_to_pass_to_eval = \"(Z[layer], prime=True)\"\n",
    "\n",
    "\n",
    "            self.delta[layer] = (\n",
    "                    self.W[layer + 1].T @ self.delta[layer + 1] *\n",
    "                    eval(active_function + arg_to_pass_to_eval)\n",
    "            )\n",
    "\n",
    "            # calculate the gradient\n",
    "\n",
    "        for layer in range(1, self.layers):\n",
    "            self.gradient_W[layer] = (self.delta[layer] @ A[layer - 1].T) / self.m\n",
    "            self.gradient_B[layer] = np.sum(self.delta[layer], axis=1, keepdims=True) / self.m\n",
    "\n",
    "        # update the weights\n",
    "        for layer in range(1, self.layers):\n",
    "            cost = calculate_error(Y, A[-1])\n",
    "            cost_other = -np.mean(Y * np.log(A[-1] + 1e-8))\n",
    "            self.W[layer] -= self.alpha * self.gradient_W[layer]\n",
    "            self.B[layer] -= self.alpha * self.gradient_B[layer]\n",
    "\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.m = X.shape[1] # where (no_of_features, no_of_training_examples)\n",
    "        self.initialise_weights()\n",
    "\n",
    "        # By default the method is SGD(Stochastic Gradient Descent) if one wishes to use\n",
    "        # the whole batch, simply pass the number of traning examples available as the\n",
    "        # batch size when instantiating the class\n",
    "        self.no_of_iterations = 0\n",
    "        shuffled = np.arange(self.m)\n",
    "        if self.verbose:\n",
    "            print(\"Initialising weights...\")\n",
    "            print(\"Starting the training...\")\n",
    "            print(\"Initial cost: %.10f\\n\" % calculate_error(Y, self.get_A(X)[-1]))\n",
    "        for epoch_no in range(1, self.epoch + 1):\n",
    "            np.random.shuffle(shuffled)\n",
    "            X_shuffled = X[:, shuffled]\n",
    "            Y_shuffled = Y[:, shuffled]\n",
    "            for i in range(0, self.m, self.mini_batch_size):\n",
    "                self.no_of_iterations += 1\n",
    "                X_mini_batch = X_shuffled[:, i: i + self.mini_batch_size]\n",
    "                Y_mini_batch = Y_shuffled[:, i: i + self.mini_batch_size]\n",
    "\n",
    "                A, Z = self.forwardpass(X_mini_batch)\n",
    "                self.backpropagation(Y_mini_batch, A, Z)\n",
    "                if self.no_of_iterations % 5000 == 0 and self.verbose:\n",
    "                    self.display_information(X, Y, epoch_no)\n",
    "\n",
    "    def predict(\n",
    "            self,\n",
    "            X: np.ndarray,\n",
    "            return_prob_matrix=False\n",
    "    ):\n",
    "        \"\"\"Predict the output given the training data.\n",
    "\n",
    "            Returns the predicted values in two forms:\n",
    "\n",
    "            1.either by picking up the highest value along the columns for every row,\n",
    "                i.e. \"np.argmax(self.A[-1].T, axis=1)\"\n",
    "            2.or by returning a matrix that is in the shape of Y.T where each column\n",
    "                represents the probability of the instance belonging to that class.\n",
    "                Please note that every column in Y.T represents a class. To be able to\n",
    "                return the probability matrix, the final activation function must be\n",
    "                softmax!\n",
    "                i.e. \"array([0.9650488423, 0.0354737543, 0.0005225966])\"\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Training set in the shape of\n",
    "                (no_of_features, no_of_training examples).\n",
    "            return_prob_matrix (bool, optional): Returns the probability matrix if True.\n",
    "                Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray:\n",
    "\n",
    "            if return_prob_matrix is False, the output is in the shape of\n",
    "                (no_of_training_examples, 1)\n",
    "            if return_prob_matrix is True, the output is in the shape of\n",
    "                (no_of_training_examples, no_of_features)\n",
    "        \"\"\"\n",
    "        A, Z = self.forwardpass(X)\n",
    "        if return_prob_matrix and self.activation_functions[-1] == \"softmax\":\n",
    "            np.set_printoptions(precision=10, suppress=True)\n",
    "            return A[-1].T\n",
    "        return np.argmax(A[-1].T, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with benchmark datasets\n",
    "\n",
    "## 1.Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "x = data.data[:,[0,2]]\n",
    "y = data.target\n",
    "\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T\n",
    "\n",
    "# train, test = split_data_as(x, y, train=0.8, test=0.2)\n",
    "\n",
    "# X_train = train[:, :-1]\n",
    "# Y_train = train[:, -1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 150)\n",
      "(3, 150)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "# print(\"\\n\")\n",
    "# print(X_test.shape)\n",
    "# print(Y_test.shape)\n",
    "# print(\"\\n\")\n",
    "# print(X_validation.shape)\n",
    "# print(Y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def grid_search(X, y, clf, metric, n_fold=3, **kwargs):\n",
    "    # X and y are in the shape of (no_of_features, no_of_training_examples)\n",
    "    split_indices = np.int_(np.linspace(len(X)/n_fold, len(X), num=n_fold))\n",
    "    dataset_shuffled = random_shuffle(X, y)\n",
    "\n",
    "    splitted = np.array(\n",
    "        np.split(\n",
    "            dataset_shuffled,\n",
    "            split_indices\n",
    "        )[:-1]\n",
    "    )\n",
    "\n",
    "    models = {}\n",
    "    results_dict_all_models = {}\n",
    "    results_average_dict = {}\n",
    "\n",
    "    grid = ParameterGrid(kwargs['param_grid'])\n",
    "    n_to_run = len(grid) * n_fold\n",
    "    count = 1\n",
    "    for index_model, params in enumerate(grid):\n",
    "        models[\"model_\" + str(index_model + 1)] = clf(**params)\n",
    "        results_dict = {}\n",
    "\n",
    "        for index_fold in range(n_fold):\n",
    "            print(\"\\n*********{}/{}*********\".format(count ,n_to_run))\n",
    "            print(\"Running model {0} fold {1}\".format(\n",
    "                str(index_model + 1),\n",
    "                str(index_fold + 1)\n",
    "            ))\n",
    "            arrays_to_be_joined = np.delete(splitted, index_fold, axis = 0)\n",
    "            dataset_train = np.concatenate(arrays_to_be_joined)\n",
    "            dataset_test = splitted[index_fold]\n",
    "\n",
    "            x_train = dataset_train[:, :-1].T\n",
    "            y_train = one_hot_encode(dataset_train[:, -1]).T\n",
    "\n",
    "            models[\"model_\" + str(index_model + 1)].fit(x_train, y_train)\n",
    "\n",
    "            x_test = dataset_test[:, :-1].T\n",
    "            y_test = one_hot_encode(dataset_test[:, -1]).T\n",
    "\n",
    "            results_dict[\"model_\" + str(index_model + 1) + \"_fold_\" + str(index_fold + 1)] = \\\n",
    "            calculate_model_performance(\n",
    "                np.argmax(y_test, axis=0),\n",
    "                models[\"model_\" + str(index_model + 1)].predict(x_test)\n",
    "            )[metric]\n",
    "            count += 1\n",
    "            \n",
    "        results_dict_all_models[index_model + 1] = results_dict\n",
    "\n",
    "        vals = np.fromiter(results_dict.values(), dtype=float)\n",
    "        results_average_dict[\"model_\" + str(index_model + 1)] = np.average(vals)\n",
    "        results_average_dict = dict(sorted(results_average_dict.items(), key=lambda x: x[1])[::-1])\n",
    "\n",
    "    return results_dict_all_models, results_average_dict, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*********1/96*********\n",
      "Running model 1 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.9573791653\n",
      "\n",
      "\n",
      "*********2/96*********\n",
      "Running model 1 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.8939648063\n",
      "\n",
      "\n",
      "*********3/96*********\n",
      "Running model 1 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.9619631243\n",
      "\n",
      "\n",
      "*********4/96*********\n",
      "Running model 2 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3583599141\n",
      "\n",
      "\n",
      "*********5/96*********\n",
      "Running model 2 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3310232629\n",
      "\n",
      "\n",
      "*********6/96*********\n",
      "Running model 2 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3545285979\n",
      "\n",
      "\n",
      "*********7/96*********\n",
      "Running model 3 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2588600307\n",
      "\n",
      "\n",
      "*********8/96*********\n",
      "Running model 3 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2372829437\n",
      "\n",
      "\n",
      "*********9/96*********\n",
      "Running model 3 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2519764210\n",
      "\n",
      "\n",
      "*********10/96*********\n",
      "Running model 4 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2356830890\n",
      "\n",
      "\n",
      "*********11/96*********\n",
      "Running model 4 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2156481860\n",
      "\n",
      "\n",
      "*********12/96*********\n",
      "Running model 4 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2272074932\n",
      "\n",
      "\n",
      "*********13/96*********\n",
      "Running model 5 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.9573791653\n",
      "\n",
      "\n",
      "*********14/96*********\n",
      "Running model 5 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.8939648063\n",
      "\n",
      "\n",
      "*********15/96*********\n",
      "Running model 5 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.9619631243\n",
      "\n",
      "\n",
      "*********16/96*********\n",
      "Running model 6 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3583599141\n",
      "\n",
      "\n",
      "*********17/96*********\n",
      "Running model 6 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3310232629\n",
      "\n",
      "\n",
      "*********18/96*********\n",
      "Running model 6 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3545285979\n",
      "\n",
      "\n",
      "*********19/96*********\n",
      "Running model 7 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2588600307\n",
      "\n",
      "\n",
      "*********20/96*********\n",
      "Running model 7 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2372829437\n",
      "\n",
      "\n",
      "*********21/96*********\n",
      "Running model 7 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2519764210\n",
      "\n",
      "\n",
      "*********22/96*********\n",
      "Running model 8 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2356830890\n",
      "\n",
      "\n",
      "*********23/96*********\n",
      "Running model 8 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2156481860\n",
      "\n",
      "\n",
      "*********24/96*********\n",
      "Running model 8 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2272074932\n",
      "\n",
      "\n",
      "*********25/96*********\n",
      "Running model 9 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.9573791653\n",
      "\n",
      "\n",
      "*********26/96*********\n",
      "Running model 9 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.8939648063\n",
      "\n",
      "\n",
      "*********27/96*********\n",
      "Running model 9 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.9619631243\n",
      "\n",
      "\n",
      "*********28/96*********\n",
      "Running model 10 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3583599141\n",
      "\n",
      "\n",
      "*********29/96*********\n",
      "Running model 10 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3310232629\n",
      "\n",
      "\n",
      "*********30/96*********\n",
      "Running model 10 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3545285979\n",
      "\n",
      "\n",
      "*********31/96*********\n",
      "Running model 11 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2588600307\n",
      "\n",
      "\n",
      "*********32/96*********\n",
      "Running model 11 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2372829437\n",
      "\n",
      "\n",
      "*********33/96*********\n",
      "Running model 11 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2519764210\n",
      "\n",
      "\n",
      "*********34/96*********\n",
      "Running model 12 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2356830890\n",
      "\n",
      "\n",
      "*********35/96*********\n",
      "Running model 12 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2156481860\n",
      "\n",
      "\n",
      "*********36/96*********\n",
      "Running model 12 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2272074932\n",
      "\n",
      "\n",
      "*********37/96*********\n",
      "Running model 13 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.9573791653\n",
      "\n",
      "\n",
      "*********38/96*********\n",
      "Running model 13 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.8939648063\n",
      "\n",
      "\n",
      "*********39/96*********\n",
      "Running model 13 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.9619631243\n",
      "\n",
      "\n",
      "*********40/96*********\n",
      "Running model 14 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3583599141\n",
      "\n",
      "\n",
      "*********41/96*********\n",
      "Running model 14 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3310232629\n",
      "\n",
      "\n",
      "*********42/96*********\n",
      "Running model 14 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3545285979\n",
      "\n",
      "\n",
      "*********43/96*********\n",
      "Running model 15 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2588600307\n",
      "\n",
      "\n",
      "*********44/96*********\n",
      "Running model 15 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2372829437\n",
      "\n",
      "\n",
      "*********45/96*********\n",
      "Running model 15 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2519764210\n",
      "\n",
      "\n",
      "*********46/96*********\n",
      "Running model 16 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2356830890\n",
      "\n",
      "\n",
      "*********47/96*********\n",
      "Running model 16 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2156481860\n",
      "\n",
      "\n",
      "*********48/96*********\n",
      "Running model 16 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2272074932\n",
      "\n",
      "\n",
      "*********49/96*********\n",
      "Running model 17 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.9573791653\n",
      "\n",
      "\n",
      "*********50/96*********\n",
      "Running model 17 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.8939648063\n",
      "\n",
      "\n",
      "*********51/96*********\n",
      "Running model 17 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.9619631243\n",
      "\n",
      "\n",
      "*********52/96*********\n",
      "Running model 18 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3583599141\n",
      "\n",
      "\n",
      "*********53/96*********\n",
      "Running model 18 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3310232629\n",
      "\n",
      "\n",
      "*********54/96*********\n",
      "Running model 18 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3545285979\n",
      "\n",
      "\n",
      "*********55/96*********\n",
      "Running model 19 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2588600307\n",
      "\n",
      "\n",
      "*********56/96*********\n",
      "Running model 19 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2372829437\n",
      "\n",
      "\n",
      "*********57/96*********\n",
      "Running model 19 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2519764210\n",
      "\n",
      "\n",
      "*********58/96*********\n",
      "Running model 20 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2356830890\n",
      "\n",
      "\n",
      "*********59/96*********\n",
      "Running model 20 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2156481860\n",
      "\n",
      "\n",
      "*********60/96*********\n",
      "Running model 20 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2272074932\n",
      "\n",
      "\n",
      "*********61/96*********\n",
      "Running model 21 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.9573791653\n",
      "\n",
      "\n",
      "*********62/96*********\n",
      "Running model 21 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.8939648063\n",
      "\n",
      "\n",
      "*********63/96*********\n",
      "Running model 21 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.9619631243\n",
      "\n",
      "\n",
      "*********64/96*********\n",
      "Running model 22 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3583599141\n",
      "\n",
      "\n",
      "*********65/96*********\n",
      "Running model 22 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3310232629\n",
      "\n",
      "\n",
      "*********66/96*********\n",
      "Running model 22 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3545285979\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*********67/96*********\n",
      "Running model 23 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2588600307\n",
      "\n",
      "\n",
      "*********68/96*********\n",
      "Running model 23 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2372829437\n",
      "\n",
      "\n",
      "*********69/96*********\n",
      "Running model 23 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2519764210\n",
      "\n",
      "\n",
      "*********70/96*********\n",
      "Running model 24 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2356830890\n",
      "\n",
      "\n",
      "*********71/96*********\n",
      "Running model 24 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2156481860\n",
      "\n",
      "\n",
      "*********72/96*********\n",
      "Running model 24 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2272074932\n",
      "\n",
      "\n",
      "*********73/96*********\n",
      "Running model 25 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.9573791653\n",
      "\n",
      "\n",
      "*********74/96*********\n",
      "Running model 25 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.8939648063\n",
      "\n",
      "\n",
      "*********75/96*********\n",
      "Running model 25 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.9619631243\n",
      "\n",
      "\n",
      "*********76/96*********\n",
      "Running model 26 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3583599141\n",
      "\n",
      "\n",
      "*********77/96*********\n",
      "Running model 26 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3310232629\n",
      "\n",
      "\n",
      "*********78/96*********\n",
      "Running model 26 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3545285979\n",
      "\n",
      "\n",
      "*********79/96*********\n",
      "Running model 27 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2588600307\n",
      "\n",
      "\n",
      "*********80/96*********\n",
      "Running model 27 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2372829437\n",
      "\n",
      "\n",
      "*********81/96*********\n",
      "Running model 27 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2519764210\n",
      "\n",
      "\n",
      "*********82/96*********\n",
      "Running model 28 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2356830890\n",
      "\n",
      "\n",
      "*********83/96*********\n",
      "Running model 28 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2156481860\n",
      "\n",
      "\n",
      "*********84/96*********\n",
      "Running model 28 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2272074932\n",
      "\n",
      "\n",
      "*********85/96*********\n",
      "Running model 29 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.9573791653\n",
      "\n",
      "\n",
      "*********86/96*********\n",
      "Running model 29 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.8939648063\n",
      "\n",
      "\n",
      "*********87/96*********\n",
      "Running model 29 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.9619631243\n",
      "\n",
      "\n",
      "*********88/96*********\n",
      "Running model 30 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3583599141\n",
      "\n",
      "\n",
      "*********89/96*********\n",
      "Running model 30 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3310232629\n",
      "\n",
      "\n",
      "*********90/96*********\n",
      "Running model 30 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.3545285979\n",
      "\n",
      "\n",
      "*********91/96*********\n",
      "Running model 31 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2588600307\n",
      "\n",
      "\n",
      "*********92/96*********\n",
      "Running model 31 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2372829437\n",
      "\n",
      "\n",
      "*********93/96*********\n",
      "Running model 31 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2519764210\n",
      "\n",
      "\n",
      "*********94/96*********\n",
      "Running model 32 fold 1\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2356830890\n",
      "\n",
      "\n",
      "*********95/96*********\n",
      "Running model 32 fold 2\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2156481860\n",
      "\n",
      "\n",
      "*********96/96*********\n",
      "Running model 32 fold 3\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2272074932\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cagri/anaconda3/envs/my_env/lib/python3.7/site-packages/ipykernel_launcher.py:67: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/cagri/anaconda3/envs/my_env/lib/python3.7/site-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in less\n",
      "/home/cagri/anaconda3/envs/my_env/lib/python3.7/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in less\n"
     ]
    }
   ],
   "source": [
    "result_dict_all_models, results_average_dict, models = grid_search(\n",
    "    x,\n",
    "    y,\n",
    "    metric='F1',\n",
    "    clf=NeuralNetwork,\n",
    "    n_fold=3,\n",
    "    param_grid={\n",
    "        'batch_size': [8, 16, 32, 64],\n",
    "        'input_layer': [(2, 'relu'), (2, 'tanh')],\n",
    "        'hidden_layer': [[(4,'relu'), (4,'relu'),(4,'softmax')], [(4,'sigmoid'),(4,'softmax')]],\n",
    "        'output_layer': [3],\n",
    "        'alpha': [2, 4],\n",
    "        'verbose': [True],\n",
    "        'epoch': [10]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'model_1_fold_1': 69.38775509875552,\n",
       "  'model_1_fold_2': 66.66666666359554,\n",
       "  'model_1_fold_3': 61.22448979299459},\n",
       " 2: {'model_2_fold_1': 69.38775509875552,\n",
       "  'model_2_fold_2': 0.0,\n",
       "  'model_2_fold_3': 48.38709677226588},\n",
       " 3: {'model_3_fold_1': 21.052631576542936,\n",
       "  'model_3_fold_2': 0.0,\n",
       "  'model_3_fold_3': 61.22448979299459},\n",
       " 4: {'model_4_fold_1': 0.0,\n",
       "  'model_4_fold_2': 0.0,\n",
       "  'model_4_fold_3': 61.22448979299459},\n",
       " 5: {'model_5_fold_1': 0.0, 'model_5_fold_2': 0.0, 'model_5_fold_3': 0.0},\n",
       " 6: {'model_6_fold_1': 0.0,\n",
       "  'model_6_fold_2': 0.0,\n",
       "  'model_6_fold_3': 61.22448979299459},\n",
       " 7: {'model_7_fold_1': 90.32258063883872,\n",
       "  'model_7_fold_2': 0.0,\n",
       "  'model_7_fold_3': 61.22448979299459},\n",
       " 8: {'model_8_fold_1': 0.0,\n",
       "  'model_8_fold_2': 0.0,\n",
       "  'model_8_fold_3': 46.15384615207101},\n",
       " 9: {'model_9_fold_1': 0.0, 'model_9_fold_2': 0.0, 'model_9_fold_3': 0.0},\n",
       " 10: {'model_10_fold_1': 0.0, 'model_10_fold_2': 0.0, 'model_10_fold_3': 0.0},\n",
       " 11: {'model_11_fold_1': 0.0,\n",
       "  'model_11_fold_2': 0.0,\n",
       "  'model_11_fold_3': 61.22448979299459},\n",
       " 12: {'model_12_fold_1': 0.0, 'model_12_fold_2': 0.0, 'model_12_fold_3': 0.0},\n",
       " 13: {'model_13_fold_1': 0.0,\n",
       "  'model_13_fold_2': 0.0,\n",
       "  'model_13_fold_3': 46.15384615207101},\n",
       " 14: {'model_14_fold_1': 0.0,\n",
       "  'model_14_fold_2': 0.0,\n",
       "  'model_14_fold_3': 46.15384615207101},\n",
       " 15: {'model_15_fold_1': 63.99999999444481,\n",
       "  'model_15_fold_2': 0.0,\n",
       "  'model_15_fold_3': 0.0},\n",
       " 16: {'model_16_fold_1': 0.0, 'model_16_fold_2': 0.0, 'model_16_fold_3': 0.0},\n",
       " 17: {'model_17_fold_1': 0.0, 'model_17_fold_2': 0.0, 'model_17_fold_3': 0.0},\n",
       " 18: {'model_18_fold_1': 0.0, 'model_18_fold_2': 0.0, 'model_18_fold_3': 0.0},\n",
       " 19: {'model_19_fold_1': 69.38775509875552,\n",
       "  'model_19_fold_2': 61.53846153330178,\n",
       "  'model_19_fold_3': 61.22448979299459},\n",
       " 20: {'model_20_fold_1': 69.38775509875552,\n",
       "  'model_20_fold_2': 0.0,\n",
       "  'model_20_fold_3': 61.22448979299459},\n",
       " 21: {'model_21_fold_1': 0.0,\n",
       "  'model_21_fold_2': 0.0,\n",
       "  'model_21_fold_3': 46.15384615207101},\n",
       " 22: {'model_22_fold_1': 0.0,\n",
       "  'model_22_fold_2': 0.0,\n",
       "  'model_22_fold_3': 47.619047617173095},\n",
       " 23: {'model_23_fold_1': 69.38775509875552,\n",
       "  'model_23_fold_2': 81.2499999944297,\n",
       "  'model_23_fold_3': 61.22448979299459},\n",
       " 24: {'model_24_fold_1': 69.38775509875552,\n",
       "  'model_24_fold_2': 0.0,\n",
       "  'model_24_fold_3': 61.22448979299459},\n",
       " 25: {'model_25_fold_1': 0.0, 'model_25_fold_2': 0.0, 'model_25_fold_3': 0.0},\n",
       " 26: {'model_26_fold_1': 0.0, 'model_26_fold_2': 0.0, 'model_26_fold_3': 0.0},\n",
       " 27: {'model_27_fold_1': 0.0,\n",
       "  'model_27_fold_2': 0.0,\n",
       "  'model_27_fold_3': 61.22448979299459},\n",
       " 28: {'model_28_fold_1': 0.0,\n",
       "  'model_28_fold_2': 0.0,\n",
       "  'model_28_fold_3': 61.22448979299459},\n",
       " 29: {'model_29_fold_1': 0.0, 'model_29_fold_2': 0.0, 'model_29_fold_3': 0.0},\n",
       " 30: {'model_30_fold_1': 0.0, 'model_30_fold_2': 0.0, 'model_30_fold_3': 0.0},\n",
       " 31: {'model_31_fold_1': 0.0,\n",
       "  'model_31_fold_2': 53.731343281585204,\n",
       "  'model_31_fold_3': 46.15384615207101},\n",
       " 32: {'model_32_fold_1': 51.51515151320799,\n",
       "  'model_32_fold_2': 0.0,\n",
       "  'model_32_fold_3': 34.14634146128495}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict_all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_23': 70.62074829539326,\n",
       " 'model_1': 65.75963718511521,\n",
       " 'model_19': 64.0502354750173,\n",
       " 'model_7': 50.51569014394443,\n",
       " 'model_24': 43.537414963916696,\n",
       " 'model_20': 43.537414963916696,\n",
       " 'model_2': 39.25828395700713,\n",
       " 'model_31': 33.295063144552074,\n",
       " 'model_32': 28.553830991497648,\n",
       " 'model_3': 27.425707123179176,\n",
       " 'model_15': 21.333333331481604,\n",
       " 'model_28': 20.40816326433153,\n",
       " 'model_6': 20.40816326433153,\n",
       " 'model_4': 20.40816326433153,\n",
       " 'model_11': 20.40816326433153,\n",
       " 'model_27': 20.40816326433153,\n",
       " 'model_22': 15.873015872391031,\n",
       " 'model_14': 15.38461538402367,\n",
       " 'model_8': 15.38461538402367,\n",
       " 'model_13': 15.38461538402367,\n",
       " 'model_21': 15.38461538402367,\n",
       " 'model_30': 0.0,\n",
       " 'model_26': 0.0,\n",
       " 'model_18': 0.0,\n",
       " 'model_16': 0.0,\n",
       " 'model_12': 0.0,\n",
       " 'model_10': 0.0,\n",
       " 'model_5': 0.0,\n",
       " 'model_9': 0.0,\n",
       " 'model_17': 0.0,\n",
       " 'model_25': 0.0,\n",
       " 'model_29': 0.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_average_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_1': <__main__.NeuralNetwork at 0x7f7e301b5690>,\n",
       " 'model_2': <__main__.NeuralNetwork at 0x7f7e301b5510>,\n",
       " 'model_3': <__main__.NeuralNetwork at 0x7f7e301b5b90>,\n",
       " 'model_4': <__main__.NeuralNetwork at 0x7f7e30103390>,\n",
       " 'model_5': <__main__.NeuralNetwork at 0x7f7e30103e10>,\n",
       " 'model_6': <__main__.NeuralNetwork at 0x7f7e301b5850>,\n",
       " 'model_7': <__main__.NeuralNetwork at 0x7f7e301b50d0>,\n",
       " 'model_8': <__main__.NeuralNetwork at 0x7f7e301b5dd0>,\n",
       " 'model_9': <__main__.NeuralNetwork at 0x7f7e301b5490>,\n",
       " 'model_10': <__main__.NeuralNetwork at 0x7f7e2fdcc0d0>,\n",
       " 'model_11': <__main__.NeuralNetwork at 0x7f7e2fdcc9d0>,\n",
       " 'model_12': <__main__.NeuralNetwork at 0x7f7e2fdccc50>,\n",
       " 'model_13': <__main__.NeuralNetwork at 0x7f7e2fdccf10>,\n",
       " 'model_14': <__main__.NeuralNetwork at 0x7f7e2fdcc990>,\n",
       " 'model_15': <__main__.NeuralNetwork at 0x7f7e2fdccfd0>,\n",
       " 'model_16': <__main__.NeuralNetwork at 0x7f7e2fdcce10>,\n",
       " 'model_17': <__main__.NeuralNetwork at 0x7f7e2fdd8210>,\n",
       " 'model_18': <__main__.NeuralNetwork at 0x7f7e2fdd8810>,\n",
       " 'model_19': <__main__.NeuralNetwork at 0x7f7e2fdccd50>,\n",
       " 'model_20': <__main__.NeuralNetwork at 0x7f7e2fdd8a50>,\n",
       " 'model_21': <__main__.NeuralNetwork at 0x7f7e2fdd8990>,\n",
       " 'model_22': <__main__.NeuralNetwork at 0x7f7e2fdd8fd0>,\n",
       " 'model_23': <__main__.NeuralNetwork at 0x7f7e2fdd8f50>,\n",
       " 'model_24': <__main__.NeuralNetwork at 0x7f7e2fdd8bd0>,\n",
       " 'model_25': <__main__.NeuralNetwork at 0x7f7e2fdd8f10>,\n",
       " 'model_26': <__main__.NeuralNetwork at 0x7f7e2fdd8dd0>,\n",
       " 'model_27': <__main__.NeuralNetwork at 0x7f7e2fdd3390>,\n",
       " 'model_28': <__main__.NeuralNetwork at 0x7f7e2fdd3690>,\n",
       " 'model_29': <__main__.NeuralNetwork at 0x7f7e2fdd32d0>,\n",
       " 'model_30': <__main__.NeuralNetwork at 0x7f7e2fdd3550>,\n",
       " 'model_31': <__main__.NeuralNetwork at 0x7f7e2fdd3ad0>,\n",
       " 'model_32': <__main__.NeuralNetwork at 0x7f7e2fdd3310>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer: (2, 'tanh')\n",
      "Hidden layer: [(4, 'sigmoid'), (4, 'softmax')]\n",
      "Output layer: 3\n",
      "Batch size: 64\n",
      "Learning rate: 4\n",
      "Epoch: 10\n",
      "Seed: 42\n",
      "Verbose: True\n",
      "Metric: accuracy\n"
     ]
    }
   ],
   "source": [
    "print(models[\"model_32\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calculate_model_performance(\n",
    "    np.argmax(Y, axis=0),\n",
    "    models[\"model_5\"].predict(X)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "dt = data.data[:,[0,2]]\n",
    "x_min, x_max = dt[:, 0].min() - 1, dt[:, 0].max() + 1\n",
    "y_min, y_max = dt[:, 1].min() - 1, dt[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = models[\"model_5\"].predict(np.c_[xx.ravel(), yy.ravel()].T) \n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx, yy, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "\n",
    "plt.scatter(dt[:, 0], dt[:, 1], c=y,s=20, edgecolor='k')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('petal length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Make Moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "x,y =make_moons(n_samples=1500, noise=.05)\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(2, \"sigmoid\"),\n",
    "    hidden_layer=[(8, \"tanh\"),\n",
    "                  (6, \"relu\"),\n",
    "                  (4, \"softmax\")],\n",
    "    output_layer=2,\n",
    "    batch_size=64,\n",
    "    alpha=0.5,\n",
    "    epoch=2500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Decision Boundaries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "dt = x\n",
    "x_min, x_max = dt[:, 0].min() - 0.5, dt[:, 0].max() + 0.5\n",
    "y_min, y_max = dt[:, 1].min() - 0.5, dt[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()].T) \n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx, yy, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "plt.scatter(dt[:, 0], dt[:, 1], c=y, s=20, edgecolor='k')\n",
    "plt.title('Decision Boundaries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Andrew NG Assignment 2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex2data2 = np.loadtxt(\"../ex2/data/ex2data2.txt\", delimiter=\",\")\n",
    "\n",
    "X = ex2data2[:, :-1]\n",
    "y = ex2data2[:, -1]\n",
    "\n",
    "X = X.T\n",
    "Y = one_hot_encode(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(2, \"relu\"),\n",
    "    hidden_layer=[(4, \"relu\"),\n",
    "                  (4, \"softmax\")],\n",
    "    output_layer=2,\n",
    "    batch_size=64,\n",
    "    alpha=0.5,\n",
    "    epoch=25000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_metrics = calculate_model_performance(\n",
    "    np.argmax(Y, axis=0),\n",
    "    model.predict(X)\n",
    ")\n",
    "\n",
    "model_performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decision Boundaries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "X = ex2data2\n",
    "\n",
    "x1_min, x1_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3,\n",
    "x2_min, x2_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3,\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx1.ravel(), xx2.ravel()].T) \n",
    "\n",
    "negatives = ex2data2[ex2data2[:, -1] == 0]\n",
    "positives = ex2data2[ex2data2[:, -1] == 1]\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx1.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx1, xx2, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "plt.scatter(negatives[:, 0], negatives[:, 1],s=50, color='k')\n",
    "plt.scatter(positives[:, 0], positives[:, 1],s=50, color='r')\n",
    "plt.title('Decision Boundaries')\n",
    "\n",
    "plt.contour(xx1, xx2, Z, [0.5], linewidths=2, colors=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "data = loadmat('../ex3/data/ex3data1.mat')\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"X\"]\n",
    "y = data[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, validation = split_data_as(x,y, train=0.6, test=0.2, validation=0.2)\n",
    "\n",
    "X_train = train[:, :-1].T\n",
    "Y_train = one_hot_encode(train[:, -1]).T\n",
    "\n",
    "X_test = test[:, :-1].T\n",
    "Y_test = one_hot_encode(test[:, -1]).T\n",
    "\n",
    "X_validation = validation[:, :-1].T\n",
    "Y_validation = one_hot_encode(validation[:, -1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(\"\\n\")\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(\"\\n\")\n",
    "print(X_validation.shape)\n",
    "print(Y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "sample = np.random.choice(data[\"X\"].shape[0], 20)\n",
    "ax.imshow(data[\"X\"][sample,1:].reshape(-1,20).T)\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(X_train.shape[0], \"relu\"),\n",
    "    hidden_layer=[(10, \"relu\"),\n",
    "                  (10, \"softmax\")],\n",
    "    output_layer=Y_train.shape[0],\n",
    "    batch_size=16,\n",
    "    alpha=0.2,\n",
    "    epoch=500,\n",
    "    random_state=12\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(model.predict(X_train) == np.argmax(Y_train,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_model_performance(\n",
    "    np.argmax(Y_train,axis=0),\n",
    "    model.predict(X_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, (act, predicted) in enumerate(zip(np.argmax(Y_train,axis=0), model.predict(X_train))):\n",
    "    if act != predicted:\n",
    "        fig, ax = plt.subplots(figsize = (2,2))\n",
    "        ax.set_title(\"%s: act %s --- predicted %s\" %(index, act + 1, predicted + 1))\n",
    "        ax.imshow(X_train[:, index].reshape(-1,20).T)\n",
    "        ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_model_performance(\n",
    "    np.argmax(Y_test,axis=0),\n",
    "    model.predict(X_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, (act, predicted) in enumerate(zip(np.argmax(Y_test,axis=0), model.predict(X_test))):\n",
    "    if act != predicted:\n",
    "        fig, ax = plt.subplots(figsize = (2,2))\n",
    "        ax.set_title(\"%s: act %s --- predicted %s\" %(index, act + 1, predicted + 1))\n",
    "        ax.imshow(X_test[:, index].reshape(-1,20).T)\n",
    "        ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
