{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utility_functions import (calculate_model_performance,\n",
    "                               plot_ROC,\n",
    "                               one_hot_encode,\n",
    "                               split_data_as,\n",
    "                               grid_search,\n",
    "                               shuffled,\n",
    "                               timeit)\n",
    "\n",
    "EPSILON = 10e-08\n",
    "\n",
    "\n",
    "def get_shapes(any_):\n",
    "    for array in any_:\n",
    "        try:\n",
    "            print(array.shape)\n",
    "        except:\n",
    "            print(\"NONE\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# ============= ACTIVATION FUNCTIONS ===============#\n",
    "\n",
    "def sigmoid(Z, prime=False):\n",
    "    # np.\n",
    "    if prime:\n",
    "        return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "\n",
    "def linear(Z, prime=False):\n",
    "    if prime:\n",
    "        return np.ones_like(Z)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def relu(Z, alpha=0.01, prime=False):\n",
    "    if prime:\n",
    "        Z_relu = np.ones_like(Z, dtype=np.float64)\n",
    "        Z_relu[Z < 0] = alpha\n",
    "        return Z_relu\n",
    "    return np.where(Z < 0, alpha * Z, Z)\n",
    "\n",
    "\n",
    "def tanh(Z, prime=False):\n",
    "    # np.tanh() could be used directly to speed this up\n",
    "    if prime:\n",
    "        return 1 - np.power(tanh(Z), 2)\n",
    "    return (2 / (1 + np.exp(-2 * Z))) - 1\n",
    "\n",
    "\n",
    "def elu(Z, prime=False):\n",
    "    # https://mlfromscratch.com/activation-functions-explained/#/\n",
    "    alpha = 0.2\n",
    "    if prime:\n",
    "        return np.where(Z < 0, alpha * (np.exp(Z)), 1)\n",
    "    return np.where(Z < 0, alpha * (np.exp(Z) - 1), Z)\n",
    "\n",
    "\n",
    "def softmax(Z, prime=False):\n",
    "    # https://deepnotes.io/softmax-crossentropy\n",
    "    # max(Z) term is added to stabilise the function.\n",
    "    exps = np.exp(Z - np.max(Z))\n",
    "    return exps / np.sum(exps, axis=0)\n",
    "\n",
    "\n",
    "# References\n",
    "# https://mc.ai/multilayered-neural-network-from-scratch-using-python/\n",
    "# https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "# https://www.coursera.org/learn/machine-learning/home/week/5\n",
    "# https://www.coursera.org/specializations/deep-learning\n",
    "# https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py\n",
    "# https://github.com/JWarmenhoven/Coursera-Machine-Learning\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_layer: tuple,\n",
    "            hidden_layer: list,  # list of tuples\n",
    "            output_layer: int,\n",
    "            batch_size=16,\n",
    "            alpha=1,\n",
    "            optimizer=\"SGD\",\n",
    "            penalty=\"l2\",\n",
    "            lambd=\"0.1\",\n",
    "            keep_prob = None,\n",
    "            epoch=500,\n",
    "            random_state=42,\n",
    "            verbose=True,\n",
    "            metrics=\"accuracy\"\n",
    "    ):\n",
    "        self.input_layer = input_layer\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.output_layer = output_layer\n",
    "        self.mini_batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.optimizer = optimizer\n",
    "        self.penalty = penalty\n",
    "        self.lambd = lambd\n",
    "        self.keep_prob = keep_prob\n",
    "        # dropout: http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf\n",
    "        self.dropout = True if isinstance(self.keep_prob, float) else False\n",
    "        self.epoch = epoch\n",
    "        self.seed = random_state\n",
    "        self.verbose = verbose\n",
    "        self.metrics = metrics\n",
    "        self.layers = len(self.weight_set_dimensions) + 1\n",
    "        self.EPSILON = 10e-10\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        parameters = (\n",
    "            \"Input layer: {0}\\n\"\n",
    "            \"Hidden layer: {1}\\n\"\n",
    "            \"Output layer: {2}\\n\"\n",
    "            \"Batch size: {3}\\n\"\n",
    "            \"Learning rate: {4}\\n\"\n",
    "            \"Epoch: {5}\\n\"\n",
    "            \"Seed: {6}\\n\"\n",
    "            \"Verbose: {7}\\n\"\n",
    "            \"Metric: {8}\"\n",
    "        ).format(\n",
    "            self.input_layer,\n",
    "            \" - \".join(map(str, self.hidden_layer)),\n",
    "            self.output_layer,\n",
    "            self.mini_batch_size,\n",
    "            self.alpha,\n",
    "            self.epoch,\n",
    "            self.seed,\n",
    "            self.verbose,\n",
    "            self.metrics\n",
    "        )\n",
    "        return parameters\n",
    "\n",
    "    def get_A(self, X):\n",
    "        A, _ = self.forwardpass(X)\n",
    "        return A\n",
    "\n",
    "    def get_Z(self, X):\n",
    "        _, Z = self.forwardpass(X)\n",
    "        return Z\n",
    "    \n",
    "    # ============== LOSS FUNCTIONS ===============#\n",
    "\n",
    "    # https://deepnotes.io/softmax-crossentropy\n",
    "\n",
    "    def calculate_error(self, Y, Y_hat):\n",
    "        # Y and Y_hat should be in the form of (no_of_classes, no_of_training_examples)\n",
    "        cost = -np.sum(Y * np.log(Y_hat + EPSILON)) / self.m\n",
    "        penalise_by = 0\n",
    "        if self.penalty == \"l1\":\n",
    "            for layer in range(1, self.layers):\n",
    "                penalise_by += np.sum(np.abs(self.W[layer])) * self.lambd / (2 * self.m)\n",
    "            return cost + penalise_by\n",
    "        elif self.penalty == \"l2\":\n",
    "            for layer in range(1, self.layers):\n",
    "                penalise_by += np.sum(np.square(self.W[layer])) * self.lambd / (2 * self.m)\n",
    "            return cost + penalise_by\n",
    "        else:\n",
    "            return cost\n",
    "\n",
    "\n",
    "    def display_information(self, X, Y, epoch_no):\n",
    "        model_performance_metrics = calculate_model_performance(\n",
    "            np.argmax(Y, axis=0),\n",
    "            self.predict(X)\n",
    "        )\n",
    "        print(\"%s: %.10f - epoch %s    iteration %s - loss %.20f\" % (\n",
    "            self.metrics,\n",
    "            model_performance_metrics[self.metrics],\n",
    "            epoch_no,\n",
    "            self.no_of_iterations,\n",
    "            self.calculate_error(Y,\n",
    "                            self.get_A(X)[-1])\n",
    "        )\n",
    "              )\n",
    "\n",
    "    def get_dimensions_and_activations(self):\n",
    "        self.dimensions = []\n",
    "        self.activation_functions = []\n",
    "\n",
    "        self.dimensions.append(self.input_layer[0])\n",
    "        self.activation_functions.append(self.input_layer[1])\n",
    "\n",
    "        for dim, act_func in self.hidden_layer:\n",
    "            self.dimensions.append(dim)\n",
    "            self.activation_functions.append(act_func)\n",
    "\n",
    "        self.dimensions.append(self.output_layer)\n",
    "\n",
    "    @property\n",
    "    def weight_set_dimensions(self):\n",
    "        self.get_dimensions_and_activations()\n",
    "        a, b = itertools.tee(self.dimensions[::-1])\n",
    "        next(b, None)\n",
    "        weight_set_dimensions = list(zip(a, b))[::-1]\n",
    "        return weight_set_dimensions\n",
    "\n",
    "    def initialise_weights(self, layer=None):\n",
    "        self.W = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.B = np.empty_like(range(self.layers), dtype=object)\n",
    "        for layer, (y, x) in zip(range(1, self.layers), self.weight_set_dimensions):\n",
    "            np.random.seed(self.seed)\n",
    "            self.W[layer] = np.random.rand(y, x) / np.sqrt(self.dimensions[layer - 1])\n",
    "            self.B[layer] = np.random.rand(y, 1)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        Z = np.empty_like(range(self.layers), dtype=object)\n",
    "        A = np.empty_like(range(self.layers), dtype=object)\n",
    "        A[0] = X\n",
    "\n",
    "        for layer in range(1, self.layers):\n",
    "            # activation_function starts from 0 whereas layer starts from 1\n",
    "            active_function = self.activation_functions[layer - 1]\n",
    "            arg_to_pass_to_eval = \"(Z[layer])\"\n",
    "\n",
    "            Z[layer] = self.W[layer] @ A[layer - 1] + self.B[layer]\n",
    "            A[layer] = eval(active_function + arg_to_pass_to_eval)\n",
    "            \n",
    "            # dropout is only applied to first hidden layer\n",
    "            # https://www.kaggle.com/mtax687/dropout-regularization-of-neural-net-using-numpy\n",
    "            if self.dropout and layer == 1:\n",
    "                self.D = np.random.randn(A[layer].shape[0], A[layer].shape[1])\n",
    "                self.D = (self.D < self.keep_prob)\n",
    "                A[layer] = np.multiply(A[layer], self.D) / self.keep_prob\n",
    "        return A, Z\n",
    "\n",
    "    def backpropagation(self, Y, A, Z):\n",
    "        self.delta = np.empty_like(range(self.layers), dtype=object)\n",
    "\n",
    "        self.gradient_W = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.gradient_B = np.empty_like(range(self.layers), dtype=object)\n",
    "\n",
    "        self.delta[-1] = A[-1] - Y\n",
    "\n",
    "        # We substract 1 here as delta_final is calculated seperately above\n",
    "        for layer in reversed(range(1, self.layers - 1)):\n",
    "            # 1 is substracted from layer as activation_functions start indexing from 0\n",
    "            active_function = self.activation_functions[layer - 1]\n",
    "            arg_to_pass_to_eval = \"(Z[layer], prime=True)\"\n",
    "            \n",
    "            DA = self.W[layer + 1].T @ self.delta[layer + 1]\n",
    "            if self.dropout and layer == 1:\n",
    "                DA = np.multiply(DA, self.D) / self.keep_prob\n",
    "\n",
    "            self.delta[layer] = (\n",
    "                    DA *\n",
    "                    eval(active_function + arg_to_pass_to_eval)\n",
    "            )\n",
    "\n",
    "        for layer in range(1, self.layers):\n",
    "            self.gradient_W[layer] = (self.delta[layer] @ A[layer - 1].T) / self.m\n",
    "            self.gradient_B[layer] = np.sum(self.delta[layer], axis=1, keepdims=True) / self.m\n",
    "            \n",
    "            if self.penalty == \"l1\":\n",
    "                # https://towardsdatascience.com/only-numpy-implementing-different-combination-of-l1-norm-l2-norm-l1-regularization-and-14b01a9773b\n",
    "                self.gradient_W[layer] += np.where(self.W[layer] < 0, -1, 1) * (self.lambd / self.m)\n",
    "            elif self.penalty == \"l2\":\n",
    "                self.gradient_W[layer] += self.W[layer] * (self.lambd / self.m)\n",
    "            \n",
    "        self.update_weights()\n",
    "\n",
    "\n",
    "    def update_weights(self):\n",
    "        if self.optimizer == \"SGD\":\n",
    "            for layer in range(1, self.layers):\n",
    "                self.W[layer] -= self.alpha * self.gradient_W[layer]\n",
    "                self.B[layer] -= self.alpha * self.gradient_B[layer]\n",
    "\n",
    "        elif self.optimizer[\"method\"] == \"SGDM\":\n",
    "            for layer in range(1, self.layers):\n",
    "                beta = self.optimizer[\"beta\"]\n",
    "                self.v_dw[layer] = beta * self.v_dw[layer] + (1 - beta) * self.gradient_W[layer]\n",
    "                self.v_db[layer] = beta * self.v_db[layer] + (1 - beta) * self.gradient_B[layer]\n",
    "\n",
    "                self.W[layer] -= self.alpha * self.v_dw[layer]\n",
    "                self.B[layer] -= self.alpha * self.v_db[layer]\n",
    "\n",
    "        elif self.optimizer[\"method\"] == \"RMSP\":\n",
    "            for layer in range(1, self.layers):\n",
    "                beta = self.optimizer[\"beta\"]\n",
    "                self.s_dw[layer] = beta * self.s_dw[layer] + (1 - beta) * np.square(self.gradient_W[layer])\n",
    "                self.s_db[layer] = beta * self.s_db[layer] + (1 - beta) * np.square(self.gradient_B[layer])\n",
    "\n",
    "                w_rms_grad = self.gradient_W[layer] / (np.sqrt(self.s_dw[layer]) + self.EPSILON)\n",
    "                b_rms_grad = self.gradient_B[layer] / (np.sqrt(self.s_db[layer]) + self.EPSILON)\n",
    "\n",
    "                self.W[layer] -= self.alpha * w_rms_grad\n",
    "                self.B[layer] -= self.alpha * b_rms_grad\n",
    "\n",
    "        elif self.optimizer[\"method\"] == \"ADAM\":\n",
    "            # EWA: Exponential weighted average\n",
    "            # ToDo: Check if bias correction is necessary. The EWA will be inaccurate initially,\n",
    "            # but it shouldn't take many iterations to compute correct EWA.\n",
    "            for layer in range(1, self.layers):\n",
    "                beta1 = self.optimizer[\"beta1\"]\n",
    "                beta2 = self.optimizer[\"beta2\"]\n",
    "                self.v_dw[layer] = beta1 * self.v_dw[layer] + (1 - beta1) * self.gradient_W[layer]\n",
    "                self.v_db[layer] = beta1 * self.v_db[layer] + (1 - beta1) * self.gradient_B[layer]\n",
    "\n",
    "                self.s_dw[layer] = beta2 * self.s_dw[layer] + (1 - beta2) * np.square(self.gradient_W[layer])\n",
    "                self.s_db[layer] = beta2 * self.s_db[layer] + (1 - beta2) * np.square(self.gradient_B[layer])\n",
    "\n",
    "                v_dw_corrected = self.v_dw[layer] / (1 - beta1 ** self.no_of_iterations)\n",
    "                s_dw_corrected = self.s_dw[layer] / (1 - beta2 ** self.no_of_iterations)\n",
    "\n",
    "                v_db_corrected = self.v_db[layer] / (1 - beta1 ** self.no_of_iterations)\n",
    "                s_db_corrected = self.s_db[layer] / (1 - beta2 ** self.no_of_iterations)\n",
    "\n",
    "                self.W[layer] -= self.alpha * (v_dw_corrected / (np.sqrt(s_dw_corrected) + self.EPSILON))\n",
    "                self.B[layer] -= self.alpha * (v_db_corrected / (np.sqrt(s_db_corrected) + self.EPSILON))\n",
    "\n",
    "\n",
    "    def initialise_cache(self):\n",
    "        self.v_dw = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.v_db = np.empty_like(range(self.layers), dtype=object)\n",
    "    \n",
    "        self.s_dw = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.s_db = np.empty_like(range(self.layers), dtype=object)\n",
    "\n",
    "        for layer, (y, x) in zip(range(1, self.layers), self.weight_set_dimensions):\n",
    "            self.v_dw[layer] = np.zeros((y, x))\n",
    "            self.v_db[layer] = np.zeros((y, 1))\n",
    "            \n",
    "            self.s_dw[layer] = np.zeros((y, x))\n",
    "            self.s_db[layer] = np.zeros((y, 1))\n",
    "\n",
    "\n",
    "    @timeit\n",
    "    def fit(self, X, Y):\n",
    "        self.m = X.shape[1] # where (no_of_features, no_of_training_examples)\n",
    "        self.initialise_weights()\n",
    "        self.initialise_cache()\n",
    "\n",
    "        # By default the method is SGD(Stochastic Gradient Descent) if one wishes to use\n",
    "        # the whole batch, simply pass the number of traning examples available as the\n",
    "        # batch size when instantiating the class\n",
    "        self.no_of_iterations = 0\n",
    "        shuffled = np.arange(self.m)\n",
    "        if self.verbose:\n",
    "            print(\"Initialising weights...\")\n",
    "            print(\"Starting the training...\")\n",
    "            print(\"Initial cost: %.10f\\n\" % self.calculate_error(Y, self.get_A(X)[-1]))\n",
    "        for epoch_no in range(1, self.epoch + 1):\n",
    "            np.random.shuffle(shuffled)\n",
    "            X_shuffled = X[:, shuffled]\n",
    "            Y_shuffled = Y[:, shuffled]\n",
    "            for i in range(0, self.m, self.mini_batch_size):\n",
    "                self.no_of_iterations += 1\n",
    "                X_mini_batch = X_shuffled[:, i: i + self.mini_batch_size]\n",
    "                Y_mini_batch = Y_shuffled[:, i: i + self.mini_batch_size]\n",
    "\n",
    "                A, Z = self.forwardpass(X_mini_batch)\n",
    "                self.backpropagation(Y_mini_batch, A, Z)\n",
    "                if self.no_of_iterations % 100 == 0 and self.verbose:\n",
    "                    self.display_information(X, Y, epoch_no)\n",
    "\n",
    "    def predict(\n",
    "            self,\n",
    "            X: np.ndarray,\n",
    "            return_prob_matrix=False\n",
    "    ):\n",
    "        \"\"\"Predict the output given the training data.\n",
    "\n",
    "            Returns the predicted values in two forms:\n",
    "\n",
    "            1.either by picking up the highest value along the columns for every row,\n",
    "                i.e. \"np.argmax(self.A[-1].T, axis=1)\"\n",
    "            2.or by returning a matrix that is in the shape of Y.T where each column\n",
    "                represents the probability of the instance belonging to that class.\n",
    "                Please note that every column in Y.T represents a class. To be able to\n",
    "                return the probability matrix, the final activation function must be\n",
    "                softmax!\n",
    "                i.e. \"array([0.9650488423, 0.0354737543, 0.0005225966])\"\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Training set in the shape of\n",
    "                (no_of_features, no_of_training examples).\n",
    "            return_prob_matrix (bool, optional): Returns the probability matrix if True.\n",
    "                Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray:\n",
    "\n",
    "            if return_prob_matrix is False, the output is in the shape of\n",
    "                (no_of_training_examples, 1)\n",
    "            if return_prob_matrix is True, the output is in the shape of\n",
    "                (no_of_training_examples, no_of_features)\n",
    "        \"\"\"\n",
    "        A, Z = self.forwardpass(X)\n",
    "        if return_prob_matrix:\n",
    "            np.set_printoptions(precision=10, suppress=True)\n",
    "            return A[-1].T\n",
    "        return np.argmax(A[-1].T, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with benchmark datasets\n",
    "\n",
    "## 1.Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "x = data.data[:,[0,2]]\n",
    "y = data.target\n",
    "\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 150)\n",
      "(3, 150)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.6587031456\n",
      "\n",
      "accuracy: 66.6666666662 - epoch 6    iteration 100 - loss 0.75586593755764930336\n",
      "accuracy: 71.9999999995 - epoch 11    iteration 200 - loss 0.55747452395122465418\n",
      "accuracy: 81.3333333328 - epoch 16    iteration 300 - loss 0.48229831076182466676\n",
      "accuracy: 81.9999999995 - epoch 22    iteration 400 - loss 0.45386067197895857417\n",
      "accuracy: 84.6666666661 - epoch 27    iteration 500 - loss 0.37294113968164377404\n",
      "accuracy: 83.3333333328 - epoch 32    iteration 600 - loss 0.34163518121146330131\n",
      "accuracy: 84.6666666661 - epoch 37    iteration 700 - loss 0.31364021128498503765\n",
      "accuracy: 85.3333333328 - epoch 43    iteration 800 - loss 0.46334548920048407306\n",
      "accuracy: 89.3333333327 - epoch 48    iteration 900 - loss 0.25348162351330327802\n",
      "accuracy: 89.3333333327 - epoch 53    iteration 1000 - loss 0.29836301239307472244\n",
      "accuracy: 90.6666666661 - epoch 58    iteration 1100 - loss 0.23936236395863313975\n",
      "accuracy: 91.3333333327 - epoch 64    iteration 1200 - loss 0.30790080843985090375\n",
      "accuracy: 89.9999999994 - epoch 69    iteration 1300 - loss 0.24429024794727530190\n",
      "accuracy: 91.9999999994 - epoch 74    iteration 1400 - loss 0.28807961137514526140\n",
      "accuracy: 89.9999999994 - epoch 79    iteration 1500 - loss 0.18687946617104089775\n",
      "accuracy: 89.3333333327 - epoch 85    iteration 1600 - loss 0.23039600138453830702\n",
      "accuracy: 84.6666666661 - epoch 90    iteration 1700 - loss 0.24170566428001094561\n",
      "accuracy: 93.3333333327 - epoch 95    iteration 1800 - loss 0.20077696394812022396\n",
      "accuracy: 88.6666666661 - epoch 100    iteration 1900 - loss 0.24061602124780734346\n",
      "accuracy: 91.3333333327 - epoch 106    iteration 2000 - loss 0.24098159200740146280\n",
      "accuracy: 91.3333333327 - epoch 111    iteration 2100 - loss 0.19496875830071552782\n",
      "accuracy: 88.6666666661 - epoch 116    iteration 2200 - loss 0.27954327549120505658\n",
      "accuracy: 92.6666666660 - epoch 122    iteration 2300 - loss 0.21742966413639047696\n",
      "accuracy: 93.9999999994 - epoch 127    iteration 2400 - loss 0.17907057467052353572\n",
      "accuracy: 87.9999999994 - epoch 132    iteration 2500 - loss 0.25703560470764369450\n",
      "accuracy: 93.9999999994 - epoch 137    iteration 2600 - loss 0.17879429616391812052\n",
      "accuracy: 91.3333333327 - epoch 143    iteration 2700 - loss 0.22605422163845900752\n",
      "accuracy: 90.6666666661 - epoch 148    iteration 2800 - loss 0.20539745307309340006\n",
      "accuracy: 89.9999999994 - epoch 153    iteration 2900 - loss 0.18801849798969844096\n",
      "accuracy: 85.9999999994 - epoch 158    iteration 3000 - loss 0.37835953253781684502\n",
      "accuracy: 89.9999999994 - epoch 164    iteration 3100 - loss 0.16907664382296411221\n",
      "accuracy: 91.9999999994 - epoch 169    iteration 3200 - loss 0.18055631151074116869\n",
      "accuracy: 91.3333333327 - epoch 174    iteration 3300 - loss 0.21593229305199154733\n",
      "accuracy: 85.9999999994 - epoch 179    iteration 3400 - loss 0.19276277789739160173\n",
      "accuracy: 89.9999999994 - epoch 185    iteration 3500 - loss 0.25378015333428471934\n",
      "accuracy: 93.3333333327 - epoch 190    iteration 3600 - loss 0.18091156105516048580\n",
      "accuracy: 90.6666666661 - epoch 195    iteration 3700 - loss 0.26304390071976241350\n",
      "accuracy: 92.6666666660 - epoch 200    iteration 3800 - loss 0.21309003677254653719\n",
      "accuracy: 92.6666666660 - epoch 206    iteration 3900 - loss 0.25654944907740973026\n",
      "accuracy: 94.6666666660 - epoch 211    iteration 4000 - loss 0.17404374295691196761\n",
      "accuracy: 91.3333333327 - epoch 216    iteration 4100 - loss 0.25731312164335101711\n",
      "accuracy: 91.9999999994 - epoch 222    iteration 4200 - loss 0.18046351368815083394\n",
      "accuracy: 88.6666666661 - epoch 227    iteration 4300 - loss 0.34035471034882791574\n",
      "accuracy: 93.3333333327 - epoch 232    iteration 4400 - loss 0.19917854098609039104\n",
      "accuracy: 91.9999999994 - epoch 237    iteration 4500 - loss 0.18828369663870009321\n",
      "accuracy: 91.3333333327 - epoch 243    iteration 4600 - loss 0.19824136610717546514\n",
      "accuracy: 91.3333333327 - epoch 248    iteration 4700 - loss 0.14071408920919040164\n",
      "accuracy: 91.3333333327 - epoch 253    iteration 4800 - loss 0.22613688143605717440\n",
      "accuracy: 94.6666666660 - epoch 258    iteration 4900 - loss 0.15988853593698165723\n",
      "accuracy: 88.6666666661 - epoch 264    iteration 5000 - loss 0.22038816334825925281\n",
      "accuracy: 90.6666666661 - epoch 269    iteration 5100 - loss 0.20089253678505608613\n",
      "accuracy: 90.6666666661 - epoch 274    iteration 5200 - loss 0.17500212385261287862\n",
      "accuracy: 90.6666666661 - epoch 279    iteration 5300 - loss 0.25380298956080316053\n",
      "accuracy: 94.6666666660 - epoch 285    iteration 5400 - loss 0.18087311904622316305\n",
      "accuracy: 93.9999999994 - epoch 290    iteration 5500 - loss 0.17999258856353630986\n",
      "accuracy: 92.6666666660 - epoch 295    iteration 5600 - loss 0.24451325254415631760\n",
      "accuracy: 93.3333333327 - epoch 300    iteration 5700 - loss 0.22453656961402951309\n",
      "accuracy: 90.6666666661 - epoch 306    iteration 5800 - loss 0.23419310182229555184\n",
      "accuracy: 95.3333333327 - epoch 311    iteration 5900 - loss 0.15847036269667968966\n",
      "accuracy: 93.9999999994 - epoch 316    iteration 6000 - loss 0.12588827703506391686\n",
      "accuracy: 95.9999999994 - epoch 322    iteration 6100 - loss 0.15203795378243373237\n",
      "accuracy: 91.3333333327 - epoch 327    iteration 6200 - loss 0.18087344107179287311\n",
      "accuracy: 94.6666666660 - epoch 332    iteration 6300 - loss 0.19548858034171370801\n",
      "accuracy: 93.9999999994 - epoch 337    iteration 6400 - loss 0.14357226208635925913\n",
      "accuracy: 91.9999999994 - epoch 343    iteration 6500 - loss 0.21608160511792248726\n",
      "accuracy: 88.6666666661 - epoch 348    iteration 6600 - loss 0.23854875813298132003\n",
      "accuracy: 91.3333333327 - epoch 353    iteration 6700 - loss 0.19449352734867475512\n",
      "accuracy: 92.6666666660 - epoch 358    iteration 6800 - loss 0.11698547075949751461\n",
      "accuracy: 92.6666666660 - epoch 364    iteration 6900 - loss 0.22915730927905039738\n",
      "accuracy: 83.3333333328 - epoch 369    iteration 7000 - loss 0.47126026698391104697\n",
      "accuracy: 94.6666666660 - epoch 374    iteration 7100 - loss 0.17883437211867422856\n",
      "accuracy: 88.6666666661 - epoch 379    iteration 7200 - loss 0.30684933806131758915\n",
      "accuracy: 91.9999999994 - epoch 385    iteration 7300 - loss 0.21036887180560109867\n",
      "accuracy: 85.9999999994 - epoch 390    iteration 7400 - loss 0.32484771958414598858\n",
      "accuracy: 90.6666666661 - epoch 395    iteration 7500 - loss 0.17235761444941494203\n",
      "accuracy: 93.3333333327 - epoch 400    iteration 7600 - loss 0.18512385384328716720\n",
      "accuracy: 90.6666666661 - epoch 406    iteration 7700 - loss 0.19455688101588522465\n",
      "accuracy: 93.3333333327 - epoch 411    iteration 7800 - loss 0.20222759609817339554\n",
      "accuracy: 92.6666666660 - epoch 416    iteration 7900 - loss 0.25191897314606903624\n",
      "accuracy: 90.6666666661 - epoch 422    iteration 8000 - loss 0.23635564442045534084\n",
      "accuracy: 93.9999999994 - epoch 427    iteration 8100 - loss 0.17642582947633506096\n",
      "accuracy: 92.6666666660 - epoch 432    iteration 8200 - loss 0.12877365009935093365\n",
      "accuracy: 91.9999999994 - epoch 437    iteration 8300 - loss 0.16283804909018126383\n",
      "accuracy: 87.9999999994 - epoch 443    iteration 8400 - loss 0.25988426279647780515\n",
      "accuracy: 91.3333333327 - epoch 448    iteration 8500 - loss 0.17284727449438941349\n",
      "accuracy: 92.6666666660 - epoch 453    iteration 8600 - loss 0.15860938832738968873\n",
      "accuracy: 90.6666666661 - epoch 458    iteration 8700 - loss 0.14120226135630120634\n",
      "accuracy: 91.3333333327 - epoch 464    iteration 8800 - loss 0.23050626832816997758\n",
      "accuracy: 93.3333333327 - epoch 469    iteration 8900 - loss 0.19249092153127403404\n",
      "accuracy: 93.3333333327 - epoch 474    iteration 9000 - loss 0.18920244040882386316\n",
      "accuracy: 91.9999999994 - epoch 479    iteration 9100 - loss 0.17973878923689570741\n",
      "accuracy: 93.9999999994 - epoch 485    iteration 9200 - loss 0.17898963886769928000\n",
      "accuracy: 94.6666666660 - epoch 490    iteration 9300 - loss 0.19996098153818675502\n",
      "accuracy: 93.3333333327 - epoch 495    iteration 9400 - loss 0.27503983742452886574\n",
      "accuracy: 91.9999999994 - epoch 500    iteration 9500 - loss 0.17351505622587731770\n",
      "accuracy: 92.6666666660 - epoch 506    iteration 9600 - loss 0.16494580650667273192\n",
      "accuracy: 91.9999999994 - epoch 511    iteration 9700 - loss 0.21403452811692000313\n",
      "accuracy: 89.9999999994 - epoch 516    iteration 9800 - loss 0.16623493948544043608\n",
      "accuracy: 94.6666666660 - epoch 522    iteration 9900 - loss 0.14471934397324423549\n",
      "accuracy: 90.6666666661 - epoch 527    iteration 10000 - loss 0.20408535811022004980\n",
      "accuracy: 91.3333333327 - epoch 532    iteration 10100 - loss 0.19818954928015788952\n",
      "accuracy: 93.3333333327 - epoch 537    iteration 10200 - loss 0.15919221199332123096\n",
      "accuracy: 89.9999999994 - epoch 543    iteration 10300 - loss 0.24815052442825372148\n",
      "accuracy: 95.3333333327 - epoch 548    iteration 10400 - loss 0.21039547136587130605\n",
      "accuracy: 91.9999999994 - epoch 553    iteration 10500 - loss 0.22127750673725690711\n",
      "accuracy: 87.3333333328 - epoch 558    iteration 10600 - loss 0.31283245787435881580\n",
      "accuracy: 93.3333333327 - epoch 564    iteration 10700 - loss 0.18526566483530085283\n",
      "accuracy: 93.9999999994 - epoch 569    iteration 10800 - loss 0.19691677730255610634\n",
      "accuracy: 89.9999999994 - epoch 574    iteration 10900 - loss 0.23761848581152017479\n",
      "accuracy: 95.3333333327 - epoch 579    iteration 11000 - loss 0.17987121523652410682\n",
      "accuracy: 91.9999999994 - epoch 585    iteration 11100 - loss 0.14496920784650513259\n",
      "accuracy: 93.3333333327 - epoch 590    iteration 11200 - loss 0.19655012494787313382\n",
      "accuracy: 93.3333333327 - epoch 595    iteration 11300 - loss 0.15759158858610775034\n",
      "accuracy: 95.3333333327 - epoch 600    iteration 11400 - loss 0.17683303140190648195\n",
      "accuracy: 93.9999999994 - epoch 606    iteration 11500 - loss 0.15412526100902596005\n",
      "accuracy: 91.9999999994 - epoch 611    iteration 11600 - loss 0.23432003191074532444\n",
      "accuracy: 93.3333333327 - epoch 616    iteration 11700 - loss 0.14085991621948773522\n",
      "accuracy: 91.9999999994 - epoch 622    iteration 11800 - loss 0.17569760256571886892\n",
      "accuracy: 93.9999999994 - epoch 627    iteration 11900 - loss 0.20897040603188879460\n",
      "accuracy: 92.6666666660 - epoch 632    iteration 12000 - loss 0.18766480367695573261\n",
      "accuracy: 89.9999999994 - epoch 637    iteration 12100 - loss 0.13407544616412514493\n",
      "accuracy: 93.3333333327 - epoch 643    iteration 12200 - loss 0.19361711834632738816\n",
      "accuracy: 93.3333333327 - epoch 648    iteration 12300 - loss 0.18559974321971567379\n",
      "accuracy: 93.3333333327 - epoch 653    iteration 12400 - loss 0.20792805211152634581\n",
      "accuracy: 95.9999999994 - epoch 658    iteration 12500 - loss 0.22889669950697075218\n",
      "accuracy: 93.3333333327 - epoch 664    iteration 12600 - loss 0.15157103386107045884\n",
      "accuracy: 93.3333333327 - epoch 669    iteration 12700 - loss 0.18074640392936547340\n",
      "accuracy: 93.3333333327 - epoch 674    iteration 12800 - loss 0.13960709762317227711\n",
      "accuracy: 93.9999999994 - epoch 679    iteration 12900 - loss 0.14699528133313458089\n",
      "accuracy: 93.9999999994 - epoch 685    iteration 13000 - loss 0.13670973473823275213\n",
      "accuracy: 93.3333333327 - epoch 690    iteration 13100 - loss 0.12768570075572716060\n",
      "accuracy: 93.9999999994 - epoch 695    iteration 13200 - loss 0.14669324209612200072\n",
      "accuracy: 93.3333333327 - epoch 700    iteration 13300 - loss 0.12661628014937492681\n",
      "accuracy: 89.3333333327 - epoch 706    iteration 13400 - loss 0.15710214397763880290\n",
      "accuracy: 95.3333333327 - epoch 711    iteration 13500 - loss 0.15919306795262200271\n",
      "accuracy: 91.3333333327 - epoch 716    iteration 13600 - loss 0.17661175154805494603\n",
      "accuracy: 92.6666666660 - epoch 722    iteration 13700 - loss 0.20007085154777592972\n",
      "accuracy: 90.6666666661 - epoch 727    iteration 13800 - loss 0.21968753830129494897\n",
      "accuracy: 93.9999999994 - epoch 732    iteration 13900 - loss 0.21142202622320777072\n",
      "accuracy: 92.6666666660 - epoch 737    iteration 14000 - loss 0.17052512365242664116\n",
      "accuracy: 91.3333333327 - epoch 743    iteration 14100 - loss 0.16633518438082439683\n",
      "accuracy: 91.3333333327 - epoch 748    iteration 14200 - loss 0.22479227744876803530\n",
      "accuracy: 91.3333333327 - epoch 753    iteration 14300 - loss 0.12005165979455378888\n",
      "accuracy: 95.9999999994 - epoch 758    iteration 14400 - loss 0.15126595513438403917\n",
      "accuracy: 89.3333333327 - epoch 764    iteration 14500 - loss 0.19002675443665581478\n",
      "accuracy: 89.9999999994 - epoch 769    iteration 14600 - loss 0.18694133574028004574\n",
      "accuracy: 95.9999999994 - epoch 774    iteration 14700 - loss 0.11268279763048459630\n",
      "accuracy: 93.3333333327 - epoch 779    iteration 14800 - loss 0.18009688786588648468\n",
      "accuracy: 93.9999999994 - epoch 785    iteration 14900 - loss 0.16630742185604338590\n",
      "accuracy: 89.9999999994 - epoch 790    iteration 15000 - loss 0.16898430963491151147\n",
      "accuracy: 91.3333333327 - epoch 795    iteration 15100 - loss 0.15218260640817640739\n",
      "accuracy: 95.3333333327 - epoch 800    iteration 15200 - loss 0.08469523420718703954\n",
      "accuracy: 92.6666666660 - epoch 806    iteration 15300 - loss 0.16788645856147718649\n",
      "accuracy: 90.6666666661 - epoch 811    iteration 15400 - loss 0.15560118549735396654\n",
      "accuracy: 91.3333333327 - epoch 816    iteration 15500 - loss 0.21367232012918255246\n",
      "accuracy: 90.6666666661 - epoch 822    iteration 15600 - loss 0.17846243720258050947\n",
      "accuracy: 95.3333333327 - epoch 827    iteration 15700 - loss 0.15170115148334131772\n",
      "accuracy: 92.6666666660 - epoch 832    iteration 15800 - loss 0.11070317152546162165\n",
      "accuracy: 95.9999999994 - epoch 837    iteration 15900 - loss 0.19845938000797491707\n",
      "accuracy: 95.9999999994 - epoch 843    iteration 16000 - loss 0.12605655481665525852\n",
      "accuracy: 92.6666666660 - epoch 848    iteration 16100 - loss 0.17752269648581572858\n",
      "accuracy: 95.3333333327 - epoch 853    iteration 16200 - loss 0.15757612626297276548\n",
      "accuracy: 96.6666666660 - epoch 858    iteration 16300 - loss 0.13238961083078421010\n",
      "accuracy: 94.6666666660 - epoch 864    iteration 16400 - loss 0.14205701567393633211\n",
      "accuracy: 91.9999999994 - epoch 869    iteration 16500 - loss 0.13517413983428874791\n",
      "accuracy: 93.9999999994 - epoch 874    iteration 16600 - loss 0.15051093160180076258\n",
      "accuracy: 88.6666666661 - epoch 879    iteration 16700 - loss 0.15759235938947119271\n",
      "accuracy: 94.6666666660 - epoch 885    iteration 16800 - loss 0.17623625114875668407\n",
      "accuracy: 91.3333333327 - epoch 890    iteration 16900 - loss 0.15863369909847879757\n",
      "accuracy: 91.9999999994 - epoch 895    iteration 17000 - loss 0.12219646546489199013\n",
      "accuracy: 94.6666666660 - epoch 900    iteration 17100 - loss 0.17965787705466959578\n",
      "accuracy: 91.9999999994 - epoch 906    iteration 17200 - loss 0.17658639561451608402\n",
      "accuracy: 91.9999999994 - epoch 911    iteration 17300 - loss 0.28919341638498818892\n",
      "accuracy: 89.9999999994 - epoch 916    iteration 17400 - loss 0.13543388638272971236\n",
      "accuracy: 93.9999999994 - epoch 922    iteration 17500 - loss 0.17108429900390353939\n",
      "accuracy: 97.3333333327 - epoch 927    iteration 17600 - loss 0.15623977548027151441\n",
      "accuracy: 91.3333333327 - epoch 932    iteration 17700 - loss 0.17277289625651348626\n",
      "accuracy: 93.9999999994 - epoch 937    iteration 17800 - loss 0.16012360168277406380\n",
      "accuracy: 94.6666666660 - epoch 943    iteration 17900 - loss 0.18282518196764344087\n",
      "accuracy: 91.3333333327 - epoch 948    iteration 18000 - loss 0.16991858671917178802\n",
      "accuracy: 96.6666666660 - epoch 953    iteration 18100 - loss 0.15962240596016449845\n",
      "accuracy: 94.6666666660 - epoch 958    iteration 18200 - loss 0.14912027873976504977\n",
      "accuracy: 91.9999999994 - epoch 964    iteration 18300 - loss 0.19236455950342135068\n",
      "accuracy: 95.3333333327 - epoch 969    iteration 18400 - loss 0.14892334379755173268\n",
      "accuracy: 90.6666666661 - epoch 974    iteration 18500 - loss 0.16460331092990293511\n",
      "accuracy: 94.6666666660 - epoch 979    iteration 18600 - loss 0.14181211431444332161\n",
      "accuracy: 96.6666666660 - epoch 985    iteration 18700 - loss 0.13690499250072390991\n",
      "accuracy: 92.6666666660 - epoch 990    iteration 18800 - loss 0.17089894211543735225\n",
      "accuracy: 94.6666666660 - epoch 995    iteration 18900 - loss 0.18238890150893571906\n",
      "accuracy: 89.9999999994 - epoch 1000    iteration 19000 - loss 0.29849068401117989646\n",
      "accuracy: 92.6666666660 - epoch 1006    iteration 19100 - loss 0.14885230392696779944\n",
      "accuracy: 94.6666666660 - epoch 1011    iteration 19200 - loss 0.18093690129882145090\n",
      "accuracy: 95.9999999994 - epoch 1016    iteration 19300 - loss 0.12032366518491587060\n",
      "accuracy: 91.3333333327 - epoch 1022    iteration 19400 - loss 0.22495943719423949769\n",
      "accuracy: 93.9999999994 - epoch 1027    iteration 19500 - loss 0.14650126526783274761\n",
      "accuracy: 95.9999999994 - epoch 1032    iteration 19600 - loss 0.14737342649385576365\n",
      "accuracy: 94.6666666660 - epoch 1037    iteration 19700 - loss 0.13779653946977760137\n",
      "accuracy: 95.3333333327 - epoch 1043    iteration 19800 - loss 0.14399885459883307592\n",
      "accuracy: 94.6666666660 - epoch 1048    iteration 19900 - loss 0.14111631357419660016\n",
      "accuracy: 95.3333333327 - epoch 1053    iteration 20000 - loss 0.11513525890805619001\n",
      "accuracy: 91.9999999994 - epoch 1058    iteration 20100 - loss 0.25251257258570303232\n",
      "accuracy: 88.6666666661 - epoch 1064    iteration 20200 - loss 0.27276220937703460345\n",
      "accuracy: 92.6666666660 - epoch 1069    iteration 20300 - loss 0.26964772251484769860\n",
      "accuracy: 89.9999999994 - epoch 1074    iteration 20400 - loss 0.24508726104567127302\n",
      "accuracy: 92.6666666660 - epoch 1079    iteration 20500 - loss 0.18284930266516710717\n",
      "accuracy: 96.6666666660 - epoch 1085    iteration 20600 - loss 0.11024470927436351431\n",
      "accuracy: 93.9999999994 - epoch 1090    iteration 20700 - loss 0.19388924522860118138\n",
      "accuracy: 93.9999999994 - epoch 1095    iteration 20800 - loss 0.13270353920003868931\n",
      "accuracy: 94.6666666660 - epoch 1100    iteration 20900 - loss 0.16529119584705370682\n",
      "accuracy: 91.3333333327 - epoch 1106    iteration 21000 - loss 0.25454375661328865199\n",
      "accuracy: 93.3333333327 - epoch 1111    iteration 21100 - loss 0.17088344990152073977\n",
      "accuracy: 95.3333333327 - epoch 1116    iteration 21200 - loss 0.11673215545473207555\n",
      "accuracy: 93.9999999994 - epoch 1122    iteration 21300 - loss 0.17246268465596278907\n",
      "accuracy: 95.3333333327 - epoch 1127    iteration 21400 - loss 0.16931013026880517569\n",
      "accuracy: 95.9999999994 - epoch 1132    iteration 21500 - loss 0.14572919630705011285\n",
      "accuracy: 95.3333333327 - epoch 1137    iteration 21600 - loss 0.19352077160347366802\n",
      "accuracy: 86.6666666661 - epoch 1143    iteration 21700 - loss 0.31464971906018129877\n",
      "accuracy: 93.9999999994 - epoch 1148    iteration 21800 - loss 0.24060218408212705188\n",
      "accuracy: 89.9999999994 - epoch 1153    iteration 21900 - loss 0.12756711289402264398\n",
      "accuracy: 95.3333333327 - epoch 1158    iteration 22000 - loss 0.15135454451659963171\n",
      "accuracy: 91.9999999994 - epoch 1164    iteration 22100 - loss 0.17456183306086367479\n",
      "accuracy: 93.9999999994 - epoch 1169    iteration 22200 - loss 0.14142289797978488397\n",
      "accuracy: 91.9999999994 - epoch 1174    iteration 22300 - loss 0.20302109008638458132\n",
      "accuracy: 93.3333333327 - epoch 1179    iteration 22400 - loss 0.14261643391482045629\n",
      "accuracy: 91.3333333327 - epoch 1185    iteration 22500 - loss 0.18547250458577438370\n",
      "accuracy: 95.9999999994 - epoch 1190    iteration 22600 - loss 0.13854996295881383261\n",
      "accuracy: 92.6666666660 - epoch 1195    iteration 22700 - loss 0.12165856593188598134\n",
      "accuracy: 93.3333333327 - epoch 1200    iteration 22800 - loss 0.14506276316451655051\n",
      "accuracy: 90.6666666661 - epoch 1206    iteration 22900 - loss 0.21571463678028265920\n",
      "accuracy: 89.9999999994 - epoch 1211    iteration 23000 - loss 0.25335159939229212611\n",
      "accuracy: 92.6666666660 - epoch 1216    iteration 23100 - loss 0.24845682194552190514\n",
      "accuracy: 96.6666666660 - epoch 1222    iteration 23200 - loss 0.11814415631828485698\n",
      "accuracy: 95.3333333327 - epoch 1227    iteration 23300 - loss 0.15779451000289451690\n",
      "accuracy: 94.6666666660 - epoch 1232    iteration 23400 - loss 0.14792479414952172667\n",
      "accuracy: 90.6666666661 - epoch 1237    iteration 23500 - loss 0.27934282005209126742\n",
      "accuracy: 95.9999999994 - epoch 1243    iteration 23600 - loss 0.12432333739759582336\n",
      "accuracy: 93.3333333327 - epoch 1248    iteration 23700 - loss 0.14005265022920640461\n",
      "accuracy: 97.3333333327 - epoch 1253    iteration 23800 - loss 0.13376768098530050777\n",
      "accuracy: 95.3333333327 - epoch 1258    iteration 23900 - loss 0.13132934363583514958\n",
      "accuracy: 93.3333333327 - epoch 1264    iteration 24000 - loss 0.22117344698902116629\n",
      "accuracy: 95.3333333327 - epoch 1269    iteration 24100 - loss 0.12890252553344400122\n",
      "accuracy: 94.6666666660 - epoch 1274    iteration 24200 - loss 0.09888553316821488415\n",
      "accuracy: 93.3333333327 - epoch 1279    iteration 24300 - loss 0.17978594544933726440\n",
      "accuracy: 95.9999999994 - epoch 1285    iteration 24400 - loss 0.13271514228323841311\n",
      "accuracy: 93.9999999994 - epoch 1290    iteration 24500 - loss 0.19168378319121698317\n",
      "accuracy: 91.3333333327 - epoch 1295    iteration 24600 - loss 0.14252675087475494276\n",
      "accuracy: 92.6666666660 - epoch 1300    iteration 24700 - loss 0.18112285661621746469\n",
      "accuracy: 91.3333333327 - epoch 1306    iteration 24800 - loss 0.13745791995758879778\n",
      "accuracy: 94.6666666660 - epoch 1311    iteration 24900 - loss 0.13025605349586366621\n",
      "accuracy: 96.6666666660 - epoch 1316    iteration 25000 - loss 0.13475979901084514379\n",
      "accuracy: 91.3333333327 - epoch 1322    iteration 25100 - loss 0.20959259296878812728\n",
      "accuracy: 95.3333333327 - epoch 1327    iteration 25200 - loss 0.08635239821475747257\n",
      "accuracy: 93.9999999994 - epoch 1332    iteration 25300 - loss 0.10217066595916848848\n",
      "accuracy: 95.9999999994 - epoch 1337    iteration 25400 - loss 0.16341785017987683903\n",
      "accuracy: 92.6666666660 - epoch 1343    iteration 25500 - loss 0.12090641252181384691\n",
      "accuracy: 95.9999999994 - epoch 1348    iteration 25600 - loss 0.12123799173301369470\n",
      "accuracy: 91.9999999994 - epoch 1353    iteration 25700 - loss 0.19622640372535216091\n",
      "accuracy: 91.9999999994 - epoch 1358    iteration 25800 - loss 0.15867825269542326638\n",
      "accuracy: 94.6666666660 - epoch 1364    iteration 25900 - loss 0.14405519756700552181\n",
      "accuracy: 94.6666666660 - epoch 1369    iteration 26000 - loss 0.13629100205092720133\n",
      "accuracy: 95.3333333327 - epoch 1374    iteration 26100 - loss 0.16124940935116621743\n",
      "accuracy: 96.6666666660 - epoch 1379    iteration 26200 - loss 0.13756362710046157738\n",
      "accuracy: 95.9999999994 - epoch 1385    iteration 26300 - loss 0.21980005948011244188\n",
      "accuracy: 95.3333333327 - epoch 1390    iteration 26400 - loss 0.11342428825193247299\n",
      "accuracy: 89.9999999994 - epoch 1395    iteration 26500 - loss 0.18765467217703671321\n",
      "accuracy: 93.3333333327 - epoch 1400    iteration 26600 - loss 0.19235930533994094471\n",
      "accuracy: 92.6666666660 - epoch 1406    iteration 26700 - loss 0.18494716282004217134\n",
      "accuracy: 93.3333333327 - epoch 1411    iteration 26800 - loss 0.25955739965677798731\n",
      "accuracy: 97.3333333327 - epoch 1416    iteration 26900 - loss 0.14137003795814470641\n",
      "accuracy: 93.3333333327 - epoch 1422    iteration 27000 - loss 0.13576395770651214501\n",
      "accuracy: 95.3333333327 - epoch 1427    iteration 27100 - loss 0.17044274554340196892\n",
      "accuracy: 89.9999999994 - epoch 1432    iteration 27200 - loss 0.15693549217724864220\n",
      "accuracy: 95.3333333327 - epoch 1437    iteration 27300 - loss 0.12677553678475297905\n",
      "accuracy: 91.3333333327 - epoch 1443    iteration 27400 - loss 0.19477881052521842098\n",
      "accuracy: 89.3333333327 - epoch 1448    iteration 27500 - loss 0.29346149483077604847\n",
      "accuracy: 90.6666666661 - epoch 1453    iteration 27600 - loss 0.21101030328949108017\n",
      "accuracy: 92.6666666660 - epoch 1458    iteration 27700 - loss 0.14535820242437769156\n",
      "accuracy: 88.6666666661 - epoch 1464    iteration 27800 - loss 0.21943650294054775629\n",
      "accuracy: 93.9999999994 - epoch 1469    iteration 27900 - loss 0.18643354876109044516\n",
      "accuracy: 90.6666666661 - epoch 1474    iteration 28000 - loss 0.15540386947240084070\n",
      "accuracy: 92.6666666660 - epoch 1479    iteration 28100 - loss 0.17069143847944001635\n",
      "accuracy: 95.3333333327 - epoch 1485    iteration 28200 - loss 0.14170369655901188244\n",
      "accuracy: 90.6666666661 - epoch 1490    iteration 28300 - loss 0.21284772328132767205\n",
      "accuracy: 91.3333333327 - epoch 1495    iteration 28400 - loss 0.13483960784448156822\n",
      "accuracy: 93.3333333327 - epoch 1500    iteration 28500 - loss 0.14565882653893488796\n",
      "accuracy: 94.6666666660 - epoch 1506    iteration 28600 - loss 0.10467039830317674542\n",
      "accuracy: 91.9999999994 - epoch 1511    iteration 28700 - loss 0.20118978926889036218\n",
      "accuracy: 95.3333333327 - epoch 1516    iteration 28800 - loss 0.21298475406308522984\n",
      "accuracy: 91.9999999994 - epoch 1522    iteration 28900 - loss 0.14963531305432223317\n",
      "accuracy: 90.6666666661 - epoch 1527    iteration 29000 - loss 0.15651657281000982969\n",
      "accuracy: 92.6666666660 - epoch 1532    iteration 29100 - loss 0.19331344061126595757\n",
      "accuracy: 92.6666666660 - epoch 1537    iteration 29200 - loss 0.16766232349427540038\n",
      "accuracy: 94.6666666660 - epoch 1543    iteration 29300 - loss 0.17874716397574277882\n",
      "accuracy: 93.9999999994 - epoch 1548    iteration 29400 - loss 0.18007295732802583443\n",
      "accuracy: 93.9999999994 - epoch 1553    iteration 29500 - loss 0.18468034998308294203\n",
      "accuracy: 93.3333333327 - epoch 1558    iteration 29600 - loss 0.17811844964618203657\n",
      "accuracy: 95.9999999994 - epoch 1564    iteration 29700 - loss 0.13164886098241204704\n",
      "accuracy: 92.6666666660 - epoch 1569    iteration 29800 - loss 0.13877827835559561342\n",
      "accuracy: 93.9999999994 - epoch 1574    iteration 29900 - loss 0.12322897299368511248\n",
      "accuracy: 95.9999999994 - epoch 1579    iteration 30000 - loss 0.09865872338316072909\n",
      "accuracy: 93.9999999994 - epoch 1585    iteration 30100 - loss 0.18298870030022917899\n",
      "accuracy: 93.3333333327 - epoch 1590    iteration 30200 - loss 0.16104729535908271498\n",
      "accuracy: 95.9999999994 - epoch 1595    iteration 30300 - loss 0.14237297808919316400\n",
      "accuracy: 93.9999999994 - epoch 1600    iteration 30400 - loss 0.14142241582944431455\n",
      "accuracy: 88.6666666661 - epoch 1606    iteration 30500 - loss 0.19323659881923607728\n",
      "accuracy: 95.3333333327 - epoch 1611    iteration 30600 - loss 0.17258135298535628999\n",
      "accuracy: 92.6666666660 - epoch 1616    iteration 30700 - loss 0.14074038807781427685\n",
      "accuracy: 89.9999999994 - epoch 1622    iteration 30800 - loss 0.27354433180305331996\n",
      "accuracy: 90.6666666661 - epoch 1627    iteration 30900 - loss 0.23211660946891071733\n",
      "accuracy: 95.9999999994 - epoch 1632    iteration 31000 - loss 0.13317545427057111973\n",
      "accuracy: 93.3333333327 - epoch 1637    iteration 31100 - loss 0.17345996648083489289\n",
      "accuracy: 94.6666666660 - epoch 1643    iteration 31200 - loss 0.16785433955125692029\n",
      "accuracy: 93.3333333327 - epoch 1648    iteration 31300 - loss 0.22556595371619911350\n",
      "accuracy: 92.6666666660 - epoch 1653    iteration 31400 - loss 0.16793722688282447519\n",
      "accuracy: 95.3333333327 - epoch 1658    iteration 31500 - loss 0.14721155114455886093\n",
      "accuracy: 95.3333333327 - epoch 1664    iteration 31600 - loss 0.13706457406213851113\n",
      "accuracy: 91.9999999994 - epoch 1669    iteration 31700 - loss 0.22809996519211139798\n",
      "accuracy: 90.6666666661 - epoch 1674    iteration 31800 - loss 0.17662198784532104345\n",
      "accuracy: 91.3333333327 - epoch 1679    iteration 31900 - loss 0.25383416240704653744\n",
      "accuracy: 91.9999999994 - epoch 1685    iteration 32000 - loss 0.16998241789587831274\n",
      "accuracy: 92.6666666660 - epoch 1690    iteration 32100 - loss 0.15758048065391419557\n",
      "accuracy: 94.6666666660 - epoch 1695    iteration 32200 - loss 0.11716052559028947155\n",
      "accuracy: 93.9999999994 - epoch 1700    iteration 32300 - loss 0.17169451599823856713\n",
      "accuracy: 97.3333333327 - epoch 1706    iteration 32400 - loss 0.15106955627823309696\n",
      "accuracy: 96.6666666660 - epoch 1711    iteration 32500 - loss 0.11705629484159978115\n",
      "accuracy: 91.9999999994 - epoch 1716    iteration 32600 - loss 0.15519622905996596773\n",
      "accuracy: 95.3333333327 - epoch 1722    iteration 32700 - loss 0.15361505945646977689\n",
      "accuracy: 97.9999999993 - epoch 1727    iteration 32800 - loss 0.19802704328228923347\n",
      "accuracy: 97.3333333327 - epoch 1732    iteration 32900 - loss 0.11179181114505690187\n",
      "accuracy: 95.9999999994 - epoch 1737    iteration 33000 - loss 0.13063226697162350076\n",
      "accuracy: 93.9999999994 - epoch 1743    iteration 33100 - loss 0.17285740885176784087\n",
      "accuracy: 93.3333333327 - epoch 1748    iteration 33200 - loss 0.15687626191253101204\n",
      "accuracy: 95.9999999994 - epoch 1753    iteration 33300 - loss 0.12607335115066262143\n",
      "accuracy: 96.6666666660 - epoch 1758    iteration 33400 - loss 0.13456796570074733288\n",
      "accuracy: 94.6666666660 - epoch 1764    iteration 33500 - loss 0.15363402647658114253\n",
      "accuracy: 91.3333333327 - epoch 1769    iteration 33600 - loss 0.15558599313745133386\n",
      "accuracy: 92.6666666660 - epoch 1774    iteration 33700 - loss 0.15062154270228969577\n",
      "accuracy: 92.6666666660 - epoch 1779    iteration 33800 - loss 0.11371802990318083393\n",
      "accuracy: 90.6666666661 - epoch 1785    iteration 33900 - loss 0.26643186433803350832\n",
      "accuracy: 93.9999999994 - epoch 1790    iteration 34000 - loss 0.17076429082730859577\n",
      "accuracy: 91.9999999994 - epoch 1795    iteration 34100 - loss 0.21447565474624277759\n",
      "accuracy: 93.9999999994 - epoch 1800    iteration 34200 - loss 0.15293863470943652727\n",
      "accuracy: 94.6666666660 - epoch 1806    iteration 34300 - loss 0.09692607855086689261\n",
      "accuracy: 95.3333333327 - epoch 1811    iteration 34400 - loss 0.14126005339033742558\n",
      "accuracy: 94.6666666660 - epoch 1816    iteration 34500 - loss 0.18410375985640814456\n",
      "accuracy: 92.6666666660 - epoch 1822    iteration 34600 - loss 0.13261178122905906140\n",
      "accuracy: 95.3333333327 - epoch 1827    iteration 34700 - loss 0.11197700022131353015\n",
      "accuracy: 94.6666666660 - epoch 1832    iteration 34800 - loss 0.14869673908945696406\n",
      "accuracy: 94.6666666660 - epoch 1837    iteration 34900 - loss 0.12974673156500868276\n",
      "accuracy: 93.3333333327 - epoch 1843    iteration 35000 - loss 0.15680988263946185612\n",
      "accuracy: 96.6666666660 - epoch 1848    iteration 35100 - loss 0.12101793526890096409\n",
      "accuracy: 93.3333333327 - epoch 1853    iteration 35200 - loss 0.16037213425672894318\n",
      "accuracy: 95.3333333327 - epoch 1858    iteration 35300 - loss 0.13342409776309194225\n",
      "accuracy: 96.6666666660 - epoch 1864    iteration 35400 - loss 0.11242386978170453016\n",
      "accuracy: 93.9999999994 - epoch 1869    iteration 35500 - loss 0.15173306506685685369\n",
      "accuracy: 95.9999999994 - epoch 1874    iteration 35600 - loss 0.12299137007886908057\n",
      "accuracy: 94.6666666660 - epoch 1879    iteration 35700 - loss 0.10731144535432612930\n",
      "accuracy: 94.6666666660 - epoch 1885    iteration 35800 - loss 0.12345733290736508880\n",
      "accuracy: 93.9999999994 - epoch 1890    iteration 35900 - loss 0.15720340751851669125\n",
      "accuracy: 95.3333333327 - epoch 1895    iteration 36000 - loss 0.20772959210795322038\n",
      "accuracy: 97.3333333327 - epoch 1900    iteration 36100 - loss 0.16179537657861700617\n",
      "accuracy: 92.6666666660 - epoch 1906    iteration 36200 - loss 0.20661057345431990684\n",
      "accuracy: 93.3333333327 - epoch 1911    iteration 36300 - loss 0.13643407366200255293\n",
      "accuracy: 93.3333333327 - epoch 1916    iteration 36400 - loss 0.19016299474452039164\n",
      "accuracy: 94.6666666660 - epoch 1922    iteration 36500 - loss 0.18685322680296473363\n",
      "accuracy: 95.9999999994 - epoch 1927    iteration 36600 - loss 0.12984757206012967345\n",
      "accuracy: 93.3333333327 - epoch 1932    iteration 36700 - loss 0.15631857724214737182\n",
      "accuracy: 92.6666666660 - epoch 1937    iteration 36800 - loss 0.15110608103341027775\n",
      "accuracy: 94.6666666660 - epoch 1943    iteration 36900 - loss 0.14365597884618641622\n",
      "accuracy: 92.6666666660 - epoch 1948    iteration 37000 - loss 0.21248150030766205232\n",
      "accuracy: 89.9999999994 - epoch 1953    iteration 37100 - loss 0.17588021063604045158\n",
      "accuracy: 94.6666666660 - epoch 1958    iteration 37200 - loss 0.17492275938624343712\n",
      "accuracy: 87.9999999994 - epoch 1964    iteration 37300 - loss 0.25700181382822429033\n",
      "accuracy: 94.6666666660 - epoch 1969    iteration 37400 - loss 0.17883801234006410641\n",
      "accuracy: 94.6666666660 - epoch 1974    iteration 37500 - loss 0.14910251711785890705\n",
      "accuracy: 94.6666666660 - epoch 1979    iteration 37600 - loss 0.14538089834897952035\n",
      "accuracy: 95.3333333327 - epoch 1985    iteration 37700 - loss 0.15553526813418058761\n",
      "accuracy: 92.6666666660 - epoch 1990    iteration 37800 - loss 0.15084632685745746894\n",
      "accuracy: 94.6666666660 - epoch 1995    iteration 37900 - loss 0.15927106138321983630\n",
      "accuracy: 93.9999999994 - epoch 2000    iteration 38000 - loss 0.10273350507587250546\n",
      "func:'fit' -- took: 18.2114 sec\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(2, 'relu'),\n",
    "    hidden_layer=[(4,'relu'),(4,'softmax')],\n",
    "    output_layer=3,\n",
    "    batch_size=8,\n",
    "    optimizer={\"method\": \"ADAM\", \"beta1\": 0.9, \"beta2\": 0.999},\n",
    "    keep_prob=0.5,\n",
    "    penalty=None,\n",
    "    lambd=0.0001,\n",
    "    epoch=2000,\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calculate_model_performance(np.argmax(Y, axis=0),\n",
    "                           model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dict_all_models, results_average_dict, models = grid_search(\n",
    "    x,\n",
    "    y,\n",
    "    clf=NeuralNetwork,\n",
    "    lst_metrics=[\"F1\", \"accuracy\"],\n",
    "    sort_by = \"accuracy\",\n",
    "    n_folds=5,\n",
    "    dict_param_grid={\n",
    "        'batch_size': [8, 16, 32],\n",
    "        'input_layer': [(2, 'relu')],\n",
    "        'hidden_layer': [\n",
    "            [(4,'relu'), (4,'softmax')],\n",
    "            [(4,'sigmoid'),(4,'softmax')]\n",
    "        ],\n",
    "        'optimizer': [\n",
    "            {\n",
    "                \"method\": \"RMSP\",\n",
    "                \"beta\": 0.9\n",
    "            }\n",
    "        ],\n",
    "        'output_layer': [3],\n",
    "        'alpha': [0.001],\n",
    "        'verbose': [False],\n",
    "        'epoch': [1000]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "dt = data.data[:,[0,2]]\n",
    "x_min, x_max = dt[:, 0].min() - 1, dt[:, 0].max() + 1\n",
    "y_min, y_max = dt[:, 1].min() - 1, dt[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()].T) \n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx, yy, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "\n",
    "plt.scatter(dt[:, 0], dt[:, 1], c=y,s=20, edgecolor='k')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('petal length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Make Moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "x,y =make_moons(n_samples=1500, noise=.05)\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(X.shape[0], 'relu'),\n",
    "    hidden_layer=[(10,'relu'), (4,'softmax')],\n",
    "    output_layer=Y.shape[0],\n",
    "    batch_size=8,\n",
    "    optimizer=\n",
    "    {\n",
    "        \"method\": \"ADAM\",\n",
    "        \"beta1\": 0.9,\n",
    "        \"beta2\": 0.999\n",
    "    },\n",
    "    penalty = \"l2\",\n",
    "    keep_prob=0.5,\n",
    "    lambd=0.001,\n",
    "    epoch=250,\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Decision Boundaries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "dt = x\n",
    "x_min, x_max = dt[:, 0].min() - 0.5, dt[:, 0].max() + 0.5\n",
    "y_min, y_max = dt[:, 1].min() - 0.5, dt[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()].T) \n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx, yy, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "plt.scatter(dt[:, 0], dt[:, 1], c=y, s=20, edgecolor='k')\n",
    "plt.title('Decision Boundaries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Andrew NG Assignment 2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex2data2 = np.loadtxt(\"../ex2/data/ex2data2.txt\", delimiter=\",\")\n",
    "\n",
    "x = ex2data2[:, :-1]\n",
    "y = ex2data2[:, -1]\n",
    "\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(X.shape[0], 'relu'),\n",
    "    hidden_layer=[(10,'relu'), (4,'softmax')],\n",
    "    output_layer=Y.shape[0],\n",
    "    batch_size=8,\n",
    "    optimizer={\"method\": \"ADAM\", \"beta1\": 0.9, \"beta2\": 0.999},\n",
    "    penalty = None,\n",
    "    dropout=False,\n",
    "    keep_prob=0.9,\n",
    "    lambd=0.05,\n",
    "    epoch=1500,\n",
    "    alpha=0.001\n",
    ")\n",
    "\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dict_all_models, results_average_dict, models = grid_search_stratified(\n",
    "    x,\n",
    "    y,\n",
    "    clf=NeuralNetwork,\n",
    "    metrics=[\"F1\", \"accuracy\"],\n",
    "    sort_by = \"accuracy\",\n",
    "    n_fold=6,\n",
    "    param_grid_dict={\n",
    "        'batch_size': [16, 32],\n",
    "        'input_layer': [(2, 'relu')],\n",
    "        'hidden_layer': [\n",
    "            [(4,'relu'), (4,'relu'), (4,'softmax')],\n",
    "            [(4,'sigmoid'),(4,'softmax')]\n",
    "        ],\n",
    "        'output_layer': [2],\n",
    "        'alpha': [2, 4],\n",
    "        'verbose': [False],\n",
    "        'epoch': [5000]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_average_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(models[\"model_5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Decision Boundaries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "X = ex2data2\n",
    "\n",
    "x1_min, x1_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3,\n",
    "x2_min, x2_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3,\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx1.ravel(), xx2.ravel()].T) \n",
    "\n",
    "negatives = ex2data2[ex2data2[:, -1] == 0]\n",
    "positives = ex2data2[ex2data2[:, -1] == 1]\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx1.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx1, xx2, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "plt.scatter(negatives[:, 0], negatives[:, 1],s=50, color='k')\n",
    "plt.scatter(positives[:, 0], positives[:, 1],s=50, color='r')\n",
    "plt.title('Decision Boundaries')\n",
    "\n",
    "plt.contour(xx1, xx2, Z, [0.5], linewidths=2, colors=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "data = loadmat('../ex3/data/ex3data1.mat')\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = data[\"X\"]\n",
    "y = data[\"y\"]\n",
    "y[y==10] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_test, dataset_train = split_data_as(x, y, train=0.9, test=0.1)\n",
    "X_train = dataset_train[:, :-1].T\n",
    "Y_train = one_hot_encode(dataset_train[:, -1]).T\n",
    "\n",
    "X_test = dataset_test[:, :-1].T\n",
    "Y_test = one_hot_encode(dataset_test[:, -1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = x.T\n",
    "# Y = one_hot_encode(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "sample = np.random.choice(data[\"X\"].shape[0], 20)\n",
    "ax.imshow(data[\"X\"][sample,1:].reshape(-1,20).T)\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354\n",
    "\n",
    "model = NeuralNetwork(\n",
    "    input_layer=(X_train.shape[0], 'relu'),\n",
    "    hidden_layer=[(200,'relu'),(100,'relu'),(4,'softmax')],\n",
    "    output_layer=Y_train.shape[0],\n",
    "    batch_size=16,\n",
    "    optimizer={\"method\": \"ADAM\", \"beta1\": 0.9, \"beta2\": 0.999},\n",
    "    penalty = \"l2\",\n",
    "    dropout=True,\n",
    "    lambd=0.1,\n",
    "    epoch=500,\n",
    "    alpha=0.001\n",
    ")\n",
    "\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calculate_model_performance(np.argmax(Y_test, axis=0), model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    (X.shape[0], 'relu'),\n",
    "    [(200, 'relu'), (100, 'relu'), (50, 'relu'), (10,'softmax')],\n",
    "    Y.shape[0],\n",
    "    batch_size=50,\n",
    "    optimizer={\n",
    "        \"method\": \"RMSP\",\n",
    "        \"beta\": 0.9\n",
    "                },\n",
    "    epoch=100,\n",
    "    alpha=0.001\n",
    ")\n",
    "\n",
    "model.fit(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    (X.shape[0], 'relu'),\n",
    "    [(25,'relu'), (4,'softmax')],\n",
    "    Y.shape[0],\n",
    "    batch_size=50,\n",
    "    optimizer={\n",
    "        \"method\": \"SGDM\",\n",
    "        \"beta\": 0.9\n",
    "                },\n",
    "    epoch=1000,\n",
    "    alpha=6)\n",
    "\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dict_all_models, results_average_dict, models = grid_search(\n",
    "    x,\n",
    "    y,\n",
    "    clf=NeuralNetwork,\n",
    "    lst_metrics=[\"F1\", \"accuracy\"],\n",
    "    sort_by = \"F1\",\n",
    "    n_folds=10,\n",
    "    dict_param_grid={\n",
    "        'batch_size': [64, 128, 256],\n",
    "        'input_layer': [(x.shape[1], 'relu')],\n",
    "        'hidden_layer': [\n",
    "            [(50, 'relu'), (25, 'relu'), (10,'softmax')],\n",
    "            [(200, 'relu'), (100, 'relu'), (50, 'relu'), (10,'softmax')]\n",
    "        ],\n",
    "        'optimizer':[\n",
    "            {\n",
    "                \"method\": \"ADAM\",\n",
    "                \"beta1\": 0.9,\n",
    "                \"beta2\": 0.999\n",
    "            }\n",
    "        ],\n",
    "        'output_layer': [10],\n",
    "        'alpha': [0.0001, 0.001],\n",
    "        'verbose': [False],\n",
    "        'epoch': [250]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_average_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(models[\"model_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_miss_clasifications(model, digits_to_display):\n",
    "    count = 0\n",
    "    for index, (act, predicted) in enumerate(zip(np.argmax(Y,axis=0), model.predict(X))):\n",
    "        if act != predicted:\n",
    "            fig, ax = plt.subplots(figsize = (2,2))\n",
    "            ax.set_title(\"%s: act %s --- predicted %s\" %(index, act, predicted))\n",
    "            ax.imshow(X[:, index].reshape(-1,20).T)\n",
    "            ax.axis('off');\n",
    "            count += 1\n",
    "        if count == digits_to_display:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_miss_clasifications(model, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
