{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utility_functions import (calculate_model_performance,\n",
    "                               plot_ROC,\n",
    "                               one_hot_encode,\n",
    "                               split_data_as,\n",
    "                               random_shuffle)\n",
    "\n",
    "\n",
    "def get_shapes(any_):\n",
    "    for array in any_:\n",
    "        try:\n",
    "            print(array.shape)\n",
    "        except:\n",
    "            print(\"NONE\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# ============= ACTIVATION FUNCTIONS ===============#\n",
    "\n",
    "def sigmoid(Z, prime=False):\n",
    "    # np.\n",
    "    if prime:\n",
    "        return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "\n",
    "def linear(Z, prime=False):\n",
    "    if prime:\n",
    "        return np.ones_like(Z)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def relu(Z, alpha=0.01, prime=False):\n",
    "    if prime:\n",
    "        Z_relu = np.ones_like(Z, dtype=np.float64)\n",
    "        Z_relu[Z < 0] = alpha\n",
    "        return Z_relu\n",
    "    return np.where(Z < 0, alpha * Z, Z)\n",
    "\n",
    "\n",
    "def tanh(Z, prime=False):\n",
    "    # np.tanh() could be used directly to speed this up\n",
    "    if prime:\n",
    "        return 1 - np.power(tanh(Z), 2)\n",
    "    return (2 / (1 + np.exp(-2 * Z))) - 1\n",
    "\n",
    "\n",
    "def elu(Z, prime=False):\n",
    "    # https://mlfromscratch.com/activation-functions-explained/#/\n",
    "    alpha = 0.2\n",
    "    if prime:\n",
    "        return np.where(Z < 0, alpha * (np.exp(Z)), 1)\n",
    "    return np.where(Z < 0, alpha * (np.exp(Z) - 1), Z)\n",
    "\n",
    "\n",
    "def softmax(Z, prime=False):\n",
    "    # https://deepnotes.io/softmax-crossentropy\n",
    "    # max(Z) term is added to stabilise the function.\n",
    "    exps = np.exp(Z - np.max(Z))\n",
    "    return exps / np.sum(exps, axis=0)\n",
    "\n",
    "\n",
    "# ============== LOSS FUNCTIONS ===============#\n",
    "\n",
    "# https://deepnotes.io/softmax-crossentropy\n",
    "EPSILON = 1e-8\n",
    "\n",
    "\n",
    "def calculate_error(Y, Y_hat):\n",
    "    # Y and Y_hat should be in the form of (no_of_classes, no_of_training_examples)\n",
    "    m = Y.shape[1]\n",
    "    return -np.sum(Y * np.log(Y_hat + EPSILON)) / m\n",
    "\n",
    "\n",
    "# References\n",
    "# https://mc.ai/multilayered-neural-network-from-scratch-using-python/\n",
    "# https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "# https://www.coursera.org/learn/machine-learning/home/week/5\n",
    "# https://www.coursera.org/specializations/deep-learning\n",
    "# https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py\n",
    "# https://github.com/JWarmenhoven/Coursera-Machine-Learning\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_layer: tuple,\n",
    "            hidden_layer: list,  # list of tuples\n",
    "            output_layer: int,\n",
    "            batch_size=16,\n",
    "            alpha=1,\n",
    "            epoch=500,\n",
    "            random_state=42,\n",
    "            verbose=True,\n",
    "            metrics=\"accuracy\"\n",
    "    ):\n",
    "        self.input_layer = input_layer\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.output_layer = output_layer\n",
    "        self.mini_batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.epoch = epoch\n",
    "        self.seed = random_state\n",
    "        self.verbose = verbose\n",
    "        self.metrics = metrics\n",
    "\n",
    "        self.layers = len(self.weight_set_dimensions) + 1\n",
    "\n",
    "    def get_A(self, X):\n",
    "        A, _ = self.forwardpass(X)\n",
    "        return A\n",
    "\n",
    "    def get_Z(self, X):\n",
    "        _, Z = self.forwardpass(X)\n",
    "        return Z\n",
    "\n",
    "    def display_information(self, X, Y, epoch_no):\n",
    "        model_performance_metrics = calculate_model_performance(\n",
    "            np.argmax(Y, axis=0),\n",
    "            self.predict(X)\n",
    "        )\n",
    "        print(\"%s: %.10f - epoch %s    iteration %s - loss %.20f\" % (\n",
    "            self.metrics,\n",
    "            model_performance_metrics[self.metrics],\n",
    "            epoch_no,\n",
    "            self.no_of_iterations,\n",
    "            calculate_error(Y,\n",
    "                            self.get_A(X)[-1])\n",
    "        )\n",
    "              )\n",
    "\n",
    "    def get_dimensions_and_activations(self):\n",
    "        self.dimensions = []\n",
    "        self.activation_functions = []\n",
    "\n",
    "        self.dimensions.append(self.input_layer[0])\n",
    "        self.activation_functions.append(self.input_layer[1])\n",
    "\n",
    "        for dim, act_func in self.hidden_layer:\n",
    "            self.dimensions.append(dim)\n",
    "            self.activation_functions.append(act_func)\n",
    "\n",
    "        self.dimensions.append(self.output_layer)\n",
    "\n",
    "    @property\n",
    "    def weight_set_dimensions(self):\n",
    "        self.get_dimensions_and_activations()\n",
    "        a, b = itertools.tee(self.dimensions[::-1])\n",
    "        next(b, None)\n",
    "        weight_set_dimensions = list(zip(a, b))[::-1]\n",
    "        return weight_set_dimensions\n",
    "\n",
    "    def initialise_weights(self, layer=None):\n",
    "        self.W = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.B = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.W[0] = None\n",
    "        self.B[0] = None\n",
    "        for layer, (y, x) in zip(range(1, self.layers), self.weight_set_dimensions):\n",
    "            np.random.seed(self.seed)\n",
    "            self.W[layer] = np.random.rand(y, x) / np.sqrt(self.dimensions[layer - 1])\n",
    "            self.B[layer] = np.random.rand(y, 1)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        Z = np.empty_like(range(self.layers), dtype=object)\n",
    "        A = np.empty_like(range(self.layers), dtype=object)\n",
    "        A[0] = X\n",
    "        Z[0] = None\n",
    "        for layer in range(1, self.layers):\n",
    "            # activation_function starts from 0 whereas layer starts from 1\n",
    "            active_function = self.activation_functions[layer - 1]\n",
    "            arg_to_pass_to_eval = \"(Z[layer])\"\n",
    "\n",
    "            Z[layer] = self.W[layer] @ A[layer - 1] + self.B[layer]\n",
    "            A[layer] = eval(active_function + arg_to_pass_to_eval)\n",
    "        return A, Z\n",
    "\n",
    "    def backpropagation(self, Y, A, Z):\n",
    "        self.delta = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.delta[0] = None\n",
    "\n",
    "        self.gradient_W = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.gradient_B = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.gradient_W[0] = None\n",
    "        self.gradient_B[0] = None\n",
    "\n",
    "        self.delta[-1] = A[-1] - Y\n",
    "\n",
    "        # We substract 1 here as delta_final is calculated seperately above\n",
    "        for layer in reversed(range(1, self.layers - 1)):\n",
    "            # 1 is substracted from layer as activations_functions start indexing from 0\n",
    "            active_function = self.activation_functions[layer - 1]\n",
    "            arg_to_pass_to_eval = \"(Z[layer], prime=True)\"\n",
    "\n",
    "\n",
    "            self.delta[layer] = (\n",
    "                    self.W[layer + 1].T @ self.delta[layer + 1] *\n",
    "                    eval(active_function + arg_to_pass_to_eval)\n",
    "            )\n",
    "\n",
    "            # calculate the gradient\n",
    "\n",
    "        for layer in range(1, self.layers):\n",
    "            self.gradient_W[layer] = (self.delta[layer] @ A[layer - 1].T) / self.m\n",
    "            self.gradient_B[layer] = np.sum(self.delta[layer], axis=1, keepdims=True) / self.m\n",
    "\n",
    "        # update the weights\n",
    "        for layer in range(1, self.layers):\n",
    "            cost = calculate_error(Y, A[-1])\n",
    "            cost_other = -np.mean(Y * np.log(A[-1] + 1e-8))\n",
    "            self.W[layer] -= self.alpha * self.gradient_W[layer]\n",
    "            self.B[layer] -= self.alpha * self.gradient_B[layer]\n",
    "\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.m = X.shape[1] # where (no_of_features, no_of_training_examples)\n",
    "        self.initialise_weights()\n",
    "\n",
    "        # By default the method is SGD(Stochastic Gradient Descent) if one wishes to use\n",
    "        # the whole batch, simply pass the number of traning examples available as the\n",
    "        # batch size when instantiating the class\n",
    "        self.no_of_iterations = 0\n",
    "        shuffled = np.arange(self.m)\n",
    "        if self.verbose:\n",
    "            print(\"Initialising weights...\")\n",
    "            print(\"Starting the training...\")\n",
    "            print(\"Initial cost: %.10f\\n\" % calculate_error(Y, self.get_A(X)[-1]))\n",
    "        for epoch_no in range(1, self.epoch + 1):\n",
    "            np.random.shuffle(shuffled)\n",
    "            X_shuffled = X[:, shuffled]\n",
    "            Y_shuffled = Y[:, shuffled]\n",
    "            for i in range(0, self.m, self.mini_batch_size):\n",
    "                self.no_of_iterations += 1\n",
    "                X_mini_batch = X_shuffled[:, i: i + self.mini_batch_size]\n",
    "                Y_mini_batch = Y_shuffled[:, i: i + self.mini_batch_size]\n",
    "\n",
    "                A, Z = self.forwardpass(X_mini_batch)\n",
    "                self.backpropagation(Y_mini_batch, A, Z)\n",
    "                if self.no_of_iterations % 5000 == 0 and self.verbose:\n",
    "                    self.display_information(X, Y, epoch_no)\n",
    "\n",
    "    def predict(\n",
    "            self,\n",
    "            X: np.ndarray,\n",
    "            return_prob_matrix=False\n",
    "    ):\n",
    "        \"\"\"Predict the output given the training data.\n",
    "\n",
    "            Returns the predicted values in two forms:\n",
    "\n",
    "            1.either by picking up the highest value along the columns for every row,\n",
    "                i.e. \"np.argmax(self.A[-1].T, axis=1)\"\n",
    "            2.or by returning a matrix that is in the shape of Y.T where each column\n",
    "                represents the probability of the instance belonging to that class.\n",
    "                Please note that every column in Y.T represents a class. To be able to\n",
    "                return the probability matrix, the final activation function must be\n",
    "                softmax!\n",
    "                i.e. \"array([0.9650488423, 0.0354737543, 0.0005225966])\"\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Training set in the shape of\n",
    "                (no_of_features, no_of_training examples).\n",
    "            return_prob_matrix (bool, optional): Returns the probability matrix if True.\n",
    "                Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray:\n",
    "\n",
    "            if return_prob_matrix is False, the output is in the shape of\n",
    "                (no_of_training_examples, 1)\n",
    "            if return_prob_matrix is True, the output is in the shape of\n",
    "                (no_of_training_examples, no_of_features)\n",
    "        \"\"\"\n",
    "        A, Z = self.forwardpass(X)\n",
    "        if return_prob_matrix and self.activation_functions[-1] == \"softmax\":\n",
    "            np.set_printoptions(precision=10, suppress=True)\n",
    "            return A[-1].T\n",
    "        return np.argmax(A[-1].T, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with benchmark datasets\n",
    "\n",
    "## 1.Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the dataset as train and test...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "x = data.data[:,[0,2]]\n",
    "y = data.target\n",
    "\n",
    "train, test = split_data_as(x, y, train=0.8, test=0.2)\n",
    "\n",
    "X_train = train[:, :-1]\n",
    "Y_train = train[:, -1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 2)\n",
      "(120, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "# print(\"\\n\")\n",
    "# print(X_test.shape)\n",
    "# print(Y_test.shape)\n",
    "# print(\"\\n\")\n",
    "# print(X_validation.shape)\n",
    "# print(Y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(2, \"relu\"),\n",
    "    hidden_layer=[(4, \"relu\"),\n",
    "                  (4, \"softmax\")],\n",
    "    output_layer=3,\n",
    "    batch_size=16,\n",
    "    alpha=2,\n",
    "    epoch=250,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import ParameterGrid\n",
    "\n",
    "def grid_search(x, y, clf,**kwargs):\n",
    "\n",
    "    grid = ParameterGrid(kwargs['param_grid'])\n",
    "\n",
    "    models = {}\n",
    "    for index, params in enumerate(grid):\n",
    "        print(params)\n",
    "        models[index] = clf(**params)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_layer': 3, 'batch_size': 8, 'hidden_layer': [(4, 'relu'), (4, 'softmax')], 'input_layer': (2, 'relu')}\n",
      "{'output_layer': 3, 'batch_size': 8, 'hidden_layer': [(4, 'sigmoid'), (4, 'softmax')], 'input_layer': (2, 'relu')}\n",
      "{'output_layer': 3, 'batch_size': 16, 'hidden_layer': [(4, 'relu'), (4, 'softmax')], 'input_layer': (2, 'relu')}\n",
      "{'output_layer': 3, 'batch_size': 16, 'hidden_layer': [(4, 'sigmoid'), (4, 'softmax')], 'input_layer': (2, 'relu')}\n"
     ]
    }
   ],
   "source": [
    "params = grid_search(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    NeuralNetwork,\n",
    "    param_grid={\n",
    "        'batch_size': [8,16],\n",
    "        'input_layer': [(2, 'relu')],\n",
    "        'hidden_layer': [[(4,'relu'),(4,'softmax')], [(4,'sigmoid'),(4,'softmax')]],\n",
    "        'output_layer': [3]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import ParameterGrid\n",
    "\n",
    "def grid_search(X, y, clf, n_fold=3, **kwargs):\n",
    "    # X and y are in the shape of (no_of_training_examples, no_of_features)\n",
    "    split_indices = np.int_(np.linspace(len(X)/n_fold, len(X), num=n_fold))\n",
    "    dataset_shuffled = random_shuffle(X, y)\n",
    "\n",
    "    splitted = np.array(\n",
    "        np.split(\n",
    "            dataset_shuffled,\n",
    "            split_indices\n",
    "        )[:-1]\n",
    "    )\n",
    "\n",
    "    models = {}\n",
    "    results_dict = {}\n",
    "\n",
    "    grid = ParameterGrid(kwargs['param_grid'])\n",
    "    for i, params in enumerate(grid):\n",
    "        models[i] = clf(**params)\n",
    "\n",
    "        for index in range(n_fold):\n",
    "            arrays_to_be_joined = np.delete(splitted, index, axis = 0)\n",
    "            dataset_train = np.concatenate(arrays_to_be_joined)\n",
    "            dataset_test = splitted[index]\n",
    "\n",
    "            x_train = dataset_train[:, :-1].T\n",
    "            y_train = one_hot_encode(dataset_train[:, -1]).T\n",
    "\n",
    "            models[i].fit(x_train, y_train)\n",
    "\n",
    "            x_test = dataset_test[:, :-1].T\n",
    "            y_test = one_hot_encode(dataset_test[:, -1]).T\n",
    "\n",
    "            results_dict[\"model_\" + str(i + 1) + \"_fold_\" + str(index + 1)] = calculate_model_performance(\n",
    "                np.argmax(y_test, axis=0),\n",
    "                models[i].predict(x_test)\n",
    "                )\n",
    "\n",
    "    return results_dict, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.1230945565\n",
      "\n",
      "accuracy: 92.5925925917 - epoch 358    iteration 5000 - loss 0.14965960157298166533\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.1278123396\n",
      "\n",
      "accuracy: 92.5925925917 - epoch 358    iteration 5000 - loss 0.16098633642671719701\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.2074416857\n",
      "\n",
      "accuracy: 92.5925925917 - epoch 358    iteration 5000 - loss 0.13339264529300626227\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.1065506638\n",
      "\n",
      "accuracy: 95.3703703695 - epoch 358    iteration 5000 - loss 0.10605356950131443472\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.1060353884\n",
      "\n",
      "accuracy: 93.5185185177 - epoch 358    iteration 5000 - loss 0.11509752915437758414\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.1626211316\n",
      "\n",
      "accuracy: 95.3703703695 - epoch 358    iteration 5000 - loss 0.11797386107055972404\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.2662591055\n",
      "\n",
      "accuracy: 95.3703703695 - epoch 358    iteration 5000 - loss 0.12180527938649415853\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.1438306918\n",
      "\n",
      "accuracy: 95.3703703695 - epoch 358    iteration 5000 - loss 0.10673028173868227830\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.1570714035\n",
      "\n",
      "accuracy: 95.3703703695 - epoch 358    iteration 5000 - loss 0.11053995279198025603\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.2165514485\n",
      "\n",
      "accuracy: 96.2962962954 - epoch 358    iteration 5000 - loss 0.11808008109866413582\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2266159211\n",
      "\n",
      "accuracy: 91.6666666658 - epoch 358    iteration 5000 - loss 0.23548310000008754184\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2327803549\n",
      "\n",
      "accuracy: 91.6666666658 - epoch 358    iteration 5000 - loss 0.25334741107863817389\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2614384941\n",
      "\n",
      "accuracy: 93.5185185177 - epoch 358    iteration 5000 - loss 0.14940530144369953969\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2189323588\n",
      "\n",
      "accuracy: 93.5185185177 - epoch 358    iteration 5000 - loss 0.17319452381339264790\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2251016415\n",
      "\n",
      "accuracy: 93.5185185177 - epoch 358    iteration 5000 - loss 0.15144875458699846860\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2417655035\n",
      "\n",
      "accuracy: 94.4444444436 - epoch 358    iteration 5000 - loss 0.13781922844266511485\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2781419280\n",
      "\n",
      "accuracy: 95.3703703695 - epoch 358    iteration 5000 - loss 0.14359846696639910757\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2372456874\n",
      "\n",
      "accuracy: 94.4444444436 - epoch 358    iteration 5000 - loss 0.16336748896465369540\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2417054890\n",
      "\n",
      "accuracy: 94.4444444436 - epoch 358    iteration 5000 - loss 0.16298890879269598009\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2615270684\n",
      "\n",
      "accuracy: 94.4444444436 - epoch 358    iteration 5000 - loss 0.15005725366035155277\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.1230945565\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.1278123396\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.2074416857\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.1065506638\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.1060353884\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.1626211316\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.2662591055\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.1438306918\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.1570714035\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.2165514485\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2266159211\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2327803549\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2614384941\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2189323588\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2251016415\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2417655035\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2781419280\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2372456874\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2417054890\n",
      "\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.2615270684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_dict, models = grid_search(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    clf=NeuralNetwork,\n",
    "    n_fold=10,\n",
    "    param_grid={\n",
    "        'batch_size': [8,16],\n",
    "        'input_layer': [(2, 'relu')],\n",
    "        'hidden_layer': [[(4,'relu'),(4,'softmax')], [(4,'sigmoid'),(4,'softmax')]],\n",
    "        'output_layer': [3],\n",
    "        'verbose': [True]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_1_fold_1': {'F1': 99.9999999745,\n",
       "  'accuracy': 99.99999999166667,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.999999975,\n",
       "  'prevalence': 33.333333330555554,\n",
       "  'sensitivity/recall': 99.999999975,\n",
       "  'specificity': 99.9999999875},\n",
       " 'model_1_fold_10': {'F1': 99.99999996616667,\n",
       "  'accuracy': 99.99999999166667,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999996666666,\n",
       "  'prevalence': 24.999999997916667,\n",
       "  'sensitivity/recall': 99.99999996666666,\n",
       "  'specificity': 99.99999998888889},\n",
       " 'model_1_fold_2': {'F1': 90.9090908920661,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999998,\n",
       "  'prevalence': 49.999999995833335,\n",
       "  'sensitivity/recall': 83.33333331944445,\n",
       "  'specificity': 99.99999998333334},\n",
       " 'model_1_fold_3': {'F1': 79.99999996752,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999994999999,\n",
       "  'prevalence': 24.999999997916667,\n",
       "  'sensitivity/recall': 66.66666664444443,\n",
       "  'specificity': 99.99999998888889},\n",
       " 'model_1_fold_4': {'F1': 99.9999999745,\n",
       "  'accuracy': 99.99999999166667,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.999999975,\n",
       "  'prevalence': 33.333333330555554,\n",
       "  'sensitivity/recall': 99.999999975,\n",
       "  'specificity': 99.9999999875},\n",
       " 'model_1_fold_5': {'F1': 99.99999998283334,\n",
       "  'accuracy': 99.99999999166667,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999998333334,\n",
       "  'prevalence': 49.999999995833335,\n",
       "  'sensitivity/recall': 99.99999998333334,\n",
       "  'specificity': 99.99999998333334},\n",
       " 'model_1_fold_6': {'F1': 88.88888886864198,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 12.4999999984375,\n",
       "  'precision': 79.999999984,\n",
       "  'prevalence': 33.333333330555554,\n",
       "  'sensitivity/recall': 99.999999975,\n",
       "  'specificity': 87.4999999890625},\n",
       " 'model_1_fold_7': {'F1': 99.9999998995,\n",
       "  'accuracy': 99.99999999166667,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999989999999,\n",
       "  'prevalence': 8.333333332638889,\n",
       "  'sensitivity/recall': 99.99999989999999,\n",
       "  'specificity': 99.99999999090909},\n",
       " 'model_1_fold_8': {'F1': 88.88888886864198,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.999999975,\n",
       "  'prevalence': 41.66666666319445,\n",
       "  'sensitivity/recall': 79.999999984,\n",
       "  'specificity': 99.99999998571428},\n",
       " 'model_1_fold_9': {'F1': 66.666666644,\n",
       "  'accuracy': 83.3333333263889,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999994999999,\n",
       "  'prevalence': 33.333333330555554,\n",
       "  'sensitivity/recall': 49.9999999875,\n",
       "  'specificity': 99.9999999875},\n",
       " 'model_2_fold_1': {'F1': 99.9999999745,\n",
       "  'accuracy': 99.99999999166667,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.999999975,\n",
       "  'prevalence': 33.333333330555554,\n",
       "  'sensitivity/recall': 99.999999975,\n",
       "  'specificity': 99.9999999875},\n",
       " 'model_2_fold_10': {'F1': 99.99999996616667,\n",
       "  'accuracy': 99.99999999166667,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999996666666,\n",
       "  'prevalence': 24.999999997916667,\n",
       "  'sensitivity/recall': 99.99999996666666,\n",
       "  'specificity': 99.99999998888889},\n",
       " 'model_2_fold_2': {'F1': 90.9090908920661,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999998,\n",
       "  'prevalence': 49.999999995833335,\n",
       "  'sensitivity/recall': 83.33333331944445,\n",
       "  'specificity': 99.99999998333334},\n",
       " 'model_2_fold_3': {'F1': 79.99999996752,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999994999999,\n",
       "  'prevalence': 24.999999997916667,\n",
       "  'sensitivity/recall': 66.66666664444443,\n",
       "  'specificity': 99.99999998888889},\n",
       " 'model_2_fold_4': {'F1': 99.9999999745,\n",
       "  'accuracy': 99.99999999166667,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.999999975,\n",
       "  'prevalence': 33.333333330555554,\n",
       "  'sensitivity/recall': 99.999999975,\n",
       "  'specificity': 99.9999999875},\n",
       " 'model_2_fold_5': {'F1': 99.99999998283334,\n",
       "  'accuracy': 99.99999999166667,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999998333334,\n",
       "  'prevalence': 49.999999995833335,\n",
       "  'sensitivity/recall': 99.99999998333334,\n",
       "  'specificity': 99.99999998333334},\n",
       " 'model_2_fold_6': {'F1': 88.88888886864198,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 12.4999999984375,\n",
       "  'precision': 79.999999984,\n",
       "  'prevalence': 33.333333330555554,\n",
       "  'sensitivity/recall': 99.999999975,\n",
       "  'specificity': 87.4999999890625},\n",
       " 'model_2_fold_7': {'F1': 66.66666662177778,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 9.090909090082643,\n",
       "  'precision': 49.999999974999994,\n",
       "  'prevalence': 8.333333332638889,\n",
       "  'sensitivity/recall': 99.99999989999999,\n",
       "  'specificity': 90.90909090082646},\n",
       " 'model_2_fold_8': {'F1': 88.88888886864198,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.999999975,\n",
       "  'prevalence': 41.66666666319445,\n",
       "  'sensitivity/recall': 79.999999984,\n",
       "  'specificity': 99.99999998571428},\n",
       " 'model_2_fold_9': {'F1': 85.71428568930612,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999996666666,\n",
       "  'prevalence': 33.333333330555554,\n",
       "  'sensitivity/recall': 74.99999998125,\n",
       "  'specificity': 99.9999999875},\n",
       " 'model_3_fold_1': {'F1': 99.9999999745,\n",
       "  'accuracy': 99.99999999166667,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.999999975,\n",
       "  'prevalence': 33.333333330555554,\n",
       "  'sensitivity/recall': 99.999999975,\n",
       "  'specificity': 99.9999999875},\n",
       " 'model_3_fold_10': {'F1': 99.99999996616667,\n",
       "  'accuracy': 99.99999999166667,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999996666666,\n",
       "  'prevalence': 24.999999997916667,\n",
       "  'sensitivity/recall': 99.99999996666666,\n",
       "  'specificity': 99.99999998888889},\n",
       " 'model_3_fold_2': {'F1': 90.9090908920661,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999998,\n",
       "  'prevalence': 49.999999995833335,\n",
       "  'sensitivity/recall': 83.33333331944445,\n",
       "  'specificity': 99.99999998333334},\n",
       " 'model_3_fold_3': {'F1': 79.99999996752,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999994999999,\n",
       "  'prevalence': 24.999999997916667,\n",
       "  'sensitivity/recall': 66.66666664444443,\n",
       "  'specificity': 99.99999998888889},\n",
       " 'model_3_fold_4': {'F1': 99.9999999745,\n",
       "  'accuracy': 99.99999999166667,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.999999975,\n",
       "  'prevalence': 33.333333330555554,\n",
       "  'sensitivity/recall': 99.999999975,\n",
       "  'specificity': 99.9999999875},\n",
       " 'model_3_fold_5': {'F1': 99.99999998283334,\n",
       "  'accuracy': 99.99999999166667,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999998333334,\n",
       "  'prevalence': 49.999999995833335,\n",
       "  'sensitivity/recall': 99.99999998333334,\n",
       "  'specificity': 99.99999998333334},\n",
       " 'model_3_fold_6': {'F1': 88.88888886864198,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 12.4999999984375,\n",
       "  'precision': 79.999999984,\n",
       "  'prevalence': 33.333333330555554,\n",
       "  'sensitivity/recall': 99.999999975,\n",
       "  'specificity': 87.4999999890625},\n",
       " 'model_3_fold_7': {'F1': 66.66666662177778,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 9.090909090082643,\n",
       "  'precision': 49.999999974999994,\n",
       "  'prevalence': 8.333333332638889,\n",
       "  'sensitivity/recall': 99.99999989999999,\n",
       "  'specificity': 90.90909090082646},\n",
       " 'model_3_fold_8': {'F1': 79.9999999835,\n",
       "  'accuracy': 83.3333333263889,\n",
       "  'false_positive_rate': 14.285714283673471,\n",
       "  'precision': 79.999999984,\n",
       "  'prevalence': 41.66666666319445,\n",
       "  'sensitivity/recall': 79.999999984,\n",
       "  'specificity': 85.71428570204081},\n",
       " 'model_3_fold_9': {'F1': 66.666666644,\n",
       "  'accuracy': 83.3333333263889,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999994999999,\n",
       "  'prevalence': 33.333333330555554,\n",
       "  'sensitivity/recall': 49.9999999875,\n",
       "  'specificity': 99.9999999875},\n",
       " 'model_4_fold_1': {'F1': 85.71428568930612,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999996666666,\n",
       "  'prevalence': 33.333333330555554,\n",
       "  'sensitivity/recall': 74.99999998125,\n",
       "  'specificity': 99.9999999875},\n",
       " 'model_4_fold_10': {'F1': 99.99999996616667,\n",
       "  'accuracy': 99.99999999166667,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999996666666,\n",
       "  'prevalence': 24.999999997916667,\n",
       "  'sensitivity/recall': 99.99999996666666,\n",
       "  'specificity': 99.99999998888889},\n",
       " 'model_4_fold_2': {'F1': 90.9090908920661,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999998,\n",
       "  'prevalence': 49.999999995833335,\n",
       "  'sensitivity/recall': 83.33333331944445,\n",
       "  'specificity': 99.99999998333334},\n",
       " 'model_4_fold_3': {'F1': 79.99999996752,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999994999999,\n",
       "  'prevalence': 24.999999997916667,\n",
       "  'sensitivity/recall': 66.66666664444443,\n",
       "  'specificity': 99.99999998888889},\n",
       " 'model_4_fold_4': {'F1': 99.9999999745,\n",
       "  'accuracy': 99.99999999166667,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.999999975,\n",
       "  'prevalence': 33.333333330555554,\n",
       "  'sensitivity/recall': 99.999999975,\n",
       "  'specificity': 99.9999999875},\n",
       " 'model_4_fold_5': {'F1': 99.99999998283334,\n",
       "  'accuracy': 99.99999999166667,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999998333334,\n",
       "  'prevalence': 49.999999995833335,\n",
       "  'sensitivity/recall': 99.99999998333334,\n",
       "  'specificity': 99.99999998333334},\n",
       " 'model_4_fold_6': {'F1': 88.88888886864198,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 12.4999999984375,\n",
       "  'precision': 79.999999984,\n",
       "  'prevalence': 33.333333330555554,\n",
       "  'sensitivity/recall': 99.999999975,\n",
       "  'specificity': 87.4999999890625},\n",
       " 'model_4_fold_7': {'F1': 66.66666662177778,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 9.090909090082643,\n",
       "  'precision': 49.999999974999994,\n",
       "  'prevalence': 8.333333332638889,\n",
       "  'sensitivity/recall': 99.99999989999999,\n",
       "  'specificity': 90.90909090082646},\n",
       " 'model_4_fold_8': {'F1': 88.88888886864198,\n",
       "  'accuracy': 91.66666665902777,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.999999975,\n",
       "  'prevalence': 41.66666666319445,\n",
       "  'sensitivity/recall': 79.999999984,\n",
       "  'specificity': 99.99999998571428},\n",
       " 'model_4_fold_9': {'F1': 66.666666644,\n",
       "  'accuracy': 83.3333333263889,\n",
       "  'false_positive_rate': 0.0,\n",
       "  'precision': 99.99999994999999,\n",
       "  'prevalence': 33.333333330555554,\n",
       "  'sensitivity/recall': 49.9999999875,\n",
       "  'specificity': 99.9999999875}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for value in results_dict.values():\n",
    "    print(value[\"F1\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splitted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.concatenate(np.delete(splitted, 1, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict(X_train) == np.argmax(Y_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_performance_metrics = calculate_model_performance(\n",
    "    np.argmax(Y_train, axis=0),\n",
    "    model.predict(X_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "dt = data.data[:,[0,2]]\n",
    "x_min, x_max = dt[:, 0].min() - 1, dt[:, 0].max() + 1\n",
    "y_min, y_max = dt[:, 1].min() - 1, dt[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()].T) \n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx, yy, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "\n",
    "plt.scatter(dt[:, 0], dt[:, 1], c=y,s=20, edgecolor='k')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('petal length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Make Moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "x,y =make_moons(n_samples=1500, noise=.05)\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(2, \"sigmoid\"),\n",
    "    hidden_layer=[(8, \"tanh\"),\n",
    "                  (6, \"relu\"),\n",
    "                  (4, \"softmax\")],\n",
    "    output_layer=2,\n",
    "    batch_size=64,\n",
    "    alpha=0.5,\n",
    "    epoch=2500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Decision Boundaries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "dt = x\n",
    "x_min, x_max = dt[:, 0].min() - 0.5, dt[:, 0].max() + 0.5\n",
    "y_min, y_max = dt[:, 1].min() - 0.5, dt[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()].T) \n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx, yy, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "plt.scatter(dt[:, 0], dt[:, 1], c=y, s=20, edgecolor='k')\n",
    "plt.title('Decision Boundaries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Andrew NG Assignment 2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex2data2 = np.loadtxt(\"../ex2/data/ex2data2.txt\", delimiter=\",\")\n",
    "\n",
    "X = ex2data2[:, :-1]\n",
    "y = ex2data2[:, -1]\n",
    "\n",
    "X = X.T\n",
    "Y = one_hot_encode(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(2, \"relu\"),\n",
    "    hidden_layer=[(4, \"relu\"),\n",
    "                  (4, \"softmax\")],\n",
    "    output_layer=2,\n",
    "    batch_size=64,\n",
    "    alpha=0.5,\n",
    "    epoch=25000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_performance_metrics = calculate_model_performance(\n",
    "    np.argmax(Y, axis=0),\n",
    "    model.predict(X)\n",
    ")\n",
    "\n",
    "model_performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Decision Boundaries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "X = ex2data2\n",
    "\n",
    "x1_min, x1_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3,\n",
    "x2_min, x2_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3,\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx1.ravel(), xx2.ravel()].T) \n",
    "\n",
    "negatives = ex2data2[ex2data2[:, -1] == 0]\n",
    "positives = ex2data2[ex2data2[:, -1] == 1]\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx1.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx1, xx2, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "plt.scatter(negatives[:, 0], negatives[:, 1],s=50, color='k')\n",
    "plt.scatter(positives[:, 0], positives[:, 1],s=50, color='r')\n",
    "plt.title('Decision Boundaries')\n",
    "\n",
    "plt.contour(xx1, xx2, Z, [0.5], linewidths=2, colors=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "data = loadmat('../ex3/data/ex3data1.mat')\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = data[\"X\"]\n",
    "y = data[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test, validation = split_data_as(x,y, train=0.6, test=0.2, validation=0.2)\n",
    "\n",
    "X_train = train[:, :-1].T\n",
    "Y_train = one_hot_encode(train[:, -1]).T\n",
    "\n",
    "X_test = test[:, :-1].T\n",
    "Y_test = one_hot_encode(test[:, -1]).T\n",
    "\n",
    "X_validation = validation[:, :-1].T\n",
    "Y_validation = one_hot_encode(validation[:, -1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(\"\\n\")\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(\"\\n\")\n",
    "print(X_validation.shape)\n",
    "print(Y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "sample = np.random.choice(data[\"X\"].shape[0], 20)\n",
    "ax.imshow(data[\"X\"][sample,1:].reshape(-1,20).T)\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(X_train.shape[0], \"relu\"),\n",
    "    hidden_layer=[(10, \"relu\"),\n",
    "                  (10, \"softmax\")],\n",
    "    output_layer=Y_train.shape[0],\n",
    "    batch_size=16,\n",
    "    alpha=0.2,\n",
    "    epoch=500,\n",
    "    random_state=12\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(model.predict(X_train) == np.argmax(Y_train,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calculate_model_performance(\n",
    "    np.argmax(Y_train,axis=0),\n",
    "    model.predict(X_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, (act, predicted) in enumerate(zip(np.argmax(Y_train,axis=0), model.predict(X_train))):\n",
    "    if act != predicted:\n",
    "        fig, ax = plt.subplots(figsize = (2,2))\n",
    "        ax.set_title(\"%s: act %s --- predicted %s\" %(index, act + 1, predicted + 1))\n",
    "        ax.imshow(X_train[:, index].reshape(-1,20).T)\n",
    "        ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calculate_model_performance(\n",
    "    np.argmax(Y_test,axis=0),\n",
    "    model.predict(X_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, (act, predicted) in enumerate(zip(np.argmax(Y_test,axis=0), model.predict(X_test))):\n",
    "    if act != predicted:\n",
    "        fig, ax = plt.subplots(figsize = (2,2))\n",
    "        ax.set_title(\"%s: act %s --- predicted %s\" %(index, act + 1, predicted + 1))\n",
    "        ax.imshow(X_test[:, index].reshape(-1,20).T)\n",
    "        ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
