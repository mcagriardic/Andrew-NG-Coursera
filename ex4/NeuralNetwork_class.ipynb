{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import itertools\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utility_functions import (calculate_model_performance,\n",
    "                               plot_ROC,\n",
    "                               one_hot_encode,\n",
    "                               split_data_as,\n",
    "                               grid_search,\n",
    "                               shuffled,\n",
    "                               timeit)\n",
    "\n",
    "EPSILON = 10e-08\n",
    "\n",
    "# error rate\n",
    "# https://stats.stackexchange.com/questions/133458/is-accuracy-1-test-error-rate\n",
    "\n",
    "\n",
    "def get_shapes(any_):\n",
    "    for array in any_:\n",
    "        try:\n",
    "            print(array.shape)\n",
    "        except:\n",
    "            print(\"NONE\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "def plot_loss(train_loss, test_loss):\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "    \n",
    "    ax.set_xlabel(\"iteration count x 100\", fontsize=15, labelpad=10)\n",
    "    ax.set_ylabel(\"Loss\", fontsize=15, labelpad=10)\n",
    "    ax.set_title(\"Loss vs Iteration\", fontsize=20)\n",
    "    ax.tick_params(labelsize=14)\n",
    "    \n",
    "    lines, = ax.plot(train_loss, c=\"blue\", label=\"train_loss\")\n",
    "    lines, = ax.plot(test_loss, c=\"green\", label=\"test_loss\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ============= ACTIVATION FUNCTIONS ===============#\n",
    "\n",
    "def sigmoid(Z, prime=False):\n",
    "    # np.\n",
    "    if prime:\n",
    "        return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "\n",
    "def linear(Z, prime=False):\n",
    "    if prime:\n",
    "        return np.ones_like(Z)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def relu(Z, alpha=0.01, prime=False):\n",
    "    if prime:\n",
    "        Z_relu = np.ones_like(Z, dtype=np.float64)\n",
    "        Z_relu[Z < 0] = alpha\n",
    "        return Z_relu\n",
    "    return np.where(Z < 0, alpha * Z, Z)\n",
    "\n",
    "\n",
    "def tanh(Z, prime=False):\n",
    "    # np.tanh() could be used directly to speed this up\n",
    "    if prime:\n",
    "        return 1 - np.power(tanh(Z), 2)\n",
    "    return (2 / (1 + np.exp(-2 * Z))) - 1\n",
    "\n",
    "\n",
    "def elu(Z, prime=False):\n",
    "    # https://mlfromscratch.com/activation-functions-explained/#/\n",
    "    alpha = 0.2\n",
    "    if prime:\n",
    "        return np.where(Z < 0, alpha * (np.exp(Z)), 1)\n",
    "    return np.where(Z < 0, alpha * (np.exp(Z) - 1), Z)\n",
    "\n",
    "\n",
    "def softmax(Z, prime=False):\n",
    "    # https://deepnotes.io/softmax-crossentropy\n",
    "    # max(Z) term is added to stabilise the function.\n",
    "    exps = np.exp(Z - np.max(Z))\n",
    "    return exps / np.sum(exps, axis=0)\n",
    "\n",
    "# References\n",
    "# https://mc.ai/multilayered-neural-network-from-scratch-using-python/\n",
    "# https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "# https://www.coursera.org/learn/machine-learning/home/week/5\n",
    "# https://www.coursera.org/specializations/deep-learning\n",
    "# https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py\n",
    "# https://github.com/JWarmenhoven/Coursera-Machine-Learning\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_layer: tuple,\n",
    "            hidden_layer: list,  # list of tuples\n",
    "            output_layer: int,\n",
    "            batch_size=16,\n",
    "            alpha=0.01,\n",
    "            optimizer=\"SGD\",\n",
    "            weight_initialisation = \"he_uniform\",\n",
    "            penalty=None,\n",
    "            lambd=0.01,\n",
    "            keep_prob = None,\n",
    "            epoch=500,\n",
    "            random_state=42,\n",
    "            verbose=True,\n",
    "            metrics=\"accuracy\",\n",
    "            history=True\n",
    "    ):\n",
    "        self.input_layer = input_layer\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.output_layer = output_layer\n",
    "        self.mini_batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.optimizer = optimizer\n",
    "        self.weight_initialisation = weight_initialisation\n",
    "        self.penalty = penalty\n",
    "        self.lambd = lambd\n",
    "        self.keep_prob = keep_prob\n",
    "        # dropout: http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf\n",
    "        self.dropout = True if isinstance(self.keep_prob, float) else False\n",
    "        self.epoch = epoch\n",
    "        self.seed = random_state\n",
    "        self.verbose = verbose\n",
    "        self.metrics = metrics\n",
    "        self.history = history\n",
    "        self.layers = len(self.weight_set_dimensions) + 1\n",
    "        self.EPSILON = 10e-10\n",
    "        self.train_error = []\n",
    "        self.validation_error = []\n",
    "\n",
    "        self.initial_lr = alpha[\"initial_lr\"] if isinstance(alpha, dict) else None\n",
    "        self.decay = alpha[\"decay\"] if isinstance(alpha, dict) else None\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        parameters = (\n",
    "            \"Input layer: {0}\\n\"\n",
    "            \"Hidden layer: {1}\\n\"\n",
    "            \"Output layer: {2}\\n\"\n",
    "            \"Batch size: {3}\\n\"\n",
    "            \"Learning rate: {4}\\n\"\n",
    "            \"Epoch: {5}\\n\"\n",
    "            \"Seed: {6}\\n\"\n",
    "            \"Verbose: {7}\\n\"\n",
    "            \"Metric: {8}\"\n",
    "        ).format(\n",
    "            self.input_layer,\n",
    "            \" - \".join(map(str, self.hidden_layer)),\n",
    "            self.output_layer,\n",
    "            self.mini_batch_size,\n",
    "            self.alpha,\n",
    "            self.epoch,\n",
    "            self.seed,\n",
    "            self.verbose,\n",
    "            self.metrics\n",
    "        )\n",
    "        return parameters\n",
    "\n",
    "    def get_A(self, X, predict=True):\n",
    "        A, _ = self.forwardpass(X, predict=predict)\n",
    "        return A\n",
    "\n",
    "    def get_Z(self, X, predict=True):\n",
    "        _, Z = self.forwardpass(X, predict=predict)\n",
    "        return Z\n",
    "        \n",
    "    # ============== LOSS FUNCTIONS ===============#\n",
    "\n",
    "    # https://deepnotes.io/softmax-crossentropy\n",
    "\n",
    "    def calculate_error(self, Y, Y_hat):\n",
    "        # Y and Y_hat should be in the form of (no_of_classes, no_of_training_examples)\n",
    "        m = Y.shape[1]\n",
    "        cost = -np.sum(Y * np.log(Y_hat + EPSILON)) / m\n",
    "        if self.penalty == \"l1\":\n",
    "            for layer in range(1, self.layers):\n",
    "                cost += np.sum(np.abs(self.W[layer])) * self.lambd / (2 * m)\n",
    "        elif self.penalty == \"l2\":\n",
    "            for layer in range(1, self.layers):\n",
    "                cost += np.sum(np.square(self.W[layer])) * self.lambd / (2 * m)\n",
    "        return cost\n",
    "\n",
    "\n",
    "    def display_information(self, X, Y, X_val, Y_val, alpha):\n",
    "        model_performance_metrics_train = calculate_model_performance(\n",
    "            np.argmax(Y, axis=0),\n",
    "            self.predict(X)\n",
    "        )\n",
    "        \n",
    "        train_error = np.round(self.calculate_error(Y, self.get_A(X)[-1]), 10)\n",
    "        validation_error = np.round(self.calculate_error(Y_val, self.get_A(X_val)[-1]), 10)\n",
    "        \n",
    "        if self.history:\n",
    "            self.train_error.append(train_error)\n",
    "            self.validation_error.append(validation_error)\n",
    "\n",
    "        print(\"train_{0}: {1} - epoch {2}    iteration {3} - train loss {4} ____ validation loss {5} --- alpha: {6}\".format(\n",
    "            self.metrics,\n",
    "            np.round(model_performance_metrics_train[self.metrics], 5),\n",
    "            self.epoch_count,\n",
    "            self.no_of_iterations,\n",
    "            train_error,\n",
    "            validation_error,\n",
    "            np.round(alpha, 5)\n",
    "        ))\n",
    "\n",
    "\n",
    "    def get_dimensions_and_activations(self):\n",
    "        self.dimensions = []\n",
    "        self.activation_functions = []\n",
    "\n",
    "        self.dimensions.append(self.input_layer[0])\n",
    "        self.activation_functions.append(self.input_layer[1])\n",
    "\n",
    "        for dim, act_func in self.hidden_layer:\n",
    "            self.dimensions.append(dim)\n",
    "            self.activation_functions.append(act_func)\n",
    "\n",
    "        self.dimensions.append(self.output_layer)\n",
    "\n",
    "    @property\n",
    "    def weight_set_dimensions(self):\n",
    "        self.get_dimensions_and_activations()\n",
    "        a, b = itertools.tee(self.dimensions[::-1])\n",
    "        next(b, None)\n",
    "        weight_set_dimensions = list(zip(a, b))[::-1]\n",
    "        return weight_set_dimensions\n",
    "\n",
    "    def initialise_weights(self, layer=None):\n",
    "        self.W = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.B = np.empty_like(range(self.layers), dtype=object)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        for layer, (y, x) in zip(range(1, self.layers), self.weight_set_dimensions):\n",
    "            # x = (layer - 1) *** fan_in\n",
    "            # y = (layer)  *** fan_out\n",
    "            \n",
    "            # difference between randn and normal:\n",
    "            # https://stackoverflow.com/questions/21738383/python-difference-between-randn-and-normal\n",
    "\n",
    "            if isinstance(self.weight_initialisation, dict):\n",
    "                if self.weight_initialisation[\"method\"] == \"random_normal\":\n",
    "                    mean = self.weight_initialisation[\"mean\"]\n",
    "                    standard_deviation = self.weight_initialisation[\"standard_deviation\"]\n",
    "\n",
    "                    self.W[layer] = np.random.normal(mean, standard_deviation, size=(y, x))\n",
    "                    self.B[layer] = np.random.normal(mean, standard_deviation, size=(y, 1))\n",
    "\n",
    "                elif self.weight_initialisation[\"method\"] == \"random_uniform\":\n",
    "                    min_ = self.weight_initialisation[\"min\"]\n",
    "                    max_ = self.weight_initialisation[\"max\"]\n",
    "\n",
    "                    self.W[layer] = np.random.uniform(min_, max_, size=(y, x))\n",
    "                    self.B[layer] = np.random.uniform(min_, max_, size=(y, 1))\n",
    "\n",
    "            elif self.weight_initialisation == \"zeros\":\n",
    "                self.W[layer] = np.zeros((y, x))\n",
    "                self.B[layer] = np.zeros((y, 1))\n",
    "\n",
    "            elif self.weight_initialisation == \"ones\":\n",
    "                self.W[layer] = np.ones((y, x))\n",
    "                self.B[layer] = np.ones((y, 1))\n",
    "\n",
    "            elif self.weight_initialisation == \"he_normal\":\n",
    "                # https://arxiv.org/abs/1502.01852\n",
    "                # https://keras.rstudio.com/reference/initializer_he_normal.html\n",
    "                self.W[layer] = np.random.normal(\n",
    "                    loc=0,\n",
    "                    scale=np.sqrt(2 / x),\n",
    "                    size=(y, x)\n",
    "                )\n",
    "                self.B[layer] = np.zeros((y, 1))\n",
    "                \n",
    "            elif self.weight_initialisation == \"he_uniform\":\n",
    "                # https://docs.chainer.org/en/stable/reference/generated/chainer.initializers.HeUniform.html\n",
    "                # https://keras.rstudio.com/reference/initializer_he_uniform.html\n",
    "                limit = np.sqrt(6 / x)\n",
    "\n",
    "                self.W[layer] = np.random.uniform(\n",
    "                    low=-limit,\n",
    "                    high=limit,\n",
    "                    size=(y, x)\n",
    "                )\n",
    "                self.B[layer] = np.zeros((y, 1))\n",
    "            \n",
    "            elif self.weight_initialisation == \"xavier_normal\":\n",
    "                # Glorot normal initializer, also called Xavier normal initializer.\n",
    "                # https://keras.rstudio.com/reference/initializer_glorot_normal.html\n",
    "                self.W[layer] = np.random.normal(\n",
    "                    loc=0,\n",
    "                    scale=np.sqrt(2 / (x + y)),\n",
    "                    size=(y, x)\n",
    "                )\n",
    "                self.B[layer] = np.zeros((y, 1))\n",
    "                \n",
    "            elif self.weight_initialisation == \"xavier_uniform\":\n",
    "                # https://keras.rstudio.com/reference/initializer_glorot_uniform.html\n",
    "                limit = np.sqrt(6 / (x + y))\n",
    "\n",
    "                self.W[layer] = np.random.uniform(\n",
    "                    low=-limit,\n",
    "                    high=limit,\n",
    "                    size=(y, x)\n",
    "                )                \n",
    "                self.B[layer] = np.zeros((y, 1))\n",
    "\n",
    "    def forwardpass(self, X, predict=False):\n",
    "        Z = np.empty_like(range(self.layers), dtype=object)\n",
    "        A = np.empty_like(range(self.layers), dtype=object)\n",
    "        A[0] = X\n",
    "\n",
    "        for layer in range(1, self.layers):\n",
    "            # activation_function starts from 0 whereas layer starts from 1\n",
    "            active_function = self.activation_functions[layer - 1]\n",
    "            arg_to_pass_to_eval = \"(Z[layer])\"\n",
    "\n",
    "            Z[layer] = self.W[layer] @ A[layer - 1] + self.B[layer]\n",
    "            A[layer] = eval(active_function + arg_to_pass_to_eval)\n",
    "            \n",
    "            # dropout is only applied to first hidden layer\n",
    "            # https://www.kaggle.com/mtax687/dropout-regularization-of-neural-net-using-numpy\n",
    "            if self.dropout and layer == 1 and not predict:\n",
    "                self.D = np.random.randn(A[layer].shape[0], A[layer].shape[1])\n",
    "                self.D = (self.D < self.keep_prob)\n",
    "                A[layer] = np.multiply(A[layer], self.D) / self.keep_prob\n",
    "\n",
    "        return A, Z\n",
    "\n",
    "    def backpropagation(self, Y, A, Z):\n",
    "        self.delta = np.empty_like(range(self.layers), dtype=object)\n",
    "\n",
    "        self.gradient_W = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.gradient_B = np.empty_like(range(self.layers), dtype=object)\n",
    "\n",
    "        self.delta[-1] = A[-1] - Y\n",
    "\n",
    "        # We substract 1 here as delta_final is calculated seperately above\n",
    "        for layer in reversed(range(1, self.layers - 1)):\n",
    "            # 1 is substracted from layer as activation_functions start indexing from 0\n",
    "            active_function = self.activation_functions[layer - 1]\n",
    "            arg_to_pass_to_eval = \"(Z[layer], prime=True)\"\n",
    "\n",
    "            DA = self.W[layer + 1].T @ self.delta[layer + 1]\n",
    "\n",
    "            # If dropout is applied\n",
    "            if self.dropout and layer == 1:\n",
    "                DA = np.multiply(DA, self.D) / self.keep_prob\n",
    "\n",
    "\n",
    "            self.delta[layer] = (\n",
    "                    DA *\n",
    "                    eval(active_function + arg_to_pass_to_eval)\n",
    "            )\n",
    "\n",
    "        for layer in range(1, self.layers):\n",
    "            self.gradient_W[layer] = (self.delta[layer] @ A[layer - 1].T) / self.m\n",
    "            self.gradient_B[layer] = np.sum(self.delta[layer], axis=1, keepdims=True) / self.m\n",
    "            \n",
    "            if self.penalty == \"l1\":\n",
    "                # https://towardsdatascience.com/only-numpy-implementing-different-combination-of-l1-norm-l2-norm-l1-regularization-and-14b01a9773b\n",
    "                self.gradient_W[layer] += np.where(self.W[layer] < 0, -1, 1) * (self.lambd / self.m)\n",
    "            elif self.penalty == \"l2\":\n",
    "                self.gradient_W[layer] += self.W[layer] * (self.lambd / self.m)\n",
    "\n",
    "        alpha = self.update_weights()\n",
    "        return alpha\n",
    "\n",
    "    def update_weights(self):\n",
    "        # decay the learning rate\n",
    "        if isinstance(self.alpha, dict):\n",
    "            alpha = self.initial_lr / (1. + self.decay * self.epoch_count)\n",
    "        else:\n",
    "            alpha = self.alpha\n",
    "\n",
    "\n",
    "        if self.optimizer == \"SGD\":\n",
    "            for layer in range(1, self.layers):\n",
    "                self.W[layer] -= alpha * self.gradient_W[layer]\n",
    "                self.B[layer] -= alpha * self.gradient_B[layer]\n",
    "\n",
    "        elif self.optimizer[\"method\"] == \"SGDM\":\n",
    "            for layer in range(1, self.layers):\n",
    "                beta = self.optimizer[\"beta\"]\n",
    "                self.v_dw[layer] = beta * self.v_dw[layer] + (1 - beta) * self.gradient_W[layer]\n",
    "                self.v_db[layer] = beta * self.v_db[layer] + (1 - beta) * self.gradient_B[layer]\n",
    "\n",
    "                self.W[layer] -= alpha * self.v_dw[layer]\n",
    "                self.B[layer] -= alpha * self.v_db[layer]\n",
    "\n",
    "        elif self.optimizer[\"method\"] == \"RMSP\":\n",
    "            for layer in range(1, self.layers):\n",
    "                beta = self.optimizer[\"beta\"]\n",
    "                self.s_dw[layer] = beta * self.s_dw[layer] + (1 - beta) * np.square(self.gradient_W[layer])\n",
    "                self.s_db[layer] = beta * self.s_db[layer] + (1 - beta) * np.square(self.gradient_B[layer])\n",
    "\n",
    "                w_rms_grad = self.gradient_W[layer] / (np.sqrt(self.s_dw[layer]) + self.EPSILON)\n",
    "                b_rms_grad = self.gradient_B[layer] / (np.sqrt(self.s_db[layer]) + self.EPSILON)\n",
    "\n",
    "                self.W[layer] -= alpha * w_rms_grad\n",
    "                self.B[layer] -= alpha * b_rms_grad\n",
    "\n",
    "        elif self.optimizer[\"method\"] == \"ADAM\":\n",
    "            # EWA: Exponential weighted average\n",
    "            # ToDo: Check if bias correction is necessary. The EWA will be inaccurate initially,\n",
    "            # but it shouldn't take many iterations to compute correct EWA.\n",
    "            for layer in range(1, self.layers):\n",
    "                beta1 = self.optimizer[\"beta1\"]\n",
    "                beta2 = self.optimizer[\"beta2\"]\n",
    "                self.v_dw[layer] = beta1 * self.v_dw[layer] + (1 - beta1) * self.gradient_W[layer]\n",
    "                self.v_db[layer] = beta1 * self.v_db[layer] + (1 - beta1) * self.gradient_B[layer]\n",
    "\n",
    "                self.s_dw[layer] = beta2 * self.s_dw[layer] + (1 - beta2) * np.square(self.gradient_W[layer])\n",
    "                self.s_db[layer] = beta2 * self.s_db[layer] + (1 - beta2) * np.square(self.gradient_B[layer])\n",
    "\n",
    "                v_dw_corrected = self.v_dw[layer] / (1 - beta1 ** self.no_of_iterations)\n",
    "                s_dw_corrected = self.s_dw[layer] / (1 - beta2 ** self.no_of_iterations)\n",
    "\n",
    "                v_db_corrected = self.v_db[layer] / (1 - beta1 ** self.no_of_iterations)\n",
    "                s_db_corrected = self.s_db[layer] / (1 - beta2 ** self.no_of_iterations)\n",
    "\n",
    "                self.W[layer] -= alpha * (v_dw_corrected / (np.sqrt(s_dw_corrected) + self.EPSILON))\n",
    "                self.B[layer] -= alpha * (v_db_corrected / (np.sqrt(s_db_corrected) + self.EPSILON))\n",
    "        \n",
    "        return alpha\n",
    "\n",
    "\n",
    "    def initialise_cache(self):\n",
    "        self.v_dw = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.v_db = np.empty_like(range(self.layers), dtype=object)\n",
    "    \n",
    "        self.s_dw = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.s_db = np.empty_like(range(self.layers), dtype=object)\n",
    "\n",
    "        for layer, (y, x) in zip(range(1, self.layers), self.weight_set_dimensions):\n",
    "            self.v_dw[layer] = np.zeros((y, x))\n",
    "            self.v_db[layer] = np.zeros((y, 1))\n",
    "            \n",
    "            self.s_dw[layer] = np.zeros((y, x))\n",
    "            self.s_db[layer] = np.zeros((y, 1))\n",
    "\n",
    "    @timeit\n",
    "    def fit(self, X, Y, X_val, Y_val):\n",
    "        self.m = X.shape[1] # where (no_of_features, no_of_training_examples)\n",
    "        self.initialise_weights()\n",
    "        self.initialise_cache()\n",
    "\n",
    "        # By default the method is SGD(Stochastic Gradient Descent) if one wishes to use\n",
    "        # the whole batch, simply pass the number of traning examples available as the\n",
    "        # batch size when instantiating the class\n",
    "        self.epoch_count = 0\n",
    "        self.no_of_iterations = 0\n",
    "\n",
    "        shuffled = np.arange(self.m)\n",
    "        if self.verbose:\n",
    "            print(\"Initialising weights...\")\n",
    "            print(\"Starting the training...\")\n",
    "            print(\"Initial cost: %.10f\\n\" % self.calculate_error(Y, self.get_A(X)[-1]))\n",
    "\n",
    "        for epoch_no in range(1, self.epoch + 1):\n",
    "            self.epoch_count += 1\n",
    "\n",
    "            np.random.shuffle(shuffled)\n",
    "            X_shuffled = X[:, shuffled]\n",
    "            Y_shuffled = Y[:, shuffled]\n",
    "\n",
    "            for i in range(0, self.m, self.mini_batch_size):\n",
    "                self.no_of_iterations += 1\n",
    "                X_mini_batch = X_shuffled[:, i: i + self.mini_batch_size]\n",
    "                Y_mini_batch = Y_shuffled[:, i: i + self.mini_batch_size]\n",
    "\n",
    "                A, Z = self.forwardpass(X_mini_batch)\n",
    "                alpha = self.backpropagation(Y_mini_batch, A, Z)\n",
    "\n",
    "                if self.no_of_iterations % 100 == 0 and self.verbose:\n",
    "                    self.display_information(X, Y, X_val, Y_val, alpha)\n",
    "\n",
    "\n",
    "    def predict(\n",
    "            self,\n",
    "            X: np.ndarray,\n",
    "            apply_dropout=False,\n",
    "            return_prob_matrix=False,\n",
    "    ):\n",
    "        \"\"\"Predict the output given the training data.\n",
    "\n",
    "            Returns the predicted values in two forms:\n",
    "\n",
    "            1.either by picking up the highest value along the columns for every row,\n",
    "                i.e. \"np.argmax(self.A[-1].T, axis=1)\"\n",
    "            2.or by returning a matrix that is in the shape of Y.T where each column\n",
    "                represents the probability of the instance belonging to that class.\n",
    "                Please note that every column in Y.T represents a class. To be able to\n",
    "                return the probability matrix, the final activation function must be\n",
    "                softmax!\n",
    "                i.e. \"array([0.9650488423, 0.0354737543, 0.0005225966])\"\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Training set in the shape of\n",
    "                (no_of_features, no_of_training examples).\n",
    "            return_prob_matrix (bool, optional): Returns the probability matrix if True.\n",
    "                Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray:\n",
    "\n",
    "            if return_prob_matrix is False, the output is in the shape of\n",
    "                (no_of_training_examples, 1)\n",
    "            if return_prob_matrix is True, the output is in the shape of\n",
    "                (no_of_training_examples, no_of_features)\n",
    "        \"\"\"\n",
    "        # here predict means if forwardpass is called from predict method\n",
    "        # if so, there is an option to apply or not apply dropout\n",
    "        predict = np.invert(apply_dropout)\n",
    "        A, Z = self.forwardpass(X, predict=predict)\n",
    "        if return_prob_matrix:\n",
    "            np.set_printoptions(precision=10, suppress=True)\n",
    "            return A[-1].T\n",
    "        return np.argmax(A[-1].T, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "x = data.data[:,[0,2]]\n",
    "y = data.target\n",
    "\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.layers import Dropout\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model_keras = Sequential()\n",
    "model_keras.add(Dense(4, input_dim=2, activation='relu'))\n",
    "model_keras.add(Dropout(0.5))\n",
    "model_keras.add(Dense(4, activation='relu'))\n",
    "model_keras.add(Dense(3, activation='softmax'))\n",
    "adam = optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model_keras.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model_keras.fit(X.T,Y.T, batch_size=10, epochs=100)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with benchmark datasets\n",
    "\n",
    "## 1.Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the dataset as test and train...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "x = data.data[:,[0,2]]\n",
    "y = data.target\n",
    "\n",
    "dataset_test, dataset_train = split_data_as(x, y, train=0.8, test=0.2)\n",
    "\n",
    "X_train = dataset_train[:, :-1].T\n",
    "Y_train = one_hot_encode(dataset_train[:, -1]).T\n",
    "\n",
    "X_test = dataset_test[:, :-1].T\n",
    "Y_test = one_hot_encode(dataset_test[:, -1]).T\n",
    "\n",
    "\n",
    "# X = x.T\n",
    "# Y = one_hot_encode(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 120)\n",
      "(3, 120)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 1.5365566066\n",
      "\n",
      "train_accuracy: 90.0 - epoch 9    iteration 100 - train loss 0.2821164384 ____ validation loss 0.0649487144 --- alpha: 0.01\n",
      "train_accuracy: 94.16667 - epoch 17    iteration 200 - train loss 0.1661307868 ____ validation loss 0.0342919567 --- alpha: 0.00556\n",
      "train_accuracy: 94.16667 - epoch 25    iteration 300 - train loss 0.1603909339 ____ validation loss 0.0357908724 --- alpha: 0.00385\n",
      "train_accuracy: 94.16667 - epoch 34    iteration 400 - train loss 0.1491435987 ____ validation loss 0.0291781662 --- alpha: 0.00286\n",
      "train_accuracy: 94.16667 - epoch 42    iteration 500 - train loss 0.1471502477 ____ validation loss 0.0289363383 --- alpha: 0.00233\n",
      "train_accuracy: 94.16667 - epoch 50    iteration 600 - train loss 0.145952248 ____ validation loss 0.0285908132 --- alpha: 0.00196\n",
      "train_accuracy: 94.16667 - epoch 59    iteration 700 - train loss 0.144385284 ____ validation loss 0.0267060122 --- alpha: 0.00167\n",
      "train_accuracy: 95.0 - epoch 67    iteration 800 - train loss 0.1473788445 ____ validation loss 0.0299364534 --- alpha: 0.00147\n",
      "train_accuracy: 94.16667 - epoch 75    iteration 900 - train loss 0.1438951539 ____ validation loss 0.0277683777 --- alpha: 0.00132\n",
      "train_accuracy: 94.16667 - epoch 84    iteration 1000 - train loss 0.1422501414 ____ validation loss 0.0258418965 --- alpha: 0.00118\n",
      "train_accuracy: 94.16667 - epoch 92    iteration 1100 - train loss 0.1425274669 ____ validation loss 0.0248538826 --- alpha: 0.00108\n",
      "train_accuracy: 94.16667 - epoch 100    iteration 1200 - train loss 0.1417574919 ____ validation loss 0.026338388 --- alpha: 0.00099\n",
      "func:'fit' -- took: 0.5463 sec\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(2, 'relu'),\n",
    "    hidden_layer=[(4,'relu'),(4,'softmax')],\n",
    "    output_layer=3,\n",
    "    batch_size=10,\n",
    "    optimizer={\"method\": \"ADAM\", \"beta1\": 0.9, \"beta2\": 0.999},\n",
    "    weight_initialisation = \"xavier_normal\",\n",
    "    keep_prob=1,\n",
    "    penalty=None,\n",
    "    epoch=100,\n",
    "    alpha={\"initial_lr\": 0.1, \"decay\": 1},\n",
    "    random_state=np.random.randint(0, 100)\n",
    ")\n",
    "\n",
    "model.fit(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F1': 99.99999998950001,\n",
       " 'accuracy': 99.99999999666667,\n",
       " 'false_positive_rate': 0.0,\n",
       " 'precision': 99.99999998999999,\n",
       " 'prevalence': 33.33333333222222,\n",
       " 'sensitivity/recall': 99.99999998999999,\n",
       " 'specificity': 99.999999995}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_model_performance(np.argmax(Y_test, axis=0),\n",
    "                           model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results_dict_all_models, results_average_dict, models = grid_search(\n",
    "#     x,\n",
    "#     y,\n",
    "#     clf=NeuralNetwork,\n",
    "#     lst_metrics=[\"F1\", \"accuracy\"],\n",
    "#     sort_by = \"accuracy\",\n",
    "#     n_folds=5,\n",
    "#     dict_param_grid={\n",
    "#         'batch_size': [8, 16, 32],\n",
    "#         'input_layer': [(2, 'relu')],\n",
    "#         'hidden_layer': [\n",
    "#             [(4,'relu'), (4,'softmax')],\n",
    "#             [(4,'sigmoid'),(4,'softmax')]\n",
    "#         ],\n",
    "#         'optimizer': [\n",
    "#             {\n",
    "#                 \"method\": \"RMSP\",\n",
    "#                 \"beta\": 0.9\n",
    "#             }\n",
    "#         ],\n",
    "#         'output_layer': [3],\n",
    "#         'alpha': [0.001],\n",
    "#         'verbose': [False],\n",
    "#         'epoch': [1000]\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\ma\\core.py:6461: MaskedArrayFutureWarning: In the future the default for ma.maximum.reduce will be axis=0, not the current None, to match np.maximum.reduce. Explicitly pass 0 or None to silence this warning.\n",
      "  return self.reduce(a)\n",
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\ma\\core.py:6461: MaskedArrayFutureWarning: In the future the default for ma.minimum.reduce will be axis=0, not the current None, to match np.minimum.reduce. Explicitly pass 0 or None to silence this warning.\n",
      "  return self.reduce(a)\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "dt = data.data[:,[0,2]]\n",
    "x_min, x_max = dt[:, 0].min() - 1, dt[:, 0].max() + 1\n",
    "y_min, y_max = dt[:, 1].min() - 1, dt[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()].T, apply_dropout=False)\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx, yy, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "\n",
    "plt.scatter(dt[:, 0], dt[:, 1], c=y,s=20, edgecolor='k')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('petal length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_loss(\n",
    "    model.train_error,\n",
    "    model.test_error\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Make Moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "x,y =make_moons(n_samples=1500, noise=.05)\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(X.shape[0], 'relu'),\n",
    "    hidden_layer=[(10,'relu'), (4,'softmax')],\n",
    "    output_layer=Y.shape[0],\n",
    "    batch_size=8,\n",
    "    optimizer=\n",
    "    {\n",
    "        \"method\": \"ADAM\",\n",
    "        \"beta1\": 0.9,\n",
    "        \"beta2\": 0.999\n",
    "    },\n",
    "    penalty = \"l2\",\n",
    "    keep_prob=0.5,\n",
    "    lambd=0.001,\n",
    "    epoch=250,\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Decision Boundaries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "dt = x\n",
    "x_min, x_max = dt[:, 0].min() - 0.5, dt[:, 0].max() + 0.5\n",
    "y_min, y_max = dt[:, 1].min() - 0.5, dt[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()].T, apply_dropout=False) \n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx, yy, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "plt.scatter(dt[:, 0], dt[:, 1], c=y, s=20, edgecolor='k')\n",
    "plt.title('Decision Boundaries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Andrew NG Assignment 2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex2data2 = np.loadtxt(\"../ex2/data/ex2data2.txt\", delimiter=\",\")\n",
    "\n",
    "x = ex2data2[:, :-1]\n",
    "y = ex2data2[:, -1]\n",
    "\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(X.shape[0], 'relu'),\n",
    "    hidden_layer=[(8,'relu'), (4,'softmax')],\n",
    "    output_layer=Y.shape[0],\n",
    "    batch_size=8,\n",
    "    optimizer={\"method\": \"ADAM\", \"beta1\": 0.9, \"beta2\": 0.999},\n",
    "    keep_prob=0.8,\n",
    "    epoch=1500,\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results_dict_all_models, results_average_dict, models = grid_search_stratified(\n",
    "#     x,\n",
    "#     y,\n",
    "#     clf=NeuralNetwork,\n",
    "#     metrics=[\"F1\", \"accuracy\"],\n",
    "#     sort_by = \"accuracy\",\n",
    "#     n_fold=6,\n",
    "#     param_grid_dict={\n",
    "#         'batch_size': [16, 32],\n",
    "#         'input_layer': [(2, 'relu')],\n",
    "#         'hidden_layer': [\n",
    "#             [(4,'relu'), (4,'relu'), (4,'softmax')],\n",
    "#             [(4,'sigmoid'),(4,'softmax')]\n",
    "#         ],\n",
    "#         'output_layer': [2],\n",
    "#         'alpha': [2, 4],\n",
    "#         'verbose': [False],\n",
    "#         'epoch': [5000]\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_average_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(models[\"model_5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Decision Boundaries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "data = ex2data2\n",
    "\n",
    "x1_min, x1_max = data[:, 0].min() - 0.3, data[:, 0].max() + 0.3,\n",
    "x2_min, x2_max = data[:, 1].min() - 0.3, data[:, 1].max() + 0.3,\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx1.ravel(), xx2.ravel()].T, apply_dropout=False) \n",
    "\n",
    "negatives = ex2data2[ex2data2[:, -1] == 0]\n",
    "positives = ex2data2[ex2data2[:, -1] == 1]\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx1.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx1, xx2, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "plt.scatter(negatives[:, 0], negatives[:, 1],s=50, color='k')\n",
    "plt.scatter(positives[:, 0], positives[:, 1],s=50, color='r')\n",
    "plt.title('Decision Boundaries')\n",
    "\n",
    "plt.contour(xx1, xx2, Z, [0.5], linewidths=2, colors=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__globals__', '__header__', 'y', '__version__', 'X'])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "data = loadmat('../ex3/data/ex3data1.mat')\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = data[\"X\"]\n",
    "y = data[\"y\"]\n",
    "y[y==10] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the dataset as test, train and validation...\n"
     ]
    }
   ],
   "source": [
    "dataset_test, dataset_train, dataset_validation = split_data_as(x, y, train=0.85, test=0.05, validation=0.1)\n",
    "X_train = dataset_train[:, :-1].T\n",
    "Y_train = one_hot_encode(dataset_train[:, -1]).T\n",
    "\n",
    "X_test = dataset_test[:, :-1].T\n",
    "Y_test = one_hot_encode(dataset_test[:, -1]).T\n",
    "\n",
    "X_validation = dataset_validation[:, :-1].T\n",
    "Y_validation = one_hot_encode(dataset_validation[:, -1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "sample = np.random.choice(data[\"X\"].shape[0], 20)\n",
    "ax.imshow(data[\"X\"][sample,1:].reshape(-1,20).T)\n",
    "ax.axis('off');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4260"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacing = 100\n",
    "training_sizes = np.int_(np.linspace(0, X_train.shape[1], num=10))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 473,  946, 1420, 1893, 2366, 2840, 3313, 3786, 4260])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4260)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(X_train.shape[0], 'relu'),\n",
    "    hidden_layer=[(200,'relu'),(100,'relu'),(50,'softmax')],\n",
    "    output_layer=Y_train.shape[0],\n",
    "    batch_size=48,\n",
    "    optimizer={\"method\": \"ADAM\", \"beta1\": 0.9, \"beta2\": 0.999},\n",
    "    weight_initialisation = \"xavier_uniform\",\n",
    "    metrics=\"accuracy\",\n",
    "    penalty=\"l2\",\n",
    "    lambd=0.1,\n",
    "    keep_prob=0.5,\n",
    "    epoch=200,\n",
    "    alpha={\"initial_lr\": 0.01, \"decay\": 1},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "def learning_curve(model, spacing, X_train, Y_train, X_validation, Y_validation):\n",
    "    training_sizes = np.int_(np.linspace(0, X_train.shape[1], num=spacing+1))[1:]\n",
    "    dict_error_values = {}\n",
    "    for training_size in training_sizes:\n",
    "        data_to_train_X = X_train[:, :training_size]\n",
    "        data_to_train_Y = Y_train[:, :training_size]\n",
    "        \n",
    "        print(\"Running the model with %s training samples\" %training_size)\n",
    "        print(\"*\" * 10)\n",
    "        print(\"\\n\")\n",
    "        model.fit(data_to_train_X, data_to_train_Y, X_validation, Y_validation)\n",
    "        dict_error_values[training_size] = (model.train_error[-1], model.validation_error[-1])\n",
    "    \n",
    "    return dict_error_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.2351799138\n",
      "\n",
      "train_accuracy: 100.0 - epoch 12    iteration 100 - train loss 0.0182109009 ____ validation loss 14.3938822181 --- alpha: 0.00077\n",
      "train_accuracy: 100.0 - epoch 23    iteration 200 - train loss 0.0138466517 ____ validation loss 14.1384591458 --- alpha: 0.00042\n",
      "train_accuracy: 100.0 - epoch 34    iteration 300 - train loss 0.0115910019 ____ validation loss 13.7161801444 --- alpha: 0.00029\n",
      "train_accuracy: 100.0 - epoch 45    iteration 400 - train loss 0.0101178395 ____ validation loss 13.1873773424 --- alpha: 0.00022\n",
      "train_accuracy: 100.0 - epoch 56    iteration 500 - train loss 0.0090454333 ____ validation loss 12.5699423698 --- alpha: 0.00018\n",
      "train_accuracy: 100.0 - epoch 67    iteration 600 - train loss 0.0082137169 ____ validation loss 11.888417924 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 78    iteration 700 - train loss 0.007541808 ____ validation loss 11.1817667647 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 89    iteration 800 - train loss 0.0069843911 ____ validation loss 10.4784923664 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 100    iteration 900 - train loss 0.0065149683 ____ validation loss 9.7971593161 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 112    iteration 1000 - train loss 0.0061186014 ____ validation loss 9.1530667016 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 123    iteration 1100 - train loss 0.0057882295 ____ validation loss 8.5547680707 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 134    iteration 1200 - train loss 0.0055220867 ____ validation loss 8.0160007423 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 145    iteration 1300 - train loss 0.0053226566 ____ validation loss 7.5341355451 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 156    iteration 1400 - train loss 0.0051912014 ____ validation loss 7.1163056242 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 167    iteration 1500 - train loss 0.0051198204 ____ validation loss 6.7706561565 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 178    iteration 1600 - train loss 0.0050981299 ____ validation loss 6.4886543119 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 189    iteration 1700 - train loss 0.0051129045 ____ validation loss 6.2617103272 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 200    iteration 1800 - train loss 0.0051322897 ____ validation loss 6.0926720081 --- alpha: 5e-05\n",
      "func:'fit' -- took: 18.2857 sec\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.2931153462\n",
      "\n",
      "train_accuracy: 100.0 - epoch 6    iteration 100 - train loss 0.0106511359 ____ validation loss 12.4292554602 --- alpha: 0.00143\n",
      "train_accuracy: 100.0 - epoch 12    iteration 200 - train loss 0.0080596725 ____ validation loss 11.5360394272 --- alpha: 0.00077\n",
      "train_accuracy: 100.0 - epoch 17    iteration 300 - train loss 0.006582249 ____ validation loss 10.3872203336 --- alpha: 0.00056\n",
      "train_accuracy: 100.0 - epoch 23    iteration 400 - train loss 0.0062574226 ____ validation loss 9.2997437946 --- alpha: 0.00042\n",
      "train_accuracy: 100.0 - epoch 28    iteration 500 - train loss 0.0055132555 ____ validation loss 8.3021429246 --- alpha: 0.00034\n",
      "train_accuracy: 100.0 - epoch 34    iteration 600 - train loss 0.0053288749 ____ validation loss 7.5481752121 --- alpha: 0.00029\n",
      "train_accuracy: 100.0 - epoch 39    iteration 700 - train loss 0.0056531056 ____ validation loss 6.9580695056 --- alpha: 0.00025\n",
      "train_accuracy: 100.0 - epoch 45    iteration 800 - train loss 0.0057821104 ____ validation loss 6.4774395666 --- alpha: 0.00022\n",
      "train_accuracy: 100.0 - epoch 50    iteration 900 - train loss 0.0059679052 ____ validation loss 6.1737768334 --- alpha: 0.0002\n",
      "train_accuracy: 100.0 - epoch 56    iteration 1000 - train loss 0.0061965498 ____ validation loss 6.0046350804 --- alpha: 0.00018\n",
      "train_accuracy: 100.0 - epoch 62    iteration 1100 - train loss 0.0063027031 ____ validation loss 5.837016683 --- alpha: 0.00016\n",
      "train_accuracy: 100.0 - epoch 67    iteration 1200 - train loss 0.006466157 ____ validation loss 5.7416476201 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 73    iteration 1300 - train loss 0.0067432628 ____ validation loss 5.6449626428 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 78    iteration 1400 - train loss 0.006865172 ____ validation loss 5.5727168692 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 84    iteration 1500 - train loss 0.0066883122 ____ validation loss 5.5652181107 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 89    iteration 1600 - train loss 0.0066415141 ____ validation loss 5.5254808522 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 95    iteration 1700 - train loss 0.0065235176 ____ validation loss 5.5032114411 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 100    iteration 1800 - train loss 0.0066151715 ____ validation loss 5.4713373942 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 106    iteration 1900 - train loss 0.0065043567 ____ validation loss 5.4674000353 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 112    iteration 2000 - train loss 0.0065933717 ____ validation loss 5.4308876803 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 117    iteration 2100 - train loss 0.0064312973 ____ validation loss 5.4437116767 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 123    iteration 2200 - train loss 0.0064196619 ____ validation loss 5.4392777367 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 128    iteration 2300 - train loss 0.0067240062 ____ validation loss 5.3722722415 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 134    iteration 2400 - train loss 0.0068969328 ____ validation loss 5.334908658 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 139    iteration 2500 - train loss 0.0064984077 ____ validation loss 5.3547791534 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 145    iteration 2600 - train loss 0.006405389 ____ validation loss 5.3491944683 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 150    iteration 2700 - train loss 0.0062778085 ____ validation loss 5.3535924724 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 156    iteration 2800 - train loss 0.0063053076 ____ validation loss 5.340992143 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 162    iteration 2900 - train loss 0.0062979654 ____ validation loss 5.3363234914 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 167    iteration 3000 - train loss 0.0062854703 ____ validation loss 5.329206947 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 173    iteration 3100 - train loss 0.0061648811 ____ validation loss 5.3244081498 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 178    iteration 3200 - train loss 0.0062971916 ____ validation loss 5.2882110965 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 184    iteration 3300 - train loss 0.0062274126 ____ validation loss 5.2787490981 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 189    iteration 3400 - train loss 0.0062948799 ____ validation loss 5.2554857123 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 195    iteration 3500 - train loss 0.0061433215 ____ validation loss 5.2694734 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 200    iteration 3600 - train loss 0.0062956364 ____ validation loss 5.236061371 --- alpha: 5e-05\n",
      "func:'fit' -- took: 37.0552 sec\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.3499188428\n",
      "\n",
      "train_accuracy: 99.45227 - epoch 4    iteration 100 - train loss 0.0359637066 ____ validation loss 5.8074858688 --- alpha: 0.002\n",
      "train_accuracy: 99.76526 - epoch 8    iteration 200 - train loss 0.0239713975 ____ validation loss 5.2805627127 --- alpha: 0.00111\n",
      "train_accuracy: 99.92175 - epoch 12    iteration 300 - train loss 0.0191517967 ____ validation loss 5.0531130662 --- alpha: 0.00077\n",
      "train_accuracy: 99.92175 - epoch 15    iteration 400 - train loss 0.0172335266 ____ validation loss 4.9068312579 --- alpha: 0.00062\n",
      "train_accuracy: 100.0 - epoch 19    iteration 500 - train loss 0.0154486231 ____ validation loss 4.8848877447 --- alpha: 0.0005\n",
      "train_accuracy: 100.0 - epoch 23    iteration 600 - train loss 0.0137949471 ____ validation loss 4.776189371 --- alpha: 0.00042\n",
      "train_accuracy: 100.0 - epoch 26    iteration 700 - train loss 0.0140653689 ____ validation loss 4.7072831267 --- alpha: 0.00037\n",
      "train_accuracy: 100.0 - epoch 30    iteration 800 - train loss 0.013153191 ____ validation loss 4.614596788 --- alpha: 0.00032\n",
      "train_accuracy: 100.0 - epoch 34    iteration 900 - train loss 0.0135625968 ____ validation loss 4.5062064826 --- alpha: 0.00029\n",
      "train_accuracy: 100.0 - epoch 38    iteration 1000 - train loss 0.0126016446 ____ validation loss 4.5229444663 --- alpha: 0.00026\n",
      "train_accuracy: 100.0 - epoch 41    iteration 1100 - train loss 0.012620138 ____ validation loss 4.4862620113 --- alpha: 0.00024\n",
      "train_accuracy: 100.0 - epoch 45    iteration 1200 - train loss 0.012182559 ____ validation loss 4.4758108138 --- alpha: 0.00022\n",
      "train_accuracy: 100.0 - epoch 49    iteration 1300 - train loss 0.0139137281 ____ validation loss 4.5280903378 --- alpha: 0.0002\n",
      "train_accuracy: 100.0 - epoch 52    iteration 1400 - train loss 0.0127534819 ____ validation loss 4.3647240055 --- alpha: 0.00019\n",
      "train_accuracy: 100.0 - epoch 56    iteration 1500 - train loss 0.0120954879 ____ validation loss 4.4321786268 --- alpha: 0.00018\n",
      "train_accuracy: 100.0 - epoch 60    iteration 1600 - train loss 0.012114522 ____ validation loss 4.3575035895 --- alpha: 0.00016\n",
      "train_accuracy: 100.0 - epoch 63    iteration 1700 - train loss 0.0117019196 ____ validation loss 4.3904821349 --- alpha: 0.00016\n",
      "train_accuracy: 100.0 - epoch 67    iteration 1800 - train loss 0.0122188735 ____ validation loss 4.4000092939 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 71    iteration 1900 - train loss 0.0121980978 ____ validation loss 4.4334331214 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 75    iteration 2000 - train loss 0.011398767 ____ validation loss 4.3487892538 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 78    iteration 2100 - train loss 0.0114584549 ____ validation loss 4.3520028827 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 82    iteration 2200 - train loss 0.0114576572 ____ validation loss 4.3419411041 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 86    iteration 2300 - train loss 0.0117485302 ____ validation loss 4.3027332562 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 89    iteration 2400 - train loss 0.0116045282 ____ validation loss 4.2892460226 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 93    iteration 2500 - train loss 0.0114207746 ____ validation loss 4.3127857633 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 97    iteration 2600 - train loss 0.011235892 ____ validation loss 4.3029899307 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 100    iteration 2700 - train loss 0.0116150666 ____ validation loss 4.2902092487 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 104    iteration 2800 - train loss 0.0114857333 ____ validation loss 4.3510011015 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 108    iteration 2900 - train loss 0.0112783875 ____ validation loss 4.3173520017 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 112    iteration 3000 - train loss 0.0110599465 ____ validation loss 4.2899590425 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 115    iteration 3100 - train loss 0.0111705514 ____ validation loss 4.3044655727 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 119    iteration 3200 - train loss 0.0113761366 ____ validation loss 4.2924404817 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 123    iteration 3300 - train loss 0.0111575444 ____ validation loss 4.3215329134 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 126    iteration 3400 - train loss 0.0112193339 ____ validation loss 4.2745719003 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 130    iteration 3500 - train loss 0.0112421091 ____ validation loss 4.2674843586 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 134    iteration 3600 - train loss 0.0110074588 ____ validation loss 4.2777546305 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 138    iteration 3700 - train loss 0.0113614015 ____ validation loss 4.3044674787 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 141    iteration 3800 - train loss 0.0114054134 ____ validation loss 4.2671382908 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 145    iteration 3900 - train loss 0.0113739116 ____ validation loss 4.211476245 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 149    iteration 4000 - train loss 0.0110719697 ____ validation loss 4.2044249012 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 152    iteration 4100 - train loss 0.0109983154 ____ validation loss 4.2175383928 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 156    iteration 4200 - train loss 0.0114063413 ____ validation loss 4.1965954063 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 160    iteration 4300 - train loss 0.0111705015 ____ validation loss 4.2381504281 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 163    iteration 4400 - train loss 0.0109186661 ____ validation loss 4.2741327181 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 167    iteration 4500 - train loss 0.0108837704 ____ validation loss 4.249722348 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 171    iteration 4600 - train loss 0.0107327515 ____ validation loss 4.2327962416 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 175    iteration 4700 - train loss 0.0107557601 ____ validation loss 4.2148457736 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 178    iteration 4800 - train loss 0.0107623395 ____ validation loss 4.2282087107 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 182    iteration 4900 - train loss 0.0109989213 ____ validation loss 4.2292364247 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 186    iteration 5000 - train loss 0.0108975386 ____ validation loss 4.2241082519 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 189    iteration 5100 - train loss 0.0106806754 ____ validation loss 4.2382518253 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 193    iteration 5200 - train loss 0.0106599162 ____ validation loss 4.2177036121 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 197    iteration 5300 - train loss 0.0106193282 ____ validation loss 4.2541102056 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 200    iteration 5400 - train loss 0.0108553809 ____ validation loss 4.2448200202 --- alpha: 5e-05\n",
      "func:'fit' -- took: 56.4244 sec\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.3305104940\n",
      "\n",
      "train_accuracy: 98.88498 - epoch 3    iteration 100 - train loss 0.0653078573 ____ validation loss 5.2909982218 --- alpha: 0.0025\n",
      "train_accuracy: 99.70657 - epoch 6    iteration 200 - train loss 0.0376155152 ____ validation loss 4.8699679583 --- alpha: 0.00143\n",
      "train_accuracy: 99.76526 - epoch 9    iteration 300 - train loss 0.02958469 ____ validation loss 4.3374726558 --- alpha: 0.001\n",
      "train_accuracy: 99.88263 - epoch 12    iteration 400 - train loss 0.0226853349 ____ validation loss 4.4175117124 --- alpha: 0.00077\n",
      "train_accuracy: 100.0 - epoch 14    iteration 500 - train loss 0.02184554 ____ validation loss 4.2188670803 --- alpha: 0.00067\n",
      "train_accuracy: 99.94131 - epoch 17    iteration 600 - train loss 0.0176624898 ____ validation loss 4.280033295 --- alpha: 0.00056\n",
      "train_accuracy: 100.0 - epoch 20    iteration 700 - train loss 0.0180444478 ____ validation loss 4.0965517206 --- alpha: 0.00048\n",
      "train_accuracy: 100.0 - epoch 23    iteration 800 - train loss 0.0174765024 ____ validation loss 4.0488505625 --- alpha: 0.00042\n",
      "train_accuracy: 100.0 - epoch 25    iteration 900 - train loss 0.0212425317 ____ validation loss 4.0232222233 --- alpha: 0.00038\n",
      "train_accuracy: 100.0 - epoch 28    iteration 1000 - train loss 0.0163110107 ____ validation loss 3.9531546476 --- alpha: 0.00034\n",
      "train_accuracy: 100.0 - epoch 31    iteration 1100 - train loss 0.0163242053 ____ validation loss 3.9881695777 --- alpha: 0.00031\n",
      "train_accuracy: 100.0 - epoch 34    iteration 1200 - train loss 0.016092739 ____ validation loss 3.9037378865 --- alpha: 0.00029\n",
      "train_accuracy: 100.0 - epoch 37    iteration 1300 - train loss 0.0156422658 ____ validation loss 3.9472918197 --- alpha: 0.00026\n",
      "train_accuracy: 100.0 - epoch 39    iteration 1400 - train loss 0.0168706816 ____ validation loss 3.8513331789 --- alpha: 0.00025\n",
      "train_accuracy: 100.0 - epoch 42    iteration 1500 - train loss 0.0157953085 ____ validation loss 3.8831545028 --- alpha: 0.00023\n",
      "train_accuracy: 100.0 - epoch 45    iteration 1600 - train loss 0.0170728563 ____ validation loss 3.7848500676 --- alpha: 0.00022\n",
      "train_accuracy: 100.0 - epoch 48    iteration 1700 - train loss 0.0176216751 ____ validation loss 3.8522776234 --- alpha: 0.0002\n",
      "train_accuracy: 100.0 - epoch 50    iteration 1800 - train loss 0.0155171986 ____ validation loss 3.8330372895 --- alpha: 0.0002\n",
      "train_accuracy: 100.0 - epoch 53    iteration 1900 - train loss 0.0148220273 ____ validation loss 3.8546547993 --- alpha: 0.00019\n",
      "train_accuracy: 100.0 - epoch 56    iteration 2000 - train loss 0.0150092805 ____ validation loss 3.8220812411 --- alpha: 0.00018\n",
      "train_accuracy: 100.0 - epoch 59    iteration 2100 - train loss 0.0155221973 ____ validation loss 3.8054549718 --- alpha: 0.00017\n",
      "train_accuracy: 100.0 - epoch 62    iteration 2200 - train loss 0.0149901833 ____ validation loss 3.8311485813 --- alpha: 0.00016\n",
      "train_accuracy: 100.0 - epoch 64    iteration 2300 - train loss 0.0155066136 ____ validation loss 3.7593485309 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 67    iteration 2400 - train loss 0.0151954457 ____ validation loss 3.7549862542 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 70    iteration 2500 - train loss 0.0156208952 ____ validation loss 3.7285569168 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 73    iteration 2600 - train loss 0.0159176581 ____ validation loss 3.7450365681 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 75    iteration 2700 - train loss 0.0150650608 ____ validation loss 3.7732996881 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 78    iteration 2800 - train loss 0.0148987813 ____ validation loss 3.7991958766 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 81    iteration 2900 - train loss 0.0148014856 ____ validation loss 3.7913265892 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 84    iteration 3000 - train loss 0.0149754847 ____ validation loss 3.7482133221 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 87    iteration 3100 - train loss 0.0156273012 ____ validation loss 3.7726696548 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 89    iteration 3200 - train loss 0.0150417935 ____ validation loss 3.7567468261 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 92    iteration 3300 - train loss 0.0144460663 ____ validation loss 3.7772731894 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 95    iteration 3400 - train loss 0.0150535337 ____ validation loss 3.7643182188 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 98    iteration 3500 - train loss 0.0150925949 ____ validation loss 3.7473524577 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 100    iteration 3600 - train loss 0.0151916529 ____ validation loss 3.7668226525 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 103    iteration 3700 - train loss 0.0150558733 ____ validation loss 3.7537705535 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 106    iteration 3800 - train loss 0.0148527871 ____ validation loss 3.7582651101 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 109    iteration 3900 - train loss 0.015115454 ____ validation loss 3.7852448338 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 112    iteration 4000 - train loss 0.0147290282 ____ validation loss 3.7239853862 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 114    iteration 4100 - train loss 0.0146074701 ____ validation loss 3.7288174029 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 117    iteration 4200 - train loss 0.0143443728 ____ validation loss 3.7324693025 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 120    iteration 4300 - train loss 0.0145686734 ____ validation loss 3.7492386167 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 123    iteration 4400 - train loss 0.0148319602 ____ validation loss 3.7568666264 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 125    iteration 4500 - train loss 0.0144988729 ____ validation loss 3.777778975 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 128    iteration 4600 - train loss 0.0144487573 ____ validation loss 3.7856076864 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 131    iteration 4700 - train loss 0.014821529 ____ validation loss 3.7602552431 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 134    iteration 4800 - train loss 0.014815714 ____ validation loss 3.7151643628 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 137    iteration 4900 - train loss 0.0149074171 ____ validation loss 3.7169332951 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 139    iteration 5000 - train loss 0.0147202126 ____ validation loss 3.740846878 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 142    iteration 5100 - train loss 0.0147433554 ____ validation loss 3.7168889348 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 145    iteration 5200 - train loss 0.0145161704 ____ validation loss 3.7358359448 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 148    iteration 5300 - train loss 0.0144845466 ____ validation loss 3.747016705 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 150    iteration 5400 - train loss 0.0142293418 ____ validation loss 3.7559389089 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 153    iteration 5500 - train loss 0.0143598519 ____ validation loss 3.7587150359 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 156    iteration 5600 - train loss 0.0143737362 ____ validation loss 3.7871272629 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 159    iteration 5700 - train loss 0.0145183033 ____ validation loss 3.7522772443 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 162    iteration 5800 - train loss 0.0146471957 ____ validation loss 3.7592592768 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 164    iteration 5900 - train loss 0.0145401554 ____ validation loss 3.7322228001 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 167    iteration 6000 - train loss 0.0142381353 ____ validation loss 3.6929306201 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 170    iteration 6100 - train loss 0.0143423659 ____ validation loss 3.7318241311 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 173    iteration 6200 - train loss 0.014405159 ____ validation loss 3.7230015673 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 175    iteration 6300 - train loss 0.0144571633 ____ validation loss 3.724572893 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 178    iteration 6400 - train loss 0.0141526763 ____ validation loss 3.7320085531 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 181    iteration 6500 - train loss 0.0143517235 ____ validation loss 3.7509279174 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 184    iteration 6600 - train loss 0.0144107162 ____ validation loss 3.7250386881 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 187    iteration 6700 - train loss 0.0145032891 ____ validation loss 3.7275639175 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 189    iteration 6800 - train loss 0.014259427 ____ validation loss 3.7351628937 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 192    iteration 6900 - train loss 0.0142168803 ____ validation loss 3.7133010254 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 195    iteration 7000 - train loss 0.0145269749 ____ validation loss 3.7632895113 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 198    iteration 7100 - train loss 0.0143249732 ____ validation loss 3.7450662656 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 200    iteration 7200 - train loss 0.0143426681 ____ validation loss 3.7189932625 --- alpha: 5e-05\n",
      "func:'fit' -- took: 77.4462 sec\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.3015087343\n",
      "\n",
      "train_accuracy: 98.73239 - epoch 3    iteration 100 - train loss 0.1030358447 ____ validation loss 3.5475440555 --- alpha: 0.0025\n",
      "train_accuracy: 99.15493 - epoch 5    iteration 200 - train loss 0.0538742684 ____ validation loss 3.822192563 --- alpha: 0.00167\n",
      "train_accuracy: 99.53052 - epoch 7    iteration 300 - train loss 0.0425230911 ____ validation loss 3.676456367 --- alpha: 0.00125\n",
      "train_accuracy: 99.81221 - epoch 9    iteration 400 - train loss 0.0342334049 ____ validation loss 3.6584246593 --- alpha: 0.001\n",
      "train_accuracy: 99.76526 - epoch 12    iteration 500 - train loss 0.0320854797 ____ validation loss 3.6044640124 --- alpha: 0.00077\n",
      "train_accuracy: 99.81221 - epoch 14    iteration 600 - train loss 0.0249170621 ____ validation loss 3.6925847105 --- alpha: 0.00067\n",
      "train_accuracy: 99.81221 - epoch 16    iteration 700 - train loss 0.0241855087 ____ validation loss 3.6208550763 --- alpha: 0.00059\n",
      "train_accuracy: 100.0 - epoch 18    iteration 800 - train loss 0.0229094343 ____ validation loss 3.5571181901 --- alpha: 0.00053\n",
      "train_accuracy: 99.95305 - epoch 20    iteration 900 - train loss 0.0207228101 ____ validation loss 3.5883223912 --- alpha: 0.00048\n",
      "train_accuracy: 100.0 - epoch 23    iteration 1000 - train loss 0.0216487533 ____ validation loss 3.4935531726 --- alpha: 0.00042\n",
      "train_accuracy: 100.0 - epoch 25    iteration 1100 - train loss 0.0217871202 ____ validation loss 3.5481485944 --- alpha: 0.00038\n",
      "train_accuracy: 100.0 - epoch 27    iteration 1200 - train loss 0.0224117672 ____ validation loss 3.3645330916 --- alpha: 0.00036\n",
      "train_accuracy: 100.0 - epoch 29    iteration 1300 - train loss 0.0200816829 ____ validation loss 3.4347364906 --- alpha: 0.00033\n",
      "train_accuracy: 100.0 - epoch 32    iteration 1400 - train loss 0.0210106175 ____ validation loss 3.3404987651 --- alpha: 0.0003\n",
      "train_accuracy: 100.0 - epoch 34    iteration 1500 - train loss 0.0196768665 ____ validation loss 3.4157171165 --- alpha: 0.00029\n",
      "train_accuracy: 100.0 - epoch 36    iteration 1600 - train loss 0.020669429 ____ validation loss 3.3494371166 --- alpha: 0.00027\n",
      "train_accuracy: 100.0 - epoch 38    iteration 1700 - train loss 0.0201802966 ____ validation loss 3.3250031567 --- alpha: 0.00026\n",
      "train_accuracy: 100.0 - epoch 40    iteration 1800 - train loss 0.0206697339 ____ validation loss 3.3137486379 --- alpha: 0.00024\n",
      "train_accuracy: 100.0 - epoch 43    iteration 1900 - train loss 0.0208547761 ____ validation loss 3.3439929977 --- alpha: 0.00023\n",
      "train_accuracy: 100.0 - epoch 45    iteration 2000 - train loss 0.0199620681 ____ validation loss 3.3360586498 --- alpha: 0.00022\n",
      "train_accuracy: 100.0 - epoch 47    iteration 2100 - train loss 0.0188426702 ____ validation loss 3.3539196401 --- alpha: 0.00021\n",
      "train_accuracy: 100.0 - epoch 49    iteration 2200 - train loss 0.0192817417 ____ validation loss 3.3806255829 --- alpha: 0.0002\n",
      "train_accuracy: 100.0 - epoch 52    iteration 2300 - train loss 0.0193934342 ____ validation loss 3.2983925959 --- alpha: 0.00019\n",
      "train_accuracy: 100.0 - epoch 54    iteration 2400 - train loss 0.0202507144 ____ validation loss 3.218927966 --- alpha: 0.00018\n",
      "train_accuracy: 100.0 - epoch 56    iteration 2500 - train loss 0.0189678474 ____ validation loss 3.2759513112 --- alpha: 0.00018\n",
      "train_accuracy: 100.0 - epoch 58    iteration 2600 - train loss 0.020373587 ____ validation loss 3.2232842673 --- alpha: 0.00017\n",
      "train_accuracy: 100.0 - epoch 60    iteration 2700 - train loss 0.0201661447 ____ validation loss 3.2762172721 --- alpha: 0.00016\n",
      "train_accuracy: 100.0 - epoch 63    iteration 2800 - train loss 0.0188253336 ____ validation loss 3.2572147415 --- alpha: 0.00016\n",
      "train_accuracy: 100.0 - epoch 65    iteration 2900 - train loss 0.0185942534 ____ validation loss 3.2573773786 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 67    iteration 3000 - train loss 0.0192860153 ____ validation loss 3.2363283667 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 69    iteration 3100 - train loss 0.019002798 ____ validation loss 3.2704375119 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 72    iteration 3200 - train loss 0.0186923764 ____ validation loss 3.2426650372 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 74    iteration 3300 - train loss 0.0186878379 ____ validation loss 3.2836639783 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 76    iteration 3400 - train loss 0.0193989528 ____ validation loss 3.2671695758 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 78    iteration 3500 - train loss 0.0184916862 ____ validation loss 3.2603865183 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 80    iteration 3600 - train loss 0.0185907818 ____ validation loss 3.2500527594 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 83    iteration 3700 - train loss 0.0183840093 ____ validation loss 3.2309487981 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 85    iteration 3800 - train loss 0.0193424087 ____ validation loss 3.2341910171 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 87    iteration 3900 - train loss 0.0192489788 ____ validation loss 3.1628605387 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 89    iteration 4000 - train loss 0.019028498 ____ validation loss 3.1921865622 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 92    iteration 4100 - train loss 0.0186287515 ____ validation loss 3.229484637 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 94    iteration 4200 - train loss 0.018522966 ____ validation loss 3.2142376687 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 96    iteration 4300 - train loss 0.0187741304 ____ validation loss 3.2172366061 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 98    iteration 4400 - train loss 0.0179798225 ____ validation loss 3.2364938552 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 100    iteration 4500 - train loss 0.018061333 ____ validation loss 3.239537113 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 103    iteration 4600 - train loss 0.0180230032 ____ validation loss 3.2193426775 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 105    iteration 4700 - train loss 0.0184869415 ____ validation loss 3.1866293919 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 107    iteration 4800 - train loss 0.0184113686 ____ validation loss 3.2006065023 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 109    iteration 4900 - train loss 0.0181584652 ____ validation loss 3.2105795706 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 112    iteration 5000 - train loss 0.0178508063 ____ validation loss 3.2343566503 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 114    iteration 5100 - train loss 0.0185147969 ____ validation loss 3.1727518538 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 116    iteration 5200 - train loss 0.0179485974 ____ validation loss 3.2066731693 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 118    iteration 5300 - train loss 0.0179168947 ____ validation loss 3.2384255046 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 120    iteration 5400 - train loss 0.018371085 ____ validation loss 3.1932645284 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 123    iteration 5500 - train loss 0.0184042953 ____ validation loss 3.2090485794 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 125    iteration 5600 - train loss 0.0183438221 ____ validation loss 3.2265244913 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 127    iteration 5700 - train loss 0.0181831013 ____ validation loss 3.2447857519 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 129    iteration 5800 - train loss 0.0180777786 ____ validation loss 3.2259785449 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 132    iteration 5900 - train loss 0.0182202703 ____ validation loss 3.2164097385 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 134    iteration 6000 - train loss 0.01777955 ____ validation loss 3.1991619609 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 136    iteration 6100 - train loss 0.0180028808 ____ validation loss 3.2174947483 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 138    iteration 6200 - train loss 0.0183384366 ____ validation loss 3.1582465824 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 140    iteration 6300 - train loss 0.0177397414 ____ validation loss 3.2145598993 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 143    iteration 6400 - train loss 0.0181210758 ____ validation loss 3.2323207988 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 145    iteration 6500 - train loss 0.0180717462 ____ validation loss 3.1993864846 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 147    iteration 6600 - train loss 0.0180596669 ____ validation loss 3.2123017696 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 149    iteration 6700 - train loss 0.0180558281 ____ validation loss 3.1899890296 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 152    iteration 6800 - train loss 0.0180843566 ____ validation loss 3.1928944633 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 154    iteration 6900 - train loss 0.0177979655 ____ validation loss 3.2055550176 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 156    iteration 7000 - train loss 0.0177626534 ____ validation loss 3.2142012037 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 158    iteration 7100 - train loss 0.018043835 ____ validation loss 3.1853411475 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 160    iteration 7200 - train loss 0.017929539 ____ validation loss 3.2042914337 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 163    iteration 7300 - train loss 0.0176713828 ____ validation loss 3.2125434544 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 165    iteration 7400 - train loss 0.017609273 ____ validation loss 3.1999390411 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 167    iteration 7500 - train loss 0.0176995385 ____ validation loss 3.1995573695 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 169    iteration 7600 - train loss 0.0180366674 ____ validation loss 3.1984281936 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 172    iteration 7700 - train loss 0.0176856362 ____ validation loss 3.1975748198 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 174    iteration 7800 - train loss 0.0180162374 ____ validation loss 3.1966104055 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 176    iteration 7900 - train loss 0.0176986886 ____ validation loss 3.2145646377 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 178    iteration 8000 - train loss 0.0178354417 ____ validation loss 3.1871014286 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 180    iteration 8100 - train loss 0.0182055285 ____ validation loss 3.1823946968 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 183    iteration 8200 - train loss 0.0178580429 ____ validation loss 3.2037596536 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 185    iteration 8300 - train loss 0.0173492158 ____ validation loss 3.1960416049 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 187    iteration 8400 - train loss 0.0178639648 ____ validation loss 3.1922009491 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 189    iteration 8500 - train loss 0.017584548 ____ validation loss 3.1974220505 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 192    iteration 8600 - train loss 0.0180283764 ____ validation loss 3.2004122066 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 194    iteration 8700 - train loss 0.0175512471 ____ validation loss 3.1956266117 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 196    iteration 8800 - train loss 0.0177931673 ____ validation loss 3.1887899055 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 198    iteration 8900 - train loss 0.0180327121 ____ validation loss 3.2111366153 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 200    iteration 9000 - train loss 0.0175049291 ____ validation loss 3.2064696262 --- alpha: 5e-05\n",
      "func:'fit' -- took: 97.2027 sec\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.3333119781\n",
      "\n",
      "train_accuracy: 97.14397 - epoch 2    iteration 100 - train loss 0.1315744441 ____ validation loss 3.0901351269 --- alpha: 0.00333\n",
      "train_accuracy: 98.98279 - epoch 4    iteration 200 - train loss 0.0766491254 ____ validation loss 3.1431138439 --- alpha: 0.002\n",
      "train_accuracy: 99.21753 - epoch 6    iteration 300 - train loss 0.0582671131 ____ validation loss 3.0456384207 --- alpha: 0.00143\n",
      "train_accuracy: 99.37402 - epoch 8    iteration 400 - train loss 0.0500021063 ____ validation loss 2.962456983 --- alpha: 0.00111\n",
      "train_accuracy: 99.64789 - epoch 10    iteration 500 - train loss 0.0426371926 ____ validation loss 2.8451235017 --- alpha: 0.00091\n",
      "train_accuracy: 99.80438 - epoch 12    iteration 600 - train loss 0.0403608196 ____ validation loss 2.8437397667 --- alpha: 0.00077\n",
      "train_accuracy: 99.80438 - epoch 13    iteration 700 - train loss 0.0342895779 ____ validation loss 2.8809393842 --- alpha: 0.00071\n",
      "train_accuracy: 99.92175 - epoch 15    iteration 800 - train loss 0.0320555303 ____ validation loss 2.9233633401 --- alpha: 0.00062\n",
      "train_accuracy: 99.96088 - epoch 17    iteration 900 - train loss 0.0318834008 ____ validation loss 2.7456814196 --- alpha: 0.00056\n",
      "train_accuracy: 99.84351 - epoch 19    iteration 1000 - train loss 0.0305826412 ____ validation loss 2.8657059294 --- alpha: 0.0005\n",
      "train_accuracy: 99.96088 - epoch 21    iteration 1100 - train loss 0.0279194748 ____ validation loss 2.8028206294 --- alpha: 0.00045\n",
      "train_accuracy: 99.96088 - epoch 23    iteration 1200 - train loss 0.0294095184 ____ validation loss 2.759976961 --- alpha: 0.00042\n",
      "train_accuracy: 99.96088 - epoch 25    iteration 1300 - train loss 0.0262733629 ____ validation loss 2.760066791 --- alpha: 0.00038\n",
      "train_accuracy: 99.96088 - epoch 26    iteration 1400 - train loss 0.0256733536 ____ validation loss 2.7614243925 --- alpha: 0.00037\n",
      "train_accuracy: 100.0 - epoch 28    iteration 1500 - train loss 0.0256500085 ____ validation loss 2.7301620904 --- alpha: 0.00034\n",
      "train_accuracy: 100.0 - epoch 30    iteration 1600 - train loss 0.0251527639 ____ validation loss 2.7072541797 --- alpha: 0.00032\n",
      "train_accuracy: 100.0 - epoch 32    iteration 1700 - train loss 0.0253849821 ____ validation loss 2.6883856501 --- alpha: 0.0003\n",
      "train_accuracy: 100.0 - epoch 34    iteration 1800 - train loss 0.0246112695 ____ validation loss 2.6790409324 --- alpha: 0.00029\n",
      "train_accuracy: 99.96088 - epoch 36    iteration 1900 - train loss 0.0247759949 ____ validation loss 2.7506451527 --- alpha: 0.00027\n",
      "train_accuracy: 100.0 - epoch 38    iteration 2000 - train loss 0.0241158086 ____ validation loss 2.701966707 --- alpha: 0.00026\n",
      "train_accuracy: 100.0 - epoch 39    iteration 2100 - train loss 0.0257280437 ____ validation loss 2.6428631374 --- alpha: 0.00025\n",
      "train_accuracy: 100.0 - epoch 41    iteration 2200 - train loss 0.0238486579 ____ validation loss 2.6468107751 --- alpha: 0.00024\n",
      "train_accuracy: 100.0 - epoch 43    iteration 2300 - train loss 0.023548521 ____ validation loss 2.6662451884 --- alpha: 0.00023\n",
      "train_accuracy: 100.0 - epoch 45    iteration 2400 - train loss 0.0248790255 ____ validation loss 2.6353544296 --- alpha: 0.00022\n",
      "train_accuracy: 100.0 - epoch 47    iteration 2500 - train loss 0.023844507 ____ validation loss 2.6468405528 --- alpha: 0.00021\n",
      "train_accuracy: 100.0 - epoch 49    iteration 2600 - train loss 0.0238251525 ____ validation loss 2.6571649082 --- alpha: 0.0002\n",
      "train_accuracy: 100.0 - epoch 50    iteration 2700 - train loss 0.0258204893 ____ validation loss 2.6468403496 --- alpha: 0.0002\n",
      "train_accuracy: 100.0 - epoch 52    iteration 2800 - train loss 0.0237243656 ____ validation loss 2.6273009201 --- alpha: 0.00019\n",
      "train_accuracy: 100.0 - epoch 54    iteration 2900 - train loss 0.0238234452 ____ validation loss 2.6593571919 --- alpha: 0.00018\n",
      "train_accuracy: 100.0 - epoch 56    iteration 3000 - train loss 0.0234729124 ____ validation loss 2.6047120082 --- alpha: 0.00018\n",
      "train_accuracy: 100.0 - epoch 58    iteration 3100 - train loss 0.0236014392 ____ validation loss 2.6001551917 --- alpha: 0.00017\n",
      "train_accuracy: 100.0 - epoch 60    iteration 3200 - train loss 0.0239713934 ____ validation loss 2.5934077886 --- alpha: 0.00016\n",
      "train_accuracy: 100.0 - epoch 62    iteration 3300 - train loss 0.0238191712 ____ validation loss 2.598758521 --- alpha: 0.00016\n",
      "train_accuracy: 100.0 - epoch 63    iteration 3400 - train loss 0.0236961762 ____ validation loss 2.5955402501 --- alpha: 0.00016\n",
      "train_accuracy: 100.0 - epoch 65    iteration 3500 - train loss 0.024029915 ____ validation loss 2.5622055012 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 67    iteration 3600 - train loss 0.0230024115 ____ validation loss 2.5930167334 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 69    iteration 3700 - train loss 0.022741293 ____ validation loss 2.6003284584 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 71    iteration 3800 - train loss 0.0229997076 ____ validation loss 2.5809045888 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 73    iteration 3900 - train loss 0.024262923 ____ validation loss 2.5911328339 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 75    iteration 4000 - train loss 0.0239473585 ____ validation loss 2.5593496182 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 76    iteration 4100 - train loss 0.0227725978 ____ validation loss 2.6041337527 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 78    iteration 4200 - train loss 0.0227524837 ____ validation loss 2.5778799553 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 80    iteration 4300 - train loss 0.0231167916 ____ validation loss 2.5867327719 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 82    iteration 4400 - train loss 0.0224610527 ____ validation loss 2.5839110943 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 84    iteration 4500 - train loss 0.0226934344 ____ validation loss 2.5920251441 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 86    iteration 4600 - train loss 0.022098759 ____ validation loss 2.5974417137 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 88    iteration 4700 - train loss 0.0234402822 ____ validation loss 2.5473602246 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 89    iteration 4800 - train loss 0.0230477479 ____ validation loss 2.5587970364 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 91    iteration 4900 - train loss 0.0237271382 ____ validation loss 2.5317679356 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 93    iteration 5000 - train loss 0.0225998299 ____ validation loss 2.5804834981 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 95    iteration 5100 - train loss 0.0225241928 ____ validation loss 2.5851347737 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 97    iteration 5200 - train loss 0.0227479319 ____ validation loss 2.5584016812 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 99    iteration 5300 - train loss 0.0223948735 ____ validation loss 2.5736290664 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 100    iteration 5400 - train loss 0.022140619 ____ validation loss 2.5871667343 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 102    iteration 5500 - train loss 0.0221412141 ____ validation loss 2.5766141632 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 104    iteration 5600 - train loss 0.0232079829 ____ validation loss 2.5534234537 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 106    iteration 5700 - train loss 0.0223094975 ____ validation loss 2.5801440314 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 108    iteration 5800 - train loss 0.0226472556 ____ validation loss 2.5793200067 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 110    iteration 5900 - train loss 0.0224248084 ____ validation loss 2.5663172054 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 112    iteration 6000 - train loss 0.0217939054 ____ validation loss 2.5666803136 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 113    iteration 6100 - train loss 0.0226996637 ____ validation loss 2.5475536323 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 115    iteration 6200 - train loss 0.02213829 ____ validation loss 2.5821158092 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 117    iteration 6300 - train loss 0.0220321281 ____ validation loss 2.5706530969 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 119    iteration 6400 - train loss 0.0221244815 ____ validation loss 2.5647138042 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 121    iteration 6500 - train loss 0.0220964093 ____ validation loss 2.5541191441 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 123    iteration 6600 - train loss 0.0215727846 ____ validation loss 2.592622554 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 125    iteration 6700 - train loss 0.0222881097 ____ validation loss 2.5643139311 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 126    iteration 6800 - train loss 0.0223618396 ____ validation loss 2.5577266721 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 128    iteration 6900 - train loss 0.0218886913 ____ validation loss 2.5697067265 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 130    iteration 7000 - train loss 0.0228289374 ____ validation loss 2.5392645153 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 132    iteration 7100 - train loss 0.0222588567 ____ validation loss 2.5486411945 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 134    iteration 7200 - train loss 0.0231730463 ____ validation loss 2.5273447716 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 136    iteration 7300 - train loss 0.0219157938 ____ validation loss 2.5540436346 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 138    iteration 7400 - train loss 0.0221054296 ____ validation loss 2.5584554585 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 139    iteration 7500 - train loss 0.0228615823 ____ validation loss 2.5501432603 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 141    iteration 7600 - train loss 0.0223395837 ____ validation loss 2.5682142576 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 143    iteration 7700 - train loss 0.0217583895 ____ validation loss 2.5662637726 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 145    iteration 7800 - train loss 0.0225076822 ____ validation loss 2.5177228912 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 147    iteration 7900 - train loss 0.0218223059 ____ validation loss 2.5533245989 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 149    iteration 8000 - train loss 0.0217795422 ____ validation loss 2.5682308796 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 150    iteration 8100 - train loss 0.0215483321 ____ validation loss 2.5808582999 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 152    iteration 8200 - train loss 0.0219049816 ____ validation loss 2.5538163911 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 154    iteration 8300 - train loss 0.0224291435 ____ validation loss 2.5421629794 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 156    iteration 8400 - train loss 0.0216983091 ____ validation loss 2.5589463502 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 158    iteration 8500 - train loss 0.0222234354 ____ validation loss 2.5649394614 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 160    iteration 8600 - train loss 0.0219041291 ____ validation loss 2.5402228325 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 162    iteration 8700 - train loss 0.0227406642 ____ validation loss 2.5765309065 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 163    iteration 8800 - train loss 0.0222032463 ____ validation loss 2.5427106448 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 165    iteration 8900 - train loss 0.0215378933 ____ validation loss 2.5526437277 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 167    iteration 9000 - train loss 0.0217789393 ____ validation loss 2.5578422221 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 169    iteration 9100 - train loss 0.0217295681 ____ validation loss 2.5607706973 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 171    iteration 9200 - train loss 0.0218892968 ____ validation loss 2.5528375768 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 173    iteration 9300 - train loss 0.0219572071 ____ validation loss 2.5741182123 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 175    iteration 9400 - train loss 0.0222093908 ____ validation loss 2.5582320722 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 176    iteration 9500 - train loss 0.0217009618 ____ validation loss 2.5708411425 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 178    iteration 9600 - train loss 0.0215965978 ____ validation loss 2.5642404905 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 180    iteration 9700 - train loss 0.0216178994 ____ validation loss 2.5775279569 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 182    iteration 9800 - train loss 0.0220890999 ____ validation loss 2.5552120583 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 184    iteration 9900 - train loss 0.0232976287 ____ validation loss 2.5608278318 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 186    iteration 10000 - train loss 0.021716863 ____ validation loss 2.5735354902 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 188    iteration 10100 - train loss 0.0217593957 ____ validation loss 2.5567739477 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 189    iteration 10200 - train loss 0.0217434712 ____ validation loss 2.5655344451 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 191    iteration 10300 - train loss 0.0219116965 ____ validation loss 2.5550305649 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 193    iteration 10400 - train loss 0.0214584041 ____ validation loss 2.5749675868 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 195    iteration 10500 - train loss 0.0225418479 ____ validation loss 2.534394539 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 197    iteration 10600 - train loss 0.0227483787 ____ validation loss 2.5537565814 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 199    iteration 10700 - train loss 0.0220829847 ____ validation loss 2.5462881286 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 200    iteration 10800 - train loss 0.0217072576 ____ validation loss 2.5710097784 --- alpha: 5e-05\n",
      "func:'fit' -- took: 115.4795 sec\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.3711566174\n",
      "\n",
      "train_accuracy: 96.47887 - epoch 2    iteration 100 - train loss 0.1562997252 ____ validation loss 2.4768166246 --- alpha: 0.00333\n",
      "train_accuracy: 97.68612 - epoch 4    iteration 200 - train loss 0.1291814301 ____ validation loss 2.2641914046 --- alpha: 0.002\n",
      "train_accuracy: 98.35681 - epoch 5    iteration 300 - train loss 0.0868694154 ____ validation loss 2.2318153782 --- alpha: 0.00167\n",
      "train_accuracy: 98.65862 - epoch 7    iteration 400 - train loss 0.0736467881 ____ validation loss 2.237435849 --- alpha: 0.00125\n",
      "train_accuracy: 99.29577 - epoch 8    iteration 500 - train loss 0.0590626712 ____ validation loss 2.2837439273 --- alpha: 0.00111\n",
      "train_accuracy: 99.53052 - epoch 10    iteration 600 - train loss 0.0464478761 ____ validation loss 2.238300442 --- alpha: 0.00091\n",
      "train_accuracy: 99.63112 - epoch 12    iteration 700 - train loss 0.0433336162 ____ validation loss 2.2719484766 --- alpha: 0.00077\n",
      "train_accuracy: 99.76526 - epoch 13    iteration 800 - train loss 0.0399981478 ____ validation loss 2.1770439063 --- alpha: 0.00071\n",
      "train_accuracy: 99.93293 - epoch 15    iteration 900 - train loss 0.0374396155 ____ validation loss 2.2222896977 --- alpha: 0.00062\n",
      "train_accuracy: 99.83233 - epoch 16    iteration 1000 - train loss 0.0373106343 ____ validation loss 2.1316816944 --- alpha: 0.00059\n",
      "train_accuracy: 99.96647 - epoch 18    iteration 1100 - train loss 0.0343356614 ____ validation loss 2.1160106826 --- alpha: 0.00053\n",
      "train_accuracy: 100.0 - epoch 20    iteration 1200 - train loss 0.0320827973 ____ validation loss 2.1153931302 --- alpha: 0.00048\n",
      "train_accuracy: 99.8994 - epoch 21    iteration 1300 - train loss 0.0353830942 ____ validation loss 2.1340013054 --- alpha: 0.00045\n",
      "train_accuracy: 99.96647 - epoch 23    iteration 1400 - train loss 0.0320651218 ____ validation loss 2.1322349919 --- alpha: 0.00042\n",
      "train_accuracy: 100.0 - epoch 24    iteration 1500 - train loss 0.0299066353 ____ validation loss 2.1036783954 --- alpha: 0.0004\n",
      "train_accuracy: 100.0 - epoch 26    iteration 1600 - train loss 0.0315057174 ____ validation loss 2.0403827301 --- alpha: 0.00037\n",
      "train_accuracy: 100.0 - epoch 27    iteration 1700 - train loss 0.0296867796 ____ validation loss 2.0643004927 --- alpha: 0.00036\n",
      "train_accuracy: 100.0 - epoch 29    iteration 1800 - train loss 0.0296297199 ____ validation loss 2.0767177005 --- alpha: 0.00033\n",
      "train_accuracy: 100.0 - epoch 31    iteration 1900 - train loss 0.0303395895 ____ validation loss 2.0396689588 --- alpha: 0.00031\n",
      "train_accuracy: 99.96647 - epoch 32    iteration 2000 - train loss 0.0326882225 ____ validation loss 2.0510097069 --- alpha: 0.0003\n",
      "train_accuracy: 100.0 - epoch 34    iteration 2100 - train loss 0.0297621878 ____ validation loss 1.9922247319 --- alpha: 0.00029\n",
      "train_accuracy: 100.0 - epoch 35    iteration 2200 - train loss 0.0279154476 ____ validation loss 2.0595038971 --- alpha: 0.00028\n",
      "train_accuracy: 99.96647 - epoch 37    iteration 2300 - train loss 0.027340495 ____ validation loss 2.0248706746 --- alpha: 0.00026\n",
      "train_accuracy: 100.0 - epoch 39    iteration 2400 - train loss 0.0285989713 ____ validation loss 2.0383103256 --- alpha: 0.00025\n",
      "train_accuracy: 100.0 - epoch 40    iteration 2500 - train loss 0.0287098416 ____ validation loss 2.0233680341 --- alpha: 0.00024\n",
      "train_accuracy: 100.0 - epoch 42    iteration 2600 - train loss 0.0269512954 ____ validation loss 2.0458218008 --- alpha: 0.00023\n",
      "train_accuracy: 100.0 - epoch 43    iteration 2700 - train loss 0.028597029 ____ validation loss 2.0154243693 --- alpha: 0.00023\n",
      "train_accuracy: 100.0 - epoch 45    iteration 2800 - train loss 0.0293322624 ____ validation loss 1.9992626123 --- alpha: 0.00022\n",
      "train_accuracy: 100.0 - epoch 47    iteration 2900 - train loss 0.026968623 ____ validation loss 2.0579138394 --- alpha: 0.00021\n",
      "train_accuracy: 100.0 - epoch 48    iteration 3000 - train loss 0.0271987857 ____ validation loss 1.993778712 --- alpha: 0.0002\n",
      "train_accuracy: 100.0 - epoch 50    iteration 3100 - train loss 0.0263376416 ____ validation loss 2.0352157738 --- alpha: 0.0002\n",
      "train_accuracy: 100.0 - epoch 51    iteration 3200 - train loss 0.0268437109 ____ validation loss 2.0191472579 --- alpha: 0.00019\n",
      "train_accuracy: 100.0 - epoch 53    iteration 3300 - train loss 0.0261753102 ____ validation loss 2.011247488 --- alpha: 0.00019\n",
      "train_accuracy: 100.0 - epoch 54    iteration 3400 - train loss 0.0281020464 ____ validation loss 2.048475647 --- alpha: 0.00018\n",
      "train_accuracy: 100.0 - epoch 56    iteration 3500 - train loss 0.0267779815 ____ validation loss 2.0567730102 --- alpha: 0.00018\n",
      "train_accuracy: 100.0 - epoch 58    iteration 3600 - train loss 0.0269773337 ____ validation loss 2.0049112899 --- alpha: 0.00017\n",
      "train_accuracy: 100.0 - epoch 59    iteration 3700 - train loss 0.0276687686 ____ validation loss 1.9684385586 --- alpha: 0.00017\n",
      "train_accuracy: 100.0 - epoch 61    iteration 3800 - train loss 0.0273372187 ____ validation loss 1.9771252221 --- alpha: 0.00016\n",
      "train_accuracy: 100.0 - epoch 62    iteration 3900 - train loss 0.0263558633 ____ validation loss 2.0136138515 --- alpha: 0.00016\n",
      "train_accuracy: 100.0 - epoch 64    iteration 4000 - train loss 0.0267714412 ____ validation loss 1.9925368412 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 66    iteration 4100 - train loss 0.0271366054 ____ validation loss 1.9568320456 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 67    iteration 4200 - train loss 0.0262215108 ____ validation loss 2.0020327579 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 69    iteration 4300 - train loss 0.0262771887 ____ validation loss 2.0174371972 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 70    iteration 4400 - train loss 0.0258756846 ____ validation loss 1.9990002535 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 72    iteration 4500 - train loss 0.0271466411 ____ validation loss 1.9920310656 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 74    iteration 4600 - train loss 0.0280009401 ____ validation loss 1.9993625295 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 75    iteration 4700 - train loss 0.0254701233 ____ validation loss 2.0154402144 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 77    iteration 4800 - train loss 0.0259954711 ____ validation loss 1.9980111316 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 78    iteration 4900 - train loss 0.0254020853 ____ validation loss 1.9874206081 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 80    iteration 5000 - train loss 0.0259611936 ____ validation loss 1.9832949663 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 81    iteration 5100 - train loss 0.0261264544 ____ validation loss 1.9970053966 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 83    iteration 5200 - train loss 0.0267569456 ____ validation loss 1.9917363344 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 85    iteration 5300 - train loss 0.0258347586 ____ validation loss 1.9767921732 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 86    iteration 5400 - train loss 0.0262195119 ____ validation loss 2.0022245146 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 88    iteration 5500 - train loss 0.0261360434 ____ validation loss 2.0189703921 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 89    iteration 5600 - train loss 0.0259770696 ____ validation loss 1.9720233724 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 91    iteration 5700 - train loss 0.0253250068 ____ validation loss 1.9833262991 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 93    iteration 5800 - train loss 0.02525739 ____ validation loss 1.9653020365 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 94    iteration 5900 - train loss 0.025267759 ____ validation loss 1.9851476749 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 96    iteration 6000 - train loss 0.0262817109 ____ validation loss 1.936336105 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 97    iteration 6100 - train loss 0.0250072049 ____ validation loss 1.97413011 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 99    iteration 6200 - train loss 0.0255002286 ____ validation loss 1.9844577693 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 100    iteration 6300 - train loss 0.0253178373 ____ validation loss 2.005468314 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 102    iteration 6400 - train loss 0.0249363466 ____ validation loss 2.0166868253 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 104    iteration 6500 - train loss 0.0255035411 ____ validation loss 1.9750588233 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 105    iteration 6600 - train loss 0.0255501332 ____ validation loss 2.004280395 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 107    iteration 6700 - train loss 0.0242815768 ____ validation loss 1.9886532333 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 108    iteration 6800 - train loss 0.0244381039 ____ validation loss 1.9862746991 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 110    iteration 6900 - train loss 0.024950616 ____ validation loss 1.9563791753 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 112    iteration 7000 - train loss 0.0250077143 ____ validation loss 1.9948954251 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 113    iteration 7100 - train loss 0.0260200958 ____ validation loss 1.9634812087 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 115    iteration 7200 - train loss 0.0254011204 ____ validation loss 1.9963682025 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 116    iteration 7300 - train loss 0.0251316709 ____ validation loss 1.9808780093 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 118    iteration 7400 - train loss 0.0250675429 ____ validation loss 1.9807057963 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 120    iteration 7500 - train loss 0.0244621578 ____ validation loss 2.0016373611 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 121    iteration 7600 - train loss 0.0246885741 ____ validation loss 1.9836302856 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 123    iteration 7700 - train loss 0.0246550413 ____ validation loss 1.9898183658 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 124    iteration 7800 - train loss 0.0241830841 ____ validation loss 2.0185768998 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 126    iteration 7900 - train loss 0.0246765597 ____ validation loss 1.9876278613 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 127    iteration 8000 - train loss 0.0250004173 ____ validation loss 1.9741886206 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 129    iteration 8100 - train loss 0.0253182327 ____ validation loss 2.00952564 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 131    iteration 8200 - train loss 0.0246127212 ____ validation loss 1.980940775 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 132    iteration 8300 - train loss 0.0247772398 ____ validation loss 1.9887064463 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 134    iteration 8400 - train loss 0.0247881234 ____ validation loss 1.9882718497 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 135    iteration 8500 - train loss 0.0247411755 ____ validation loss 1.9823085188 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 137    iteration 8600 - train loss 0.0251573521 ____ validation loss 2.0023389914 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 139    iteration 8700 - train loss 0.0247462634 ____ validation loss 1.9790672859 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 140    iteration 8800 - train loss 0.0242711664 ____ validation loss 1.9902820954 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 142    iteration 8900 - train loss 0.0246527765 ____ validation loss 1.9905443218 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 143    iteration 9000 - train loss 0.0254660865 ____ validation loss 2.0022756535 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 145    iteration 9100 - train loss 0.0248885287 ____ validation loss 1.9847577988 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 147    iteration 9200 - train loss 0.0252396187 ____ validation loss 1.9849327246 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 148    iteration 9300 - train loss 0.0249915744 ____ validation loss 1.9963266442 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 150    iteration 9400 - train loss 0.0249421631 ____ validation loss 1.9854353734 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 151    iteration 9500 - train loss 0.0243524199 ____ validation loss 2.0097616501 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 153    iteration 9600 - train loss 0.0244775888 ____ validation loss 1.9976448499 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 154    iteration 9700 - train loss 0.024479306 ____ validation loss 2.0050730214 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 156    iteration 9800 - train loss 0.0245957149 ____ validation loss 2.0186277286 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 158    iteration 9900 - train loss 0.0247803987 ____ validation loss 1.9867330818 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 159    iteration 10000 - train loss 0.024603704 ____ validation loss 2.0035638727 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 161    iteration 10100 - train loss 0.0243565097 ____ validation loss 2.0063108646 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 162    iteration 10200 - train loss 0.0246851808 ____ validation loss 1.9962780056 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 164    iteration 10300 - train loss 0.0242982627 ____ validation loss 1.9959550414 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 166    iteration 10400 - train loss 0.024205121 ____ validation loss 1.9959604476 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 167    iteration 10500 - train loss 0.0245587026 ____ validation loss 1.9771332036 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 169    iteration 10600 - train loss 0.0253427774 ____ validation loss 1.9734987229 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 170    iteration 10700 - train loss 0.0243017691 ____ validation loss 2.0143346581 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 172    iteration 10800 - train loss 0.0241712798 ____ validation loss 2.0099727219 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 174    iteration 10900 - train loss 0.0245978921 ____ validation loss 2.0039163052 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 175    iteration 11000 - train loss 0.0241442448 ____ validation loss 2.0138424845 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 177    iteration 11100 - train loss 0.0241592409 ____ validation loss 2.00164904 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 178    iteration 11200 - train loss 0.0243915481 ____ validation loss 2.0168814488 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 180    iteration 11300 - train loss 0.0242321388 ____ validation loss 1.9965785496 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 181    iteration 11400 - train loss 0.0239226972 ____ validation loss 2.0029886556 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 183    iteration 11500 - train loss 0.0237567885 ____ validation loss 1.9993498474 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 185    iteration 11600 - train loss 0.0237513985 ____ validation loss 2.0073603216 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 186    iteration 11700 - train loss 0.0238041737 ____ validation loss 1.9948346603 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 188    iteration 11800 - train loss 0.024535314 ____ validation loss 1.987367033 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 189    iteration 11900 - train loss 0.0240550904 ____ validation loss 1.993956746 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 191    iteration 12000 - train loss 0.0245751918 ____ validation loss 1.9768486981 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 193    iteration 12100 - train loss 0.024261684 ____ validation loss 2.006029134 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 194    iteration 12200 - train loss 0.0240330801 ____ validation loss 2.0026324215 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 196    iteration 12300 - train loss 0.0238923841 ____ validation loss 2.0154605587 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 197    iteration 12400 - train loss 0.0241589695 ____ validation loss 1.9978438231 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 199    iteration 12500 - train loss 0.0243933847 ____ validation loss 1.976532184 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 200    iteration 12600 - train loss 0.0238457601 ____ validation loss 1.99933871 --- alpha: 5e-05\n",
      "func:'fit' -- took: 136.2001 sec\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.3750006179\n",
      "\n",
      "train_accuracy: 95.89202 - epoch 2    iteration 100 - train loss 0.2054621326 ____ validation loss 1.8244805601 --- alpha: 0.00333\n",
      "train_accuracy: 96.39085 - epoch 3    iteration 200 - train loss 0.1548558651 ____ validation loss 1.8156910307 --- alpha: 0.0025\n",
      "train_accuracy: 97.68192 - epoch 5    iteration 300 - train loss 0.1175778 ____ validation loss 1.6910997197 --- alpha: 0.00167\n",
      "train_accuracy: 98.91432 - epoch 6    iteration 400 - train loss 0.079994008 ____ validation loss 1.7223784405 --- alpha: 0.00143\n",
      "train_accuracy: 99.06103 - epoch 8    iteration 500 - train loss 0.0738357892 ____ validation loss 1.6583197769 --- alpha: 0.00111\n",
      "train_accuracy: 99.20775 - epoch 9    iteration 600 - train loss 0.0647748485 ____ validation loss 1.6642197145 --- alpha: 0.001\n",
      "train_accuracy: 99.26643 - epoch 10    iteration 700 - train loss 0.0578081944 ____ validation loss 1.5624707761 --- alpha: 0.00091\n",
      "train_accuracy: 99.50117 - epoch 12    iteration 800 - train loss 0.0547785721 ____ validation loss 1.6173586386 --- alpha: 0.00077\n",
      "train_accuracy: 99.5892 - epoch 13    iteration 900 - train loss 0.0488184302 ____ validation loss 1.6151965445 --- alpha: 0.00071\n",
      "train_accuracy: 99.47183 - epoch 15    iteration 1000 - train loss 0.0530706332 ____ validation loss 1.5085861193 --- alpha: 0.00062\n",
      "train_accuracy: 99.64789 - epoch 16    iteration 1100 - train loss 0.0481879139 ____ validation loss 1.5889809274 --- alpha: 0.00059\n",
      "train_accuracy: 99.70657 - epoch 17    iteration 1200 - train loss 0.0484224287 ____ validation loss 1.5976265369 --- alpha: 0.00056\n",
      "train_accuracy: 99.82394 - epoch 19    iteration 1300 - train loss 0.0452984323 ____ validation loss 1.5371854376 --- alpha: 0.0005\n",
      "train_accuracy: 99.7946 - epoch 20    iteration 1400 - train loss 0.0423096819 ____ validation loss 1.5363598702 --- alpha: 0.00048\n",
      "train_accuracy: 99.82394 - epoch 22    iteration 1500 - train loss 0.0408131509 ____ validation loss 1.5117674025 --- alpha: 0.00043\n",
      "train_accuracy: 99.85329 - epoch 23    iteration 1600 - train loss 0.0416436109 ____ validation loss 1.5349235114 --- alpha: 0.00042\n",
      "train_accuracy: 99.88263 - epoch 24    iteration 1700 - train loss 0.0384000732 ____ validation loss 1.5446734123 --- alpha: 0.0004\n",
      "train_accuracy: 99.88263 - epoch 26    iteration 1800 - train loss 0.0376325027 ____ validation loss 1.5450073908 --- alpha: 0.00037\n",
      "train_accuracy: 99.94131 - epoch 27    iteration 1900 - train loss 0.0363117609 ____ validation loss 1.5205633742 --- alpha: 0.00036\n",
      "train_accuracy: 99.94131 - epoch 29    iteration 2000 - train loss 0.0363595609 ____ validation loss 1.5184720815 --- alpha: 0.00033\n",
      "train_accuracy: 99.94131 - epoch 30    iteration 2100 - train loss 0.0358871021 ____ validation loss 1.4898848768 --- alpha: 0.00032\n",
      "train_accuracy: 99.97066 - epoch 31    iteration 2200 - train loss 0.0355136556 ____ validation loss 1.461845285 --- alpha: 0.00031\n",
      "train_accuracy: 99.94131 - epoch 33    iteration 2300 - train loss 0.0352694461 ____ validation loss 1.5038496952 --- alpha: 0.00029\n",
      "train_accuracy: 99.94131 - epoch 34    iteration 2400 - train loss 0.0368798938 ____ validation loss 1.4846452598 --- alpha: 0.00029\n",
      "train_accuracy: 99.94131 - epoch 36    iteration 2500 - train loss 0.0353044094 ____ validation loss 1.4419337199 --- alpha: 0.00027\n",
      "train_accuracy: 100.0 - epoch 37    iteration 2600 - train loss 0.0337997091 ____ validation loss 1.5035827893 --- alpha: 0.00026\n",
      "train_accuracy: 99.97066 - epoch 39    iteration 2700 - train loss 0.0341456043 ____ validation loss 1.4927706838 --- alpha: 0.00025\n",
      "train_accuracy: 100.0 - epoch 40    iteration 2800 - train loss 0.0343074823 ____ validation loss 1.5142248649 --- alpha: 0.00024\n",
      "train_accuracy: 100.0 - epoch 41    iteration 2900 - train loss 0.032932184 ____ validation loss 1.4812742199 --- alpha: 0.00024\n",
      "train_accuracy: 100.0 - epoch 43    iteration 3000 - train loss 0.0330847391 ____ validation loss 1.4812795883 --- alpha: 0.00023\n",
      "train_accuracy: 100.0 - epoch 44    iteration 3100 - train loss 0.0344943473 ____ validation loss 1.4817862015 --- alpha: 0.00022\n",
      "train_accuracy: 100.0 - epoch 46    iteration 3200 - train loss 0.0337023236 ____ validation loss 1.462078264 --- alpha: 0.00021\n",
      "train_accuracy: 100.0 - epoch 47    iteration 3300 - train loss 0.0320319619 ____ validation loss 1.4691612067 --- alpha: 0.00021\n",
      "train_accuracy: 99.97066 - epoch 48    iteration 3400 - train loss 0.0320479922 ____ validation loss 1.4628426134 --- alpha: 0.0002\n",
      "train_accuracy: 100.0 - epoch 50    iteration 3500 - train loss 0.0323640742 ____ validation loss 1.4563577816 --- alpha: 0.0002\n",
      "train_accuracy: 99.97066 - epoch 51    iteration 3600 - train loss 0.0311830093 ____ validation loss 1.4757574864 --- alpha: 0.00019\n",
      "train_accuracy: 100.0 - epoch 53    iteration 3700 - train loss 0.03106056 ____ validation loss 1.4477741581 --- alpha: 0.00019\n",
      "train_accuracy: 99.97066 - epoch 54    iteration 3800 - train loss 0.0319794015 ____ validation loss 1.4318484319 --- alpha: 0.00018\n",
      "train_accuracy: 99.97066 - epoch 55    iteration 3900 - train loss 0.0312492007 ____ validation loss 1.4570221774 --- alpha: 0.00018\n",
      "train_accuracy: 100.0 - epoch 57    iteration 4000 - train loss 0.0312517328 ____ validation loss 1.4598973255 --- alpha: 0.00017\n",
      "train_accuracy: 99.97066 - epoch 58    iteration 4100 - train loss 0.0309203588 ____ validation loss 1.459215986 --- alpha: 0.00017\n",
      "train_accuracy: 100.0 - epoch 60    iteration 4200 - train loss 0.0320926966 ____ validation loss 1.4318516275 --- alpha: 0.00016\n",
      "train_accuracy: 100.0 - epoch 61    iteration 4300 - train loss 0.0315754193 ____ validation loss 1.4286418971 --- alpha: 0.00016\n",
      "train_accuracy: 99.97066 - epoch 62    iteration 4400 - train loss 0.0307049373 ____ validation loss 1.4574889387 --- alpha: 0.00016\n",
      "train_accuracy: 100.0 - epoch 64    iteration 4500 - train loss 0.0305834273 ____ validation loss 1.4354677863 --- alpha: 0.00015\n",
      "train_accuracy: 99.97066 - epoch 65    iteration 4600 - train loss 0.031956867 ____ validation loss 1.4647280753 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 67    iteration 4700 - train loss 0.0301770428 ____ validation loss 1.4515215638 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 68    iteration 4800 - train loss 0.0310502817 ____ validation loss 1.4284299863 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 70    iteration 4900 - train loss 0.0303870458 ____ validation loss 1.4248032001 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 71    iteration 5000 - train loss 0.0312856389 ____ validation loss 1.4271474869 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 72    iteration 5100 - train loss 0.0308918618 ____ validation loss 1.4254380477 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 74    iteration 5200 - train loss 0.0307131354 ____ validation loss 1.4121727887 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 75    iteration 5300 - train loss 0.0302673006 ____ validation loss 1.427806368 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 77    iteration 5400 - train loss 0.0302090986 ____ validation loss 1.4281652919 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 78    iteration 5500 - train loss 0.0297935215 ____ validation loss 1.4303315271 --- alpha: 0.00013\n",
      "train_accuracy: 99.97066 - epoch 79    iteration 5600 - train loss 0.029977281 ____ validation loss 1.4294581376 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 81    iteration 5700 - train loss 0.0314656686 ____ validation loss 1.4253443315 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 82    iteration 5800 - train loss 0.029997216 ____ validation loss 1.4211138731 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 84    iteration 5900 - train loss 0.030372989 ____ validation loss 1.4318042282 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 85    iteration 6000 - train loss 0.0310438591 ____ validation loss 1.4131746399 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 86    iteration 6100 - train loss 0.0300696282 ____ validation loss 1.4375986882 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 88    iteration 6200 - train loss 0.0309342301 ____ validation loss 1.4265839849 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 89    iteration 6300 - train loss 0.0312532284 ____ validation loss 1.4095003404 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 91    iteration 6400 - train loss 0.030336311 ____ validation loss 1.417424386 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 92    iteration 6500 - train loss 0.0297076906 ____ validation loss 1.4254100488 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 93    iteration 6600 - train loss 0.0296617121 ____ validation loss 1.4105609253 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 95    iteration 6700 - train loss 0.0298714574 ____ validation loss 1.4248501947 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 96    iteration 6800 - train loss 0.0300585259 ____ validation loss 1.4142036134 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 98    iteration 6900 - train loss 0.0297597373 ____ validation loss 1.4342309739 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 99    iteration 7000 - train loss 0.0302580646 ____ validation loss 1.4256140325 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 100    iteration 7100 - train loss 0.0295203513 ____ validation loss 1.4388234791 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 102    iteration 7200 - train loss 0.029773268 ____ validation loss 1.4107049382 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 103    iteration 7300 - train loss 0.0297151331 ____ validation loss 1.4082312085 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 105    iteration 7400 - train loss 0.0294866136 ____ validation loss 1.4155527264 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 106    iteration 7500 - train loss 0.0295508541 ____ validation loss 1.4340769711 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 108    iteration 7600 - train loss 0.0303985959 ____ validation loss 1.4082167319 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 109    iteration 7700 - train loss 0.0296726817 ____ validation loss 1.4073087204 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 110    iteration 7800 - train loss 0.0299636387 ____ validation loss 1.4188996351 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 112    iteration 7900 - train loss 0.0306510931 ____ validation loss 1.4130006973 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 113    iteration 8000 - train loss 0.0302682419 ____ validation loss 1.4219368339 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 115    iteration 8100 - train loss 0.0294229064 ____ validation loss 1.412295672 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 116    iteration 8200 - train loss 0.0295540525 ____ validation loss 1.4106760567 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 117    iteration 8300 - train loss 0.0296153005 ____ validation loss 1.4085730931 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 119    iteration 8400 - train loss 0.0307411445 ____ validation loss 1.3933613607 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 120    iteration 8500 - train loss 0.0287748242 ____ validation loss 1.4204013936 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 122    iteration 8600 - train loss 0.0291974196 ____ validation loss 1.4227609962 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 123    iteration 8700 - train loss 0.0289432022 ____ validation loss 1.4184560734 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 124    iteration 8800 - train loss 0.029670907 ____ validation loss 1.4149313309 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 126    iteration 8900 - train loss 0.029349891 ____ validation loss 1.4190819235 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 127    iteration 9000 - train loss 0.0290629641 ____ validation loss 1.4157732382 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 129    iteration 9100 - train loss 0.0293070396 ____ validation loss 1.421295269 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 130    iteration 9200 - train loss 0.0297551571 ____ validation loss 1.4278028387 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 131    iteration 9300 - train loss 0.0295877099 ____ validation loss 1.4195999129 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 133    iteration 9400 - train loss 0.0298526971 ____ validation loss 1.4054646846 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 134    iteration 9500 - train loss 0.028999733 ____ validation loss 1.4090119995 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 136    iteration 9600 - train loss 0.0289781694 ____ validation loss 1.4245026466 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 137    iteration 9700 - train loss 0.0294371222 ____ validation loss 1.4199708343 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 139    iteration 9800 - train loss 0.0285630829 ____ validation loss 1.4234887957 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 140    iteration 9900 - train loss 0.0287463975 ____ validation loss 1.4285248743 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 141    iteration 10000 - train loss 0.0291179912 ____ validation loss 1.4244610518 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 143    iteration 10100 - train loss 0.0294389823 ____ validation loss 1.4309999695 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 144    iteration 10200 - train loss 0.028551862 ____ validation loss 1.4142953513 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 146    iteration 10300 - train loss 0.0298791762 ____ validation loss 1.4006718359 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 147    iteration 10400 - train loss 0.0290968288 ____ validation loss 1.4283650677 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 148    iteration 10500 - train loss 0.02879373 ____ validation loss 1.42040396 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 150    iteration 10600 - train loss 0.0289412088 ____ validation loss 1.4137111022 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 151    iteration 10700 - train loss 0.0286420983 ____ validation loss 1.4220126598 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 153    iteration 10800 - train loss 0.0287804351 ____ validation loss 1.4303693936 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 154    iteration 10900 - train loss 0.0283694993 ____ validation loss 1.4369290979 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 155    iteration 11000 - train loss 0.0286918276 ____ validation loss 1.4211143555 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 157    iteration 11100 - train loss 0.0287078196 ____ validation loss 1.4349125666 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 158    iteration 11200 - train loss 0.0296861305 ____ validation loss 1.4224056963 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 160    iteration 11300 - train loss 0.0290851947 ____ validation loss 1.4143072826 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 161    iteration 11400 - train loss 0.028818377 ____ validation loss 1.4176049761 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 162    iteration 11500 - train loss 0.0286483084 ____ validation loss 1.4247182776 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 164    iteration 11600 - train loss 0.028450636 ____ validation loss 1.4181780233 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 165    iteration 11700 - train loss 0.0283089198 ____ validation loss 1.4171422721 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 167    iteration 11800 - train loss 0.0287443944 ____ validation loss 1.42101793 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 168    iteration 11900 - train loss 0.0289256543 ____ validation loss 1.4136324007 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 170    iteration 12000 - train loss 0.0285208015 ____ validation loss 1.4213118818 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 171    iteration 12100 - train loss 0.0286800801 ____ validation loss 1.418071831 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 172    iteration 12200 - train loss 0.0285938114 ____ validation loss 1.4175121643 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 174    iteration 12300 - train loss 0.0283050805 ____ validation loss 1.4223103651 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 175    iteration 12400 - train loss 0.0291665679 ____ validation loss 1.43393367 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 177    iteration 12500 - train loss 0.0284417337 ____ validation loss 1.4281206295 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 178    iteration 12600 - train loss 0.0282343596 ____ validation loss 1.4239335196 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 179    iteration 12700 - train loss 0.0292015612 ____ validation loss 1.4291667048 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 181    iteration 12800 - train loss 0.0286524647 ____ validation loss 1.4233611141 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 182    iteration 12900 - train loss 0.0285798842 ____ validation loss 1.4264472193 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 184    iteration 13000 - train loss 0.0287716737 ____ validation loss 1.4188113349 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 185    iteration 13100 - train loss 0.0286892359 ____ validation loss 1.4203386369 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 186    iteration 13200 - train loss 0.0288518001 ____ validation loss 1.4285482782 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 188    iteration 13300 - train loss 0.0290553623 ____ validation loss 1.422838976 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 189    iteration 13400 - train loss 0.02878688 ____ validation loss 1.4295810005 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 191    iteration 13500 - train loss 0.0282779842 ____ validation loss 1.4295401386 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 192    iteration 13600 - train loss 0.0283746428 ____ validation loss 1.4425361831 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 193    iteration 13700 - train loss 0.0285296281 ____ validation loss 1.4366008219 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 195    iteration 13800 - train loss 0.0286403179 ____ validation loss 1.4298357696 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 196    iteration 13900 - train loss 0.0286416107 ____ validation loss 1.4305364213 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 198    iteration 14000 - train loss 0.0285259455 ____ validation loss 1.4369254374 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 199    iteration 14100 - train loss 0.0285150089 ____ validation loss 1.4434644557 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 200    iteration 14200 - train loss 0.0284362831 ____ validation loss 1.4334449673 --- alpha: 5e-05\n",
      "func:'fit' -- took: 165.7134 sec\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.3633972896\n",
      "\n",
      "train_accuracy: 94.23579 - epoch 2    iteration 100 - train loss 0.248477932 ____ validation loss 1.0589912225 --- alpha: 0.00333\n",
      "train_accuracy: 97.00052 - epoch 3    iteration 200 - train loss 0.1456055733 ____ validation loss 0.9579014994 --- alpha: 0.0025\n",
      "train_accuracy: 96.92227 - epoch 4    iteration 300 - train loss 0.1264903181 ____ validation loss 0.9258414004 --- alpha: 0.002\n",
      "train_accuracy: 97.67866 - epoch 5    iteration 400 - train loss 0.1199260385 ____ validation loss 1.0055939362 --- alpha: 0.00167\n",
      "train_accuracy: 98.43505 - epoch 7    iteration 500 - train loss 0.0933283399 ____ validation loss 0.9221664123 --- alpha: 0.00125\n",
      "train_accuracy: 99.19144 - epoch 8    iteration 600 - train loss 0.0682553411 ____ validation loss 0.91957829 --- alpha: 0.00111\n",
      "train_accuracy: 99.34794 - epoch 9    iteration 700 - train loss 0.066612465 ____ validation loss 0.8811867127 --- alpha: 0.001\n",
      "train_accuracy: 99.24361 - epoch 10    iteration 800 - train loss 0.06766354 ____ validation loss 0.8626957263 --- alpha: 0.00091\n",
      "train_accuracy: 99.60876 - epoch 12    iteration 900 - train loss 0.0550610832 ____ validation loss 0.8863513438 --- alpha: 0.00077\n",
      "train_accuracy: 99.60876 - epoch 13    iteration 1000 - train loss 0.0504009909 ____ validation loss 0.8663715658 --- alpha: 0.00071\n",
      "train_accuracy: 99.50443 - epoch 14    iteration 1100 - train loss 0.0568510916 ____ validation loss 0.9089235479 --- alpha: 0.00067\n",
      "train_accuracy: 99.58268 - epoch 15    iteration 1200 - train loss 0.051826299 ____ validation loss 0.8742315462 --- alpha: 0.00062\n",
      "train_accuracy: 99.60876 - epoch 17    iteration 1300 - train loss 0.048804644 ____ validation loss 0.842710013 --- alpha: 0.00056\n",
      "train_accuracy: 99.58268 - epoch 18    iteration 1400 - train loss 0.0454395868 ____ validation loss 0.8623231047 --- alpha: 0.00053\n",
      "train_accuracy: 99.76526 - epoch 19    iteration 1500 - train loss 0.0453704445 ____ validation loss 0.8668109237 --- alpha: 0.0005\n",
      "train_accuracy: 99.79134 - epoch 20    iteration 1600 - train loss 0.045996972 ____ validation loss 0.8404879191 --- alpha: 0.00048\n",
      "train_accuracy: 99.76526 - epoch 22    iteration 1700 - train loss 0.0436283194 ____ validation loss 0.8923642828 --- alpha: 0.00043\n",
      "train_accuracy: 99.73918 - epoch 23    iteration 1800 - train loss 0.041996028 ____ validation loss 0.8764339871 --- alpha: 0.00042\n",
      "train_accuracy: 99.79134 - epoch 24    iteration 1900 - train loss 0.0408563786 ____ validation loss 0.8477530353 --- alpha: 0.0004\n",
      "train_accuracy: 99.81742 - epoch 25    iteration 2000 - train loss 0.043408146 ____ validation loss 0.850606496 --- alpha: 0.00038\n",
      "train_accuracy: 99.86959 - epoch 27    iteration 2100 - train loss 0.041667104 ____ validation loss 0.8353225491 --- alpha: 0.00036\n",
      "train_accuracy: 99.89567 - epoch 28    iteration 2200 - train loss 0.0416988781 ____ validation loss 0.8372294811 --- alpha: 0.00034\n",
      "train_accuracy: 99.84351 - epoch 29    iteration 2300 - train loss 0.0412161894 ____ validation loss 0.8884139996 --- alpha: 0.00033\n",
      "train_accuracy: 99.89567 - epoch 30    iteration 2400 - train loss 0.0403816833 ____ validation loss 0.8329104205 --- alpha: 0.00032\n",
      "train_accuracy: 99.89567 - epoch 32    iteration 2500 - train loss 0.0388070083 ____ validation loss 0.8326463437 --- alpha: 0.0003\n",
      "train_accuracy: 99.92175 - epoch 33    iteration 2600 - train loss 0.038645249 ____ validation loss 0.83896381 --- alpha: 0.00029\n",
      "train_accuracy: 99.97392 - epoch 34    iteration 2700 - train loss 0.0400273799 ____ validation loss 0.8636845037 --- alpha: 0.00029\n",
      "train_accuracy: 99.92175 - epoch 35    iteration 2800 - train loss 0.0383688045 ____ validation loss 0.8593803802 --- alpha: 0.00028\n",
      "train_accuracy: 99.94784 - epoch 37    iteration 2900 - train loss 0.0371911306 ____ validation loss 0.8517809307 --- alpha: 0.00026\n",
      "train_accuracy: 99.94784 - epoch 38    iteration 3000 - train loss 0.038392092 ____ validation loss 0.844527857 --- alpha: 0.00026\n",
      "train_accuracy: 99.94784 - epoch 39    iteration 3100 - train loss 0.0387540164 ____ validation loss 0.8618435808 --- alpha: 0.00025\n",
      "train_accuracy: 99.94784 - epoch 40    iteration 3200 - train loss 0.0376804566 ____ validation loss 0.8531557883 --- alpha: 0.00024\n",
      "train_accuracy: 99.94784 - epoch 42    iteration 3300 - train loss 0.0368504036 ____ validation loss 0.8482299032 --- alpha: 0.00023\n",
      "train_accuracy: 99.97392 - epoch 43    iteration 3400 - train loss 0.0369647439 ____ validation loss 0.8406388235 --- alpha: 0.00023\n",
      "train_accuracy: 100.0 - epoch 44    iteration 3500 - train loss 0.0361486926 ____ validation loss 0.825390474 --- alpha: 0.00022\n",
      "train_accuracy: 99.94784 - epoch 45    iteration 3600 - train loss 0.0392137389 ____ validation loss 0.8398819456 --- alpha: 0.00022\n",
      "train_accuracy: 100.0 - epoch 47    iteration 3700 - train loss 0.0376772968 ____ validation loss 0.8354947188 --- alpha: 0.00021\n",
      "train_accuracy: 99.92175 - epoch 48    iteration 3800 - train loss 0.0365350226 ____ validation loss 0.8384566536 --- alpha: 0.0002\n",
      "train_accuracy: 99.97392 - epoch 49    iteration 3900 - train loss 0.0374194665 ____ validation loss 0.8185820456 --- alpha: 0.0002\n",
      "train_accuracy: 100.0 - epoch 50    iteration 4000 - train loss 0.035972681 ____ validation loss 0.8257248573 --- alpha: 0.0002\n",
      "train_accuracy: 99.97392 - epoch 52    iteration 4100 - train loss 0.0356973981 ____ validation loss 0.8295858594 --- alpha: 0.00019\n",
      "train_accuracy: 99.97392 - epoch 53    iteration 4200 - train loss 0.0357017078 ____ validation loss 0.8487882703 --- alpha: 0.00019\n",
      "train_accuracy: 99.97392 - epoch 54    iteration 4300 - train loss 0.0367350052 ____ validation loss 0.834729141 --- alpha: 0.00018\n",
      "train_accuracy: 100.0 - epoch 55    iteration 4400 - train loss 0.0353531804 ____ validation loss 0.8375657716 --- alpha: 0.00018\n",
      "train_accuracy: 100.0 - epoch 57    iteration 4500 - train loss 0.0358484976 ____ validation loss 0.8259571331 --- alpha: 0.00017\n",
      "train_accuracy: 99.97392 - epoch 58    iteration 4600 - train loss 0.0342607693 ____ validation loss 0.8299343118 --- alpha: 0.00017\n",
      "train_accuracy: 100.0 - epoch 59    iteration 4700 - train loss 0.0358409062 ____ validation loss 0.8112091665 --- alpha: 0.00017\n",
      "train_accuracy: 100.0 - epoch 60    iteration 4800 - train loss 0.0348715294 ____ validation loss 0.8354187344 --- alpha: 0.00016\n",
      "train_accuracy: 99.94784 - epoch 62    iteration 4900 - train loss 0.0356981788 ____ validation loss 0.816057749 --- alpha: 0.00016\n",
      "train_accuracy: 100.0 - epoch 63    iteration 5000 - train loss 0.0349341592 ____ validation loss 0.839464699 --- alpha: 0.00016\n",
      "train_accuracy: 99.94784 - epoch 64    iteration 5100 - train loss 0.0350883811 ____ validation loss 0.8504709048 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 65    iteration 5200 - train loss 0.0338902089 ____ validation loss 0.8413451699 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 67    iteration 5300 - train loss 0.0344142992 ____ validation loss 0.8203284793 --- alpha: 0.00015\n",
      "train_accuracy: 100.0 - epoch 68    iteration 5400 - train loss 0.034096626 ____ validation loss 0.8107230455 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 69    iteration 5500 - train loss 0.0351128898 ____ validation loss 0.8114788233 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 70    iteration 5600 - train loss 0.0342891612 ____ validation loss 0.8425286731 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 72    iteration 5700 - train loss 0.0341029058 ____ validation loss 0.824716123 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 73    iteration 5800 - train loss 0.0343602635 ____ validation loss 0.8367756353 --- alpha: 0.00014\n",
      "train_accuracy: 100.0 - epoch 74    iteration 5900 - train loss 0.0338764722 ____ validation loss 0.8273557048 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 75    iteration 6000 - train loss 0.0339679985 ____ validation loss 0.8395956696 --- alpha: 0.00013\n",
      "train_accuracy: 100.0 - epoch 77    iteration 6100 - train loss 0.033938404 ____ validation loss 0.8310448359 --- alpha: 0.00013\n",
      "train_accuracy: 99.97392 - epoch 78    iteration 6200 - train loss 0.0336892688 ____ validation loss 0.8241916575 --- alpha: 0.00013\n",
      "train_accuracy: 99.94784 - epoch 79    iteration 6300 - train loss 0.0342591338 ____ validation loss 0.8456645491 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 80    iteration 6400 - train loss 0.0347768198 ____ validation loss 0.837767916 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 82    iteration 6500 - train loss 0.0337280985 ____ validation loss 0.8159110797 --- alpha: 0.00012\n",
      "train_accuracy: 99.97392 - epoch 83    iteration 6600 - train loss 0.0332606723 ____ validation loss 0.8244445399 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 84    iteration 6700 - train loss 0.0340028029 ____ validation loss 0.8166032842 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 85    iteration 6800 - train loss 0.034692364 ____ validation loss 0.8109525832 --- alpha: 0.00012\n",
      "train_accuracy: 100.0 - epoch 87    iteration 6900 - train loss 0.0342156728 ____ validation loss 0.8099427138 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 88    iteration 7000 - train loss 0.0342823957 ____ validation loss 0.8254973556 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 89    iteration 7100 - train loss 0.0339016258 ____ validation loss 0.8126481689 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 90    iteration 7200 - train loss 0.0344090424 ____ validation loss 0.8237835742 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 92    iteration 7300 - train loss 0.0343677056 ____ validation loss 0.8232025822 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 93    iteration 7400 - train loss 0.03347704 ____ validation loss 0.8261643082 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 94    iteration 7500 - train loss 0.0337308105 ____ validation loss 0.8295903854 --- alpha: 0.00011\n",
      "train_accuracy: 100.0 - epoch 95    iteration 7600 - train loss 0.0334242877 ____ validation loss 0.8253982701 --- alpha: 0.0001\n",
      "train_accuracy: 99.97392 - epoch 97    iteration 7700 - train loss 0.0329615636 ____ validation loss 0.818487846 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 98    iteration 7800 - train loss 0.0342931577 ____ validation loss 0.8223031101 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 99    iteration 7900 - train loss 0.0334228051 ____ validation loss 0.8257220131 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 100    iteration 8000 - train loss 0.0341507629 ____ validation loss 0.8277403726 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 102    iteration 8100 - train loss 0.0333745555 ____ validation loss 0.8116041602 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 103    iteration 8200 - train loss 0.0332570485 ____ validation loss 0.8296629767 --- alpha: 0.0001\n",
      "train_accuracy: 99.97392 - epoch 104    iteration 8300 - train loss 0.0326070248 ____ validation loss 0.834083242 --- alpha: 0.0001\n",
      "train_accuracy: 100.0 - epoch 105    iteration 8400 - train loss 0.0332513683 ____ validation loss 0.8274177043 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 107    iteration 8500 - train loss 0.0328785079 ____ validation loss 0.8395953581 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 108    iteration 8600 - train loss 0.0337167375 ____ validation loss 0.8275651555 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 109    iteration 8700 - train loss 0.0334591144 ____ validation loss 0.8199178571 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 110    iteration 8800 - train loss 0.0323846406 ____ validation loss 0.8310131633 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 112    iteration 8900 - train loss 0.0330275504 ____ validation loss 0.815476051 --- alpha: 9e-05\n",
      "train_accuracy: 99.97392 - epoch 113    iteration 9000 - train loss 0.033014641 ____ validation loss 0.8199394469 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 114    iteration 9100 - train loss 0.0331265781 ____ validation loss 0.8239646998 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 115    iteration 9200 - train loss 0.0333281483 ____ validation loss 0.8294321413 --- alpha: 9e-05\n",
      "train_accuracy: 100.0 - epoch 117    iteration 9300 - train loss 0.0339491006 ____ validation loss 0.8337286335 --- alpha: 8e-05\n",
      "train_accuracy: 99.97392 - epoch 118    iteration 9400 - train loss 0.0331026562 ____ validation loss 0.8256209377 --- alpha: 8e-05\n",
      "train_accuracy: 99.97392 - epoch 119    iteration 9500 - train loss 0.0336967265 ____ validation loss 0.8156249996 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 120    iteration 9600 - train loss 0.0333042595 ____ validation loss 0.8223819204 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 122    iteration 9700 - train loss 0.0329588036 ____ validation loss 0.8102400025 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 123    iteration 9800 - train loss 0.0334739326 ____ validation loss 0.8305761859 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 124    iteration 9900 - train loss 0.032961704 ____ validation loss 0.8110030532 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 125    iteration 10000 - train loss 0.0329621317 ____ validation loss 0.8156918302 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 127    iteration 10100 - train loss 0.033563972 ____ validation loss 0.8055885375 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 128    iteration 10200 - train loss 0.033314845 ____ validation loss 0.826824201 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 129    iteration 10300 - train loss 0.0331933971 ____ validation loss 0.8154614603 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 130    iteration 10400 - train loss 0.0330904684 ____ validation loss 0.8394027806 --- alpha: 8e-05\n",
      "train_accuracy: 100.0 - epoch 132    iteration 10500 - train loss 0.0329727867 ____ validation loss 0.828203823 --- alpha: 8e-05\n",
      "train_accuracy: 99.97392 - epoch 133    iteration 10600 - train loss 0.0326929486 ____ validation loss 0.8260316088 --- alpha: 7e-05\n",
      "train_accuracy: 99.97392 - epoch 134    iteration 10700 - train loss 0.0325077211 ____ validation loss 0.8226124931 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 135    iteration 10800 - train loss 0.0327834301 ____ validation loss 0.8283153299 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 137    iteration 10900 - train loss 0.0325535253 ____ validation loss 0.8281826788 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 138    iteration 11000 - train loss 0.0324529656 ____ validation loss 0.8328552131 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 139    iteration 11100 - train loss 0.0331395228 ____ validation loss 0.8174297075 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 140    iteration 11200 - train loss 0.0324864379 ____ validation loss 0.830092537 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 142    iteration 11300 - train loss 0.0325633669 ____ validation loss 0.8440860539 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 143    iteration 11400 - train loss 0.0333736747 ____ validation loss 0.8268575256 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 144    iteration 11500 - train loss 0.0326259455 ____ validation loss 0.8243691537 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 145    iteration 11600 - train loss 0.0330096923 ____ validation loss 0.8231126088 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 147    iteration 11700 - train loss 0.0326592562 ____ validation loss 0.828465468 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 148    iteration 11800 - train loss 0.0326255822 ____ validation loss 0.825922989 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 149    iteration 11900 - train loss 0.0326440116 ____ validation loss 0.8265012224 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 150    iteration 12000 - train loss 0.0325830919 ____ validation loss 0.816801067 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 152    iteration 12100 - train loss 0.0325890732 ____ validation loss 0.8265716812 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 153    iteration 12200 - train loss 0.0323402519 ____ validation loss 0.8225655484 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 154    iteration 12300 - train loss 0.0325469213 ____ validation loss 0.8232142031 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 155    iteration 12400 - train loss 0.032136051 ____ validation loss 0.8262846729 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 157    iteration 12500 - train loss 0.0320170334 ____ validation loss 0.8199507101 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 158    iteration 12600 - train loss 0.032820382 ____ validation loss 0.8288460047 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 159    iteration 12700 - train loss 0.0326447512 ____ validation loss 0.825724295 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 160    iteration 12800 - train loss 0.0326280827 ____ validation loss 0.8343575166 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 162    iteration 12900 - train loss 0.03255924 ____ validation loss 0.8233265808 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 163    iteration 13000 - train loss 0.032870537 ____ validation loss 0.83354072 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 164    iteration 13100 - train loss 0.0327289155 ____ validation loss 0.8256617051 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 165    iteration 13200 - train loss 0.0324509855 ____ validation loss 0.8223648991 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 167    iteration 13300 - train loss 0.0323679278 ____ validation loss 0.8176927388 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 168    iteration 13400 - train loss 0.0321873642 ____ validation loss 0.8266233259 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 169    iteration 13500 - train loss 0.0322947606 ____ validation loss 0.8311331542 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 170    iteration 13600 - train loss 0.0322452578 ____ validation loss 0.8238671425 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 172    iteration 13700 - train loss 0.0315739695 ____ validation loss 0.8303263862 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 173    iteration 13800 - train loss 0.0319968735 ____ validation loss 0.8215385918 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 174    iteration 13900 - train loss 0.0318146971 ____ validation loss 0.8320398402 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 175    iteration 14000 - train loss 0.0330527693 ____ validation loss 0.8197232568 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 177    iteration 14100 - train loss 0.0324028853 ____ validation loss 0.8293893763 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 178    iteration 14200 - train loss 0.0320143411 ____ validation loss 0.8291610531 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 179    iteration 14300 - train loss 0.0318165612 ____ validation loss 0.8292813974 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 180    iteration 14400 - train loss 0.0320217868 ____ validation loss 0.8312074037 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 182    iteration 14500 - train loss 0.032510443 ____ validation loss 0.833773844 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 183    iteration 14600 - train loss 0.0322540029 ____ validation loss 0.8336107237 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 184    iteration 14700 - train loss 0.0317372165 ____ validation loss 0.8293555872 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 185    iteration 14800 - train loss 0.0320101112 ____ validation loss 0.8413301003 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 187    iteration 14900 - train loss 0.0320649974 ____ validation loss 0.8352670216 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 188    iteration 15000 - train loss 0.0323635331 ____ validation loss 0.8317149223 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 189    iteration 15100 - train loss 0.0320624932 ____ validation loss 0.8272994928 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 190    iteration 15200 - train loss 0.0317893257 ____ validation loss 0.8335536682 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 192    iteration 15300 - train loss 0.0320192045 ____ validation loss 0.8299622328 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 193    iteration 15400 - train loss 0.0322410087 ____ validation loss 0.8366955618 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 194    iteration 15500 - train loss 0.0323634501 ____ validation loss 0.8320697148 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 195    iteration 15600 - train loss 0.0322945596 ____ validation loss 0.8275243709 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 197    iteration 15700 - train loss 0.032053972 ____ validation loss 0.8244491717 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 198    iteration 15800 - train loss 0.031878914 ____ validation loss 0.8315317852 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 199    iteration 15900 - train loss 0.0321273037 ____ validation loss 0.8282993319 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 200    iteration 16000 - train loss 0.0320662384 ____ validation loss 0.83727923 --- alpha: 5e-05\n",
      "func:'fit' -- took: 182.6362 sec\n",
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.3625120688\n",
      "\n",
      "train_accuracy: 93.33333 - epoch 2    iteration 100 - train loss 0.3030279409 ____ validation loss 0.3918143511 --- alpha: 0.00333\n",
      "train_accuracy: 94.20188 - epoch 3    iteration 200 - train loss 0.2320526005 ____ validation loss 0.319410097 --- alpha: 0.0025\n",
      "train_accuracy: 97.20657 - epoch 4    iteration 300 - train loss 0.1582394084 ____ validation loss 0.2811867658 --- alpha: 0.002\n",
      "train_accuracy: 97.23005 - epoch 5    iteration 400 - train loss 0.137559204 ____ validation loss 0.2677620139 --- alpha: 0.00167\n",
      "train_accuracy: 98.12207 - epoch 6    iteration 500 - train loss 0.1121477972 ____ validation loss 0.2414673518 --- alpha: 0.00143\n",
      "train_accuracy: 98.21596 - epoch 7    iteration 600 - train loss 0.1149657587 ____ validation loss 0.2353663319 --- alpha: 0.00125\n",
      "train_accuracy: 97.9108 - epoch 8    iteration 700 - train loss 0.1098045233 ____ validation loss 0.2217247619 --- alpha: 0.00111\n",
      "train_accuracy: 99.13146 - epoch 9    iteration 800 - train loss 0.0875550253 ____ validation loss 0.2091551117 --- alpha: 0.001\n",
      "train_accuracy: 98.56808 - epoch 11    iteration 900 - train loss 0.0934061432 ____ validation loss 0.2200109699 --- alpha: 0.00083\n",
      "train_accuracy: 99.10798 - epoch 12    iteration 1000 - train loss 0.0803215794 ____ validation loss 0.2134607281 --- alpha: 0.00077\n",
      "train_accuracy: 99.08451 - epoch 13    iteration 1100 - train loss 0.0769508601 ____ validation loss 0.2154476123 --- alpha: 0.00071\n",
      "train_accuracy: 99.38967 - epoch 14    iteration 1200 - train loss 0.0729887654 ____ validation loss 0.2019731285 --- alpha: 0.00067\n",
      "train_accuracy: 99.43662 - epoch 15    iteration 1300 - train loss 0.0757267766 ____ validation loss 0.211991258 --- alpha: 0.00062\n",
      "train_accuracy: 99.50704 - epoch 16    iteration 1400 - train loss 0.0652063897 ____ validation loss 0.1913528928 --- alpha: 0.00059\n",
      "train_accuracy: 99.48357 - epoch 17    iteration 1500 - train loss 0.0692029403 ____ validation loss 0.210529669 --- alpha: 0.00056\n",
      "train_accuracy: 99.57746 - epoch 18    iteration 1600 - train loss 0.0598471073 ____ validation loss 0.1993889607 --- alpha: 0.00053\n",
      "train_accuracy: 99.50704 - epoch 20    iteration 1700 - train loss 0.063689289 ____ validation loss 0.2022996957 --- alpha: 0.00048\n",
      "train_accuracy: 99.57746 - epoch 21    iteration 1800 - train loss 0.0619439604 ____ validation loss 0.1960837277 --- alpha: 0.00045\n",
      "train_accuracy: 99.67136 - epoch 22    iteration 1900 - train loss 0.0573119305 ____ validation loss 0.1971280207 --- alpha: 0.00043\n",
      "train_accuracy: 99.62441 - epoch 23    iteration 2000 - train loss 0.0567303504 ____ validation loss 0.1984410775 --- alpha: 0.00042\n",
      "train_accuracy: 99.67136 - epoch 24    iteration 2100 - train loss 0.0568713434 ____ validation loss 0.1956394129 --- alpha: 0.0004\n",
      "train_accuracy: 99.71831 - epoch 25    iteration 2200 - train loss 0.0555808634 ____ validation loss 0.1962031799 --- alpha: 0.00038\n",
      "train_accuracy: 99.64789 - epoch 26    iteration 2300 - train loss 0.0565914023 ____ validation loss 0.20149145 --- alpha: 0.00037\n",
      "train_accuracy: 99.78873 - epoch 27    iteration 2400 - train loss 0.0530373381 ____ validation loss 0.1938636532 --- alpha: 0.00036\n",
      "train_accuracy: 99.81221 - epoch 29    iteration 2500 - train loss 0.0527302944 ____ validation loss 0.1903963469 --- alpha: 0.00033\n",
      "train_accuracy: 99.78873 - epoch 30    iteration 2600 - train loss 0.0538498637 ____ validation loss 0.1971742601 --- alpha: 0.00032\n",
      "train_accuracy: 99.81221 - epoch 31    iteration 2700 - train loss 0.0499628183 ____ validation loss 0.1936162069 --- alpha: 0.00031\n",
      "train_accuracy: 99.85915 - epoch 32    iteration 2800 - train loss 0.0514155985 ____ validation loss 0.1921475362 --- alpha: 0.0003\n",
      "train_accuracy: 99.76526 - epoch 33    iteration 2900 - train loss 0.0528007781 ____ validation loss 0.1929344841 --- alpha: 0.00029\n",
      "train_accuracy: 99.81221 - epoch 34    iteration 3000 - train loss 0.0494624394 ____ validation loss 0.1881131899 --- alpha: 0.00029\n",
      "train_accuracy: 99.83568 - epoch 35    iteration 3100 - train loss 0.0479678437 ____ validation loss 0.1873945352 --- alpha: 0.00028\n",
      "train_accuracy: 99.85915 - epoch 36    iteration 3200 - train loss 0.0502024568 ____ validation loss 0.1905173914 --- alpha: 0.00027\n",
      "train_accuracy: 99.78873 - epoch 38    iteration 3300 - train loss 0.0466392344 ____ validation loss 0.1899129056 --- alpha: 0.00026\n",
      "train_accuracy: 99.85915 - epoch 39    iteration 3400 - train loss 0.0488351641 ____ validation loss 0.1892443336 --- alpha: 0.00025\n",
      "train_accuracy: 99.78873 - epoch 40    iteration 3500 - train loss 0.0470030131 ____ validation loss 0.1924692518 --- alpha: 0.00024\n",
      "train_accuracy: 99.85915 - epoch 41    iteration 3600 - train loss 0.0481166295 ____ validation loss 0.1870562834 --- alpha: 0.00024\n",
      "train_accuracy: 99.88263 - epoch 42    iteration 3700 - train loss 0.0474039522 ____ validation loss 0.1904528485 --- alpha: 0.00023\n",
      "train_accuracy: 99.85915 - epoch 43    iteration 3800 - train loss 0.0449612559 ____ validation loss 0.1875518118 --- alpha: 0.00023\n",
      "train_accuracy: 99.88263 - epoch 44    iteration 3900 - train loss 0.0453217176 ____ validation loss 0.1874114592 --- alpha: 0.00022\n",
      "train_accuracy: 99.85915 - epoch 45    iteration 4000 - train loss 0.0459125915 ____ validation loss 0.1906962887 --- alpha: 0.00022\n",
      "train_accuracy: 99.92958 - epoch 47    iteration 4100 - train loss 0.0461732404 ____ validation loss 0.1891701305 --- alpha: 0.00021\n",
      "train_accuracy: 99.9061 - epoch 48    iteration 4200 - train loss 0.0459332519 ____ validation loss 0.1931612496 --- alpha: 0.0002\n",
      "train_accuracy: 99.88263 - epoch 49    iteration 4300 - train loss 0.0444239436 ____ validation loss 0.1835513211 --- alpha: 0.0002\n",
      "train_accuracy: 99.9061 - epoch 50    iteration 4400 - train loss 0.0465857795 ____ validation loss 0.1931048503 --- alpha: 0.0002\n",
      "train_accuracy: 99.9061 - epoch 51    iteration 4500 - train loss 0.0463318606 ____ validation loss 0.1883209492 --- alpha: 0.00019\n",
      "train_accuracy: 99.92958 - epoch 52    iteration 4600 - train loss 0.044569422 ____ validation loss 0.1894406296 --- alpha: 0.00019\n",
      "train_accuracy: 99.88263 - epoch 53    iteration 4700 - train loss 0.0442928227 ____ validation loss 0.1916940777 --- alpha: 0.00019\n",
      "train_accuracy: 99.9061 - epoch 54    iteration 4800 - train loss 0.0456254053 ____ validation loss 0.1914350494 --- alpha: 0.00018\n",
      "train_accuracy: 99.92958 - epoch 56    iteration 4900 - train loss 0.043472769 ____ validation loss 0.1929698199 --- alpha: 0.00018\n",
      "train_accuracy: 99.92958 - epoch 57    iteration 5000 - train loss 0.0443251014 ____ validation loss 0.1912493853 --- alpha: 0.00017\n",
      "train_accuracy: 99.92958 - epoch 58    iteration 5100 - train loss 0.0431094819 ____ validation loss 0.1891281692 --- alpha: 0.00017\n",
      "train_accuracy: 99.92958 - epoch 59    iteration 5200 - train loss 0.0438951221 ____ validation loss 0.1858439588 --- alpha: 0.00017\n",
      "train_accuracy: 99.92958 - epoch 60    iteration 5300 - train loss 0.0431929767 ____ validation loss 0.1898068859 --- alpha: 0.00016\n",
      "train_accuracy: 99.9061 - epoch 61    iteration 5400 - train loss 0.0425792381 ____ validation loss 0.1889433346 --- alpha: 0.00016\n",
      "train_accuracy: 99.92958 - epoch 62    iteration 5500 - train loss 0.0430114984 ____ validation loss 0.1878227454 --- alpha: 0.00016\n",
      "train_accuracy: 99.92958 - epoch 63    iteration 5600 - train loss 0.0426765877 ____ validation loss 0.1825924402 --- alpha: 0.00016\n",
      "train_accuracy: 99.92958 - epoch 65    iteration 5700 - train loss 0.043335485 ____ validation loss 0.1898741524 --- alpha: 0.00015\n",
      "train_accuracy: 99.9061 - epoch 66    iteration 5800 - train loss 0.0433537775 ____ validation loss 0.191407476 --- alpha: 0.00015\n",
      "train_accuracy: 99.92958 - epoch 67    iteration 5900 - train loss 0.0439326203 ____ validation loss 0.1869713672 --- alpha: 0.00015\n",
      "train_accuracy: 99.92958 - epoch 68    iteration 6000 - train loss 0.0422131387 ____ validation loss 0.1873379864 --- alpha: 0.00014\n",
      "train_accuracy: 99.9061 - epoch 69    iteration 6100 - train loss 0.0416011032 ____ validation loss 0.1887211444 --- alpha: 0.00014\n",
      "train_accuracy: 99.92958 - epoch 70    iteration 6200 - train loss 0.042185375 ____ validation loss 0.1838014156 --- alpha: 0.00014\n",
      "train_accuracy: 99.88263 - epoch 71    iteration 6300 - train loss 0.0431437999 ____ validation loss 0.1890677721 --- alpha: 0.00014\n",
      "train_accuracy: 99.92958 - epoch 72    iteration 6400 - train loss 0.0429139443 ____ validation loss 0.1871960386 --- alpha: 0.00014\n",
      "train_accuracy: 99.92958 - epoch 74    iteration 6500 - train loss 0.0415130253 ____ validation loss 0.1859312953 --- alpha: 0.00013\n",
      "train_accuracy: 99.92958 - epoch 75    iteration 6600 - train loss 0.0416193271 ____ validation loss 0.1825849669 --- alpha: 0.00013\n",
      "train_accuracy: 99.92958 - epoch 76    iteration 6700 - train loss 0.0417479158 ____ validation loss 0.1850030172 --- alpha: 0.00013\n",
      "train_accuracy: 99.92958 - epoch 77    iteration 6800 - train loss 0.0414833633 ____ validation loss 0.1819499262 --- alpha: 0.00013\n",
      "train_accuracy: 99.9061 - epoch 78    iteration 6900 - train loss 0.0420726836 ____ validation loss 0.1866035253 --- alpha: 0.00013\n",
      "train_accuracy: 99.95305 - epoch 79    iteration 7000 - train loss 0.0411395476 ____ validation loss 0.18522587 --- alpha: 0.00012\n",
      "train_accuracy: 99.95305 - epoch 80    iteration 7100 - train loss 0.0410121922 ____ validation loss 0.1872233183 --- alpha: 0.00012\n",
      "train_accuracy: 99.92958 - epoch 81    iteration 7200 - train loss 0.041268943 ____ validation loss 0.1923297792 --- alpha: 0.00012\n",
      "train_accuracy: 99.92958 - epoch 83    iteration 7300 - train loss 0.0403622067 ____ validation loss 0.1911800846 --- alpha: 0.00012\n",
      "train_accuracy: 99.9061 - epoch 84    iteration 7400 - train loss 0.0431861466 ____ validation loss 0.1945914801 --- alpha: 0.00012\n",
      "train_accuracy: 99.9061 - epoch 85    iteration 7500 - train loss 0.0425359762 ____ validation loss 0.1891457054 --- alpha: 0.00012\n",
      "train_accuracy: 99.95305 - epoch 86    iteration 7600 - train loss 0.0402375135 ____ validation loss 0.1903362505 --- alpha: 0.00011\n",
      "train_accuracy: 99.97653 - epoch 87    iteration 7700 - train loss 0.0409559951 ____ validation loss 0.1900924753 --- alpha: 0.00011\n",
      "train_accuracy: 99.95305 - epoch 88    iteration 7800 - train loss 0.0406903703 ____ validation loss 0.1908171486 --- alpha: 0.00011\n",
      "train_accuracy: 99.92958 - epoch 89    iteration 7900 - train loss 0.0404996683 ____ validation loss 0.1895769215 --- alpha: 0.00011\n",
      "train_accuracy: 99.92958 - epoch 90    iteration 8000 - train loss 0.0407925518 ____ validation loss 0.1896199671 --- alpha: 0.00011\n",
      "train_accuracy: 99.92958 - epoch 92    iteration 8100 - train loss 0.0414248887 ____ validation loss 0.1903440424 --- alpha: 0.00011\n",
      "train_accuracy: 99.95305 - epoch 93    iteration 8200 - train loss 0.0408632618 ____ validation loss 0.1901212101 --- alpha: 0.00011\n",
      "train_accuracy: 99.97653 - epoch 94    iteration 8300 - train loss 0.041138819 ____ validation loss 0.1891014369 --- alpha: 0.00011\n",
      "train_accuracy: 99.95305 - epoch 95    iteration 8400 - train loss 0.0407943397 ____ validation loss 0.1902207735 --- alpha: 0.0001\n",
      "train_accuracy: 99.95305 - epoch 96    iteration 8500 - train loss 0.0399906848 ____ validation loss 0.191835742 --- alpha: 0.0001\n",
      "train_accuracy: 99.92958 - epoch 97    iteration 8600 - train loss 0.0403001993 ____ validation loss 0.1894666787 --- alpha: 0.0001\n",
      "train_accuracy: 99.95305 - epoch 98    iteration 8700 - train loss 0.0407053841 ____ validation loss 0.1918428075 --- alpha: 0.0001\n",
      "train_accuracy: 99.92958 - epoch 99    iteration 8800 - train loss 0.040113548 ____ validation loss 0.1931583328 --- alpha: 0.0001\n",
      "train_accuracy: 99.92958 - epoch 100    iteration 8900 - train loss 0.0418329817 ____ validation loss 0.1946507186 --- alpha: 0.0001\n",
      "train_accuracy: 99.92958 - epoch 102    iteration 9000 - train loss 0.0408618473 ____ validation loss 0.1916191326 --- alpha: 0.0001\n",
      "train_accuracy: 99.92958 - epoch 103    iteration 9100 - train loss 0.0405737244 ____ validation loss 0.189450856 --- alpha: 0.0001\n",
      "train_accuracy: 99.95305 - epoch 104    iteration 9200 - train loss 0.0402244516 ____ validation loss 0.1945241922 --- alpha: 0.0001\n",
      "train_accuracy: 99.92958 - epoch 105    iteration 9300 - train loss 0.0399953193 ____ validation loss 0.1896840111 --- alpha: 9e-05\n",
      "train_accuracy: 99.95305 - epoch 106    iteration 9400 - train loss 0.0398526847 ____ validation loss 0.190835998 --- alpha: 9e-05\n",
      "train_accuracy: 99.92958 - epoch 107    iteration 9500 - train loss 0.0402232421 ____ validation loss 0.1913403688 --- alpha: 9e-05\n",
      "train_accuracy: 99.92958 - epoch 108    iteration 9600 - train loss 0.0400294475 ____ validation loss 0.1911180659 --- alpha: 9e-05\n",
      "train_accuracy: 99.95305 - epoch 109    iteration 9700 - train loss 0.0395513334 ____ validation loss 0.1893458919 --- alpha: 9e-05\n",
      "train_accuracy: 99.97653 - epoch 111    iteration 9800 - train loss 0.0401113022 ____ validation loss 0.1909081033 --- alpha: 9e-05\n",
      "train_accuracy: 99.92958 - epoch 112    iteration 9900 - train loss 0.0399165574 ____ validation loss 0.1882812064 --- alpha: 9e-05\n",
      "train_accuracy: 99.95305 - epoch 113    iteration 10000 - train loss 0.0398667921 ____ validation loss 0.1899835863 --- alpha: 9e-05\n",
      "train_accuracy: 99.92958 - epoch 114    iteration 10100 - train loss 0.0405542199 ____ validation loss 0.1898070527 --- alpha: 9e-05\n",
      "train_accuracy: 99.95305 - epoch 115    iteration 10200 - train loss 0.0410103533 ____ validation loss 0.1931998666 --- alpha: 9e-05\n",
      "train_accuracy: 99.92958 - epoch 116    iteration 10300 - train loss 0.0398271903 ____ validation loss 0.1875973648 --- alpha: 9e-05\n",
      "train_accuracy: 99.95305 - epoch 117    iteration 10400 - train loss 0.0404606394 ____ validation loss 0.1935391708 --- alpha: 8e-05\n",
      "train_accuracy: 99.92958 - epoch 118    iteration 10500 - train loss 0.0401235964 ____ validation loss 0.1890754576 --- alpha: 8e-05\n",
      "train_accuracy: 99.92958 - epoch 120    iteration 10600 - train loss 0.0391050319 ____ validation loss 0.1857537343 --- alpha: 8e-05\n",
      "train_accuracy: 99.95305 - epoch 121    iteration 10700 - train loss 0.0390189088 ____ validation loss 0.1862643652 --- alpha: 8e-05\n",
      "train_accuracy: 99.95305 - epoch 122    iteration 10800 - train loss 0.0394111496 ____ validation loss 0.1868198878 --- alpha: 8e-05\n",
      "train_accuracy: 99.92958 - epoch 123    iteration 10900 - train loss 0.0396203853 ____ validation loss 0.1835372652 --- alpha: 8e-05\n",
      "train_accuracy: 99.92958 - epoch 124    iteration 11000 - train loss 0.0395738793 ____ validation loss 0.1906024812 --- alpha: 8e-05\n",
      "train_accuracy: 99.95305 - epoch 125    iteration 11100 - train loss 0.0398387087 ____ validation loss 0.1944032736 --- alpha: 8e-05\n",
      "train_accuracy: 99.95305 - epoch 126    iteration 11200 - train loss 0.0393364681 ____ validation loss 0.1917275755 --- alpha: 8e-05\n",
      "train_accuracy: 99.92958 - epoch 127    iteration 11300 - train loss 0.0388005369 ____ validation loss 0.1871921514 --- alpha: 8e-05\n",
      "train_accuracy: 99.95305 - epoch 129    iteration 11400 - train loss 0.0399058088 ____ validation loss 0.1906608975 --- alpha: 8e-05\n",
      "train_accuracy: 99.97653 - epoch 130    iteration 11500 - train loss 0.0391350548 ____ validation loss 0.1894709349 --- alpha: 8e-05\n",
      "train_accuracy: 99.95305 - epoch 131    iteration 11600 - train loss 0.0392522704 ____ validation loss 0.1895702305 --- alpha: 8e-05\n",
      "train_accuracy: 99.95305 - epoch 132    iteration 11700 - train loss 0.0404957861 ____ validation loss 0.1914852124 --- alpha: 8e-05\n",
      "train_accuracy: 99.92958 - epoch 133    iteration 11800 - train loss 0.0387137221 ____ validation loss 0.1913319938 --- alpha: 7e-05\n",
      "train_accuracy: 99.92958 - epoch 134    iteration 11900 - train loss 0.0391849997 ____ validation loss 0.1925284862 --- alpha: 7e-05\n",
      "train_accuracy: 99.92958 - epoch 135    iteration 12000 - train loss 0.0399595566 ____ validation loss 0.1915369256 --- alpha: 7e-05\n",
      "train_accuracy: 99.95305 - epoch 136    iteration 12100 - train loss 0.038992026 ____ validation loss 0.1884831337 --- alpha: 7e-05\n",
      "train_accuracy: 99.95305 - epoch 138    iteration 12200 - train loss 0.0390546604 ____ validation loss 0.191418676 --- alpha: 7e-05\n",
      "train_accuracy: 99.9061 - epoch 139    iteration 12300 - train loss 0.038772311 ____ validation loss 0.1901019573 --- alpha: 7e-05\n",
      "train_accuracy: 99.92958 - epoch 140    iteration 12400 - train loss 0.0390897027 ____ validation loss 0.1931600098 --- alpha: 7e-05\n",
      "train_accuracy: 99.92958 - epoch 141    iteration 12500 - train loss 0.0402444569 ____ validation loss 0.1950476885 --- alpha: 7e-05\n",
      "train_accuracy: 99.95305 - epoch 142    iteration 12600 - train loss 0.0389123697 ____ validation loss 0.1894980119 --- alpha: 7e-05\n",
      "train_accuracy: 99.92958 - epoch 143    iteration 12700 - train loss 0.0389519962 ____ validation loss 0.1924608961 --- alpha: 7e-05\n",
      "train_accuracy: 99.92958 - epoch 144    iteration 12800 - train loss 0.0386672724 ____ validation loss 0.1900818451 --- alpha: 7e-05\n",
      "train_accuracy: 99.92958 - epoch 145    iteration 12900 - train loss 0.0389430632 ____ validation loss 0.1896361056 --- alpha: 7e-05\n",
      "train_accuracy: 99.92958 - epoch 147    iteration 13000 - train loss 0.0386417412 ____ validation loss 0.1890752273 --- alpha: 7e-05\n",
      "train_accuracy: 99.97653 - epoch 148    iteration 13100 - train loss 0.0387987803 ____ validation loss 0.187906772 --- alpha: 7e-05\n",
      "train_accuracy: 100.0 - epoch 149    iteration 13200 - train loss 0.0388689124 ____ validation loss 0.1890694079 --- alpha: 7e-05\n",
      "train_accuracy: 99.95305 - epoch 150    iteration 13300 - train loss 0.0397175183 ____ validation loss 0.1889282651 --- alpha: 7e-05\n",
      "train_accuracy: 99.95305 - epoch 151    iteration 13400 - train loss 0.0388591167 ____ validation loss 0.1907235885 --- alpha: 7e-05\n",
      "train_accuracy: 99.95305 - epoch 152    iteration 13500 - train loss 0.0397504893 ____ validation loss 0.1893629993 --- alpha: 7e-05\n",
      "train_accuracy: 99.97653 - epoch 153    iteration 13600 - train loss 0.0386560787 ____ validation loss 0.1884187443 --- alpha: 6e-05\n",
      "train_accuracy: 99.95305 - epoch 154    iteration 13700 - train loss 0.0385673194 ____ validation loss 0.1883345794 --- alpha: 6e-05\n",
      "train_accuracy: 99.92958 - epoch 156    iteration 13800 - train loss 0.0398755406 ____ validation loss 0.190143995 --- alpha: 6e-05\n",
      "train_accuracy: 99.95305 - epoch 157    iteration 13900 - train loss 0.03878078 ____ validation loss 0.1878242699 --- alpha: 6e-05\n",
      "train_accuracy: 99.95305 - epoch 158    iteration 14000 - train loss 0.0384897885 ____ validation loss 0.1919440486 --- alpha: 6e-05\n",
      "train_accuracy: 99.97653 - epoch 159    iteration 14100 - train loss 0.0390097921 ____ validation loss 0.1914685788 --- alpha: 6e-05\n",
      "train_accuracy: 99.92958 - epoch 160    iteration 14200 - train loss 0.0396017421 ____ validation loss 0.1881805352 --- alpha: 6e-05\n",
      "train_accuracy: 99.95305 - epoch 161    iteration 14300 - train loss 0.0387789934 ____ validation loss 0.1894301246 --- alpha: 6e-05\n",
      "train_accuracy: 99.95305 - epoch 162    iteration 14400 - train loss 0.0387615514 ____ validation loss 0.1902664677 --- alpha: 6e-05\n",
      "train_accuracy: 99.95305 - epoch 163    iteration 14500 - train loss 0.0400640114 ____ validation loss 0.1925014448 --- alpha: 6e-05\n",
      "train_accuracy: 99.95305 - epoch 165    iteration 14600 - train loss 0.0395171269 ____ validation loss 0.1932759456 --- alpha: 6e-05\n",
      "train_accuracy: 99.97653 - epoch 166    iteration 14700 - train loss 0.0389702604 ____ validation loss 0.1898333749 --- alpha: 6e-05\n",
      "train_accuracy: 99.95305 - epoch 167    iteration 14800 - train loss 0.0386288504 ____ validation loss 0.1930311123 --- alpha: 6e-05\n",
      "train_accuracy: 99.95305 - epoch 168    iteration 14900 - train loss 0.0382085841 ____ validation loss 0.1906547268 --- alpha: 6e-05\n",
      "train_accuracy: 99.95305 - epoch 169    iteration 15000 - train loss 0.0387195126 ____ validation loss 0.1892553788 --- alpha: 6e-05\n",
      "train_accuracy: 99.97653 - epoch 170    iteration 15100 - train loss 0.0389225592 ____ validation loss 0.1927131972 --- alpha: 6e-05\n",
      "train_accuracy: 99.97653 - epoch 171    iteration 15200 - train loss 0.0381289878 ____ validation loss 0.1907750991 --- alpha: 6e-05\n",
      "train_accuracy: 99.97653 - epoch 172    iteration 15300 - train loss 0.0391024982 ____ validation loss 0.1907093839 --- alpha: 6e-05\n",
      "train_accuracy: 99.97653 - epoch 174    iteration 15400 - train loss 0.0380041299 ____ validation loss 0.189152162 --- alpha: 6e-05\n",
      "train_accuracy: 99.97653 - epoch 175    iteration 15500 - train loss 0.0385859487 ____ validation loss 0.1886717751 --- alpha: 6e-05\n",
      "train_accuracy: 99.97653 - epoch 176    iteration 15600 - train loss 0.039197364 ____ validation loss 0.1908529739 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 177    iteration 15700 - train loss 0.0384224906 ____ validation loss 0.1872577917 --- alpha: 6e-05\n",
      "train_accuracy: 99.95305 - epoch 178    iteration 15800 - train loss 0.0383760268 ____ validation loss 0.1901998535 --- alpha: 6e-05\n",
      "train_accuracy: 99.95305 - epoch 179    iteration 15900 - train loss 0.0397015812 ____ validation loss 0.1922847843 --- alpha: 6e-05\n",
      "train_accuracy: 100.0 - epoch 180    iteration 16000 - train loss 0.0388397119 ____ validation loss 0.1908822039 --- alpha: 6e-05\n",
      "train_accuracy: 99.95305 - epoch 181    iteration 16100 - train loss 0.0385814728 ____ validation loss 0.1885037764 --- alpha: 5e-05\n",
      "train_accuracy: 99.95305 - epoch 183    iteration 16200 - train loss 0.0383574904 ____ validation loss 0.1894280913 --- alpha: 5e-05\n",
      "train_accuracy: 99.95305 - epoch 184    iteration 16300 - train loss 0.0383692325 ____ validation loss 0.1905828815 --- alpha: 5e-05\n",
      "train_accuracy: 99.95305 - epoch 185    iteration 16400 - train loss 0.0384671083 ____ validation loss 0.1911813416 --- alpha: 5e-05\n",
      "train_accuracy: 99.97653 - epoch 186    iteration 16500 - train loss 0.0381877699 ____ validation loss 0.1909423337 --- alpha: 5e-05\n",
      "train_accuracy: 99.97653 - epoch 187    iteration 16600 - train loss 0.0384786579 ____ validation loss 0.191117446 --- alpha: 5e-05\n",
      "train_accuracy: 99.97653 - epoch 188    iteration 16700 - train loss 0.0377330728 ____ validation loss 0.189223891 --- alpha: 5e-05\n",
      "train_accuracy: 99.97653 - epoch 189    iteration 16800 - train loss 0.0382229158 ____ validation loss 0.19110646 --- alpha: 5e-05\n",
      "train_accuracy: 100.0 - epoch 190    iteration 16900 - train loss 0.0380215237 ____ validation loss 0.1887952598 --- alpha: 5e-05\n",
      "train_accuracy: 99.97653 - epoch 192    iteration 17000 - train loss 0.0380069955 ____ validation loss 0.1915855062 --- alpha: 5e-05\n",
      "train_accuracy: 99.97653 - epoch 193    iteration 17100 - train loss 0.0380610686 ____ validation loss 0.1912131318 --- alpha: 5e-05\n",
      "train_accuracy: 99.97653 - epoch 194    iteration 17200 - train loss 0.0379298139 ____ validation loss 0.1926757898 --- alpha: 5e-05\n",
      "train_accuracy: 99.95305 - epoch 195    iteration 17300 - train loss 0.0377650442 ____ validation loss 0.1915398178 --- alpha: 5e-05\n",
      "train_accuracy: 99.97653 - epoch 196    iteration 17400 - train loss 0.0383605979 ____ validation loss 0.1915759462 --- alpha: 5e-05\n",
      "train_accuracy: 99.97653 - epoch 197    iteration 17500 - train loss 0.038310912 ____ validation loss 0.1898776591 --- alpha: 5e-05\n",
      "train_accuracy: 99.95305 - epoch 198    iteration 17600 - train loss 0.038095297 ____ validation loss 0.194332854 --- alpha: 5e-05\n",
      "train_accuracy: 99.97653 - epoch 199    iteration 17700 - train loss 0.0383325921 ____ validation loss 0.1938199611 --- alpha: 5e-05\n",
      "train_accuracy: 99.95305 - epoch 200    iteration 17800 - train loss 0.0382588967 ____ validation loss 0.1927077452 --- alpha: 5e-05\n",
      "func:'fit' -- took: 204.4525 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{426: (0.0051322897, 6.0926720081),\n",
       " 852: (0.0062956364, 5.236061371),\n",
       " 1278: (0.0108553809, 4.2448200202),\n",
       " 1704: (0.0143426681, 3.7189932625),\n",
       " 2130: (0.0175049291, 3.2064696262),\n",
       " 2556: (0.0217072576, 2.5710097784),\n",
       " 2982: (0.0238457601, 1.99933871),\n",
       " 3408: (0.0284362831, 1.4334449673),\n",
       " 3834: (0.0320662384, 0.83727923),\n",
       " 4260: (0.0382588967, 0.1927077452)}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_curve(model, 10, X_train, Y_train, X_validation, Y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_error_values = {426: (0.0051322897, 6.0926720081),\n",
    " 852: (0.0062956364, 5.236061371),\n",
    " 1278: (0.0108553809, 4.2448200202),\n",
    " 1704: (0.0143426681, 3.7189932625),\n",
    " 2130: (0.0175049291, 3.2064696262),\n",
    " 2556: (0.0217072576, 2.5710097784),\n",
    " 2982: (0.0238457601, 1.99933871),\n",
    " 3408: (0.0284362831, 1.4334449673),\n",
    " 3834: (0.0320662384, 0.83727923),\n",
    " 4260: (0.0382588967, 0.1927077452)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = []\n",
    "train_error = []\n",
    "val_error = []\n",
    "\n",
    "for training_size, (train_error_, val_error_) in dict_error_values.items():\n",
    "    train_error.append(train_error_)\n",
    "    val_error.append(val_error_)\n",
    "    train_size.append(training_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3408, 2130, 852, 2982, 1704, 4260, 426, 2556, 3834, 1278]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0284362831,\n",
       " 0.0175049291,\n",
       " 0.0062956364,\n",
       " 0.0238457601,\n",
       " 0.0143426681,\n",
       " 0.0382588967,\n",
       " 0.0051322897,\n",
       " 0.0217072576,\n",
       " 0.0320662384,\n",
       " 0.0108553809]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4334449673,\n",
       " 3.2064696262,\n",
       " 5.236061371,\n",
       " 1.99933871,\n",
       " 3.7189932625,\n",
       " 0.1927077452,\n",
       " 6.0926720081,\n",
       " 2.5710097784,\n",
       " 0.83727923,\n",
       " 4.2448200202]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(train_size, train_error, c=\"blue\")\n",
    "plt.plot(train_size, val_error, c=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0284362831,\n",
       " 0.0175049291,\n",
       " 0.0062956364,\n",
       " 0.0238457601,\n",
       " 0.0143426681,\n",
       " 0.0382588967,\n",
       " 0.0051322897,\n",
       " 0.0217072576,\n",
       " 0.0320662384,\n",
       " 0.0108553809]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.3503411557\n",
      "\n",
      "train_accuracy: 92.88732 - epoch 2    iteration 100 - train loss 0.2776993776 ____ validation loss 0.359564137 --- alpha: 0.00333\n",
      "train_accuracy: 95.70423 - epoch 3    iteration 200 - train loss 0.1828935328 ____ validation loss 0.2787426507 --- alpha: 0.0025\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-141-487f85982d96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\mardic\\Desktop\\HackerRank\\Andrew-NG-Coursera-master\\utility_functions.py\u001b[0m in \u001b[0;36mwrap\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mte\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'func:%r -- took: %2.4f sec'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-113-57d60564bb62>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, X_val, Y_val)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m                 \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforwardpass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_mini_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m                 \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_mini_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_of_iterations\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-113-57d60564bb62>\u001b[0m in \u001b[0;36mbackpropagation\u001b[1;34m(self, Y, A, Z)\u001b[0m\n\u001b[0;32m    371\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_W\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlambd\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-113-57d60564bb62>\u001b[0m in \u001b[0;36mupdate_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    421\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms_db\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms_db\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_B\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m                 \u001b[0mv_dw_corrected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv_dw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_of_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m                 \u001b[0ms_dw_corrected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms_dw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_of_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354\n",
    "\n",
    "model = NeuralNetwork(\n",
    "    input_layer=(X_train.shape[0], 'relu'),\n",
    "    hidden_layer=[(200,'relu'),(100,'relu'),(50,'softmax')],\n",
    "    output_layer=Y_train.shape[0],\n",
    "    batch_size=48,\n",
    "    optimizer={\"method\": \"ADAM\", \"beta1\": 0.9, \"beta2\": 0.999},\n",
    "    weight_initialisation = \"xavier_uniform\",\n",
    "    metrics=\"accuracy\",\n",
    "    penalty=\"l2\",\n",
    "    lambd=0.1,\n",
    "    keep_prob=0.5,\n",
    "    epoch=200,\n",
    "    alpha={\"initial_lr\": 0.01, \"decay\": 1},\n",
    "    verbose=True,\n",
    "    random_state=np.random.randint(0, 100)\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train, X_validation, Y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F1': 90.90909090528662,\n",
       " 'accuracy': 98.07692307654587,\n",
       " 'false_positive_rate': 1.7094017093944043,\n",
       " 'precision': 86.20689654875149,\n",
       " 'prevalence': 9.999999999961538,\n",
       " 'sensitivity/recall': 96.15384615014793,\n",
       " 'specificity': 98.29059829017825}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_model_performance(np.argmax(Y_test, axis=0),\n",
    "                           model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F1': 79.33884297341656,\n",
       " 'accuracy': 94.79166666646918,\n",
       " 'false_positive_rate': 5.787037037023642,\n",
       " 'precision': 65.75342465663351,\n",
       " 'prevalence': 9.999999999979167,\n",
       " 'sensitivity/recall': 99.99999999791667,\n",
       " 'specificity': 94.21296296274488}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_model_performance(np.argmax(Y_validation, axis=0),\n",
    "                           model.predict(X_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_loss(\n",
    "    model.train_error,\n",
    "    model.validation_error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results_dict_all_models, results_average_dict, models = grid_search(\n",
    "#     x,\n",
    "#     y,\n",
    "#     clf=NeuralNetwork,\n",
    "#     lst_metrics=[\"F1\", \"accuracy\"],\n",
    "#     sort_by = \"F1\",\n",
    "#     n_folds=10,\n",
    "#     dict_param_grid={\n",
    "#         'batch_size': [64, 128, 256],\n",
    "#         'input_layer': [(x.shape[1], 'relu')],\n",
    "#         'hidden_layer': [\n",
    "#             [(50, 'relu'), (25, 'relu'), (10,'softmax')],\n",
    "#             [(200, 'relu'), (100, 'relu'), (50, 'relu'), (10,'softmax')]\n",
    "#         ],\n",
    "#         'optimizer':[\n",
    "#             {\n",
    "#                 \"method\": \"ADAM\",\n",
    "#                 \"beta1\": 0.9,\n",
    "#                 \"beta2\": 0.999\n",
    "#             }\n",
    "#         ],\n",
    "#         'output_layer': [10],\n",
    "#         'alpha': [0.0001, 0.001],\n",
    "#         'verbose': [False],\n",
    "#         'epoch': [250]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# results_average_dict\n",
    "# print(models[\"model_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_miss_clasifications(model, digits_to_display):\n",
    "    count = 0\n",
    "    for index, (act, predicted) in enumerate(zip(np.argmax(Y,axis=0), model.predict(X))):\n",
    "        if act != predicted:\n",
    "            fig, ax = plt.subplots(figsize = (2,2))\n",
    "            ax.set_title(\"%s: act %s --- predicted %s\" %(index, act, predicted))\n",
    "            ax.imshow(X[:, index].reshape(-1,20).T)\n",
    "            ax.axis('off');\n",
    "            count += 1\n",
    "        if count == digits_to_display:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_miss_clasifications(model, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
