{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utility_functions import (calculate_model_performance,\n",
    "                               plot_ROC,\n",
    "                               one_hot_encode,\n",
    "                               split_data_as,\n",
    "                               grid_search,\n",
    "                               shuffled,\n",
    "                               timeit)\n",
    "\n",
    "EPSILON = 10e-08\n",
    "\n",
    "\n",
    "def get_shapes(any_):\n",
    "    for array in any_:\n",
    "        try:\n",
    "            print(array.shape)\n",
    "        except:\n",
    "            print(\"NONE\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# ============= ACTIVATION FUNCTIONS ===============#\n",
    "\n",
    "def sigmoid(Z, prime=False):\n",
    "    # np.\n",
    "    if prime:\n",
    "        return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "\n",
    "def linear(Z, prime=False):\n",
    "    if prime:\n",
    "        return np.ones_like(Z)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def relu(Z, alpha=0.01, prime=False):\n",
    "    if prime:\n",
    "        Z_relu = np.ones_like(Z, dtype=np.float64)\n",
    "        Z_relu[Z < 0] = alpha\n",
    "        return Z_relu\n",
    "    return np.where(Z < 0, alpha * Z, Z)\n",
    "\n",
    "\n",
    "def tanh(Z, prime=False):\n",
    "    # np.tanh() could be used directly to speed this up\n",
    "    if prime:\n",
    "        return 1 - np.power(tanh(Z), 2)\n",
    "    return (2 / (1 + np.exp(-2 * Z))) - 1\n",
    "\n",
    "\n",
    "def elu(Z, prime=False):\n",
    "    # https://mlfromscratch.com/activation-functions-explained/#/\n",
    "    alpha = 0.2\n",
    "    if prime:\n",
    "        return np.where(Z < 0, alpha * (np.exp(Z)), 1)\n",
    "    return np.where(Z < 0, alpha * (np.exp(Z) - 1), Z)\n",
    "\n",
    "\n",
    "def softmax(Z, prime=False):\n",
    "    # https://deepnotes.io/softmax-crossentropy\n",
    "    # max(Z) term is added to stabilise the function.\n",
    "    exps = np.exp(Z - np.max(Z))\n",
    "    return exps / np.sum(exps, axis=0)\n",
    "\n",
    "\n",
    "# References\n",
    "# https://mc.ai/multilayered-neural-network-from-scratch-using-python/\n",
    "# https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "# https://www.coursera.org/learn/machine-learning/home/week/5\n",
    "# https://www.coursera.org/specializations/deep-learning\n",
    "# https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py\n",
    "# https://github.com/JWarmenhoven/Coursera-Machine-Learning\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_layer: tuple,\n",
    "            hidden_layer: list,  # list of tuples\n",
    "            output_layer: int,\n",
    "            batch_size=16,\n",
    "            alpha=1,\n",
    "            optimizer=\"SGD\",\n",
    "            penalty=\"l2\",\n",
    "            lambd=\"0.1\",\n",
    "            dropout = False,\n",
    "            keep_prob = 0.9,\n",
    "            epoch=500,\n",
    "            random_state=42,\n",
    "            verbose=True,\n",
    "            metrics=\"accuracy\"\n",
    "    ):\n",
    "        self.input_layer = input_layer\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.output_layer = output_layer\n",
    "        self.mini_batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.optimizer = optimizer\n",
    "        self.penalty = penalty\n",
    "        self.lambd = lambd\n",
    "        self.dropout = dropout,\n",
    "        self.keep_prob = keep_prob,\n",
    "        self.epoch = epoch\n",
    "        self.seed = random_state\n",
    "        self.verbose = verbose\n",
    "        self.metrics = metrics\n",
    "        self.layers = len(self.weight_set_dimensions) + 1\n",
    "        self.EPSILON = 10e-10\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        parameters = (\n",
    "            \"Input layer: {0}\\n\"\n",
    "            \"Hidden layer: {1}\\n\"\n",
    "            \"Output layer: {2}\\n\"\n",
    "            \"Batch size: {3}\\n\"\n",
    "            \"Learning rate: {4}\\n\"\n",
    "            \"Epoch: {5}\\n\"\n",
    "            \"Seed: {6}\\n\"\n",
    "            \"Verbose: {7}\\n\"\n",
    "            \"Metric: {8}\"\n",
    "        ).format(\n",
    "            self.input_layer,\n",
    "            \" - \".join(map(str, self.hidden_layer)),\n",
    "            self.output_layer,\n",
    "            self.mini_batch_size,\n",
    "            self.alpha,\n",
    "            self.epoch,\n",
    "            self.seed,\n",
    "            self.verbose,\n",
    "            self.metrics\n",
    "        )\n",
    "        return parameters\n",
    "\n",
    "    def get_A(self, X):\n",
    "        A, _ = self.forwardpass(X)\n",
    "        return A\n",
    "\n",
    "    def get_Z(self, X):\n",
    "        _, Z = self.forwardpass(X)\n",
    "        return Z\n",
    "    \n",
    "    # ============== LOSS FUNCTIONS ===============#\n",
    "\n",
    "    # https://deepnotes.io/softmax-crossentropy\n",
    "\n",
    "    def calculate_error(self, Y, Y_hat):\n",
    "        # Y and Y_hat should be in the form of (no_of_classes, no_of_training_examples)\n",
    "        cost = -np.sum(Y * np.log(Y_hat + EPSILON)) / self.m\n",
    "        penalise_by = 0\n",
    "        if self.penalty == \"l1\":\n",
    "            for layer in range(1, self.layers):\n",
    "                penalise_by += np.sum(np.abs(self.W[layer])) * self.lambd / (2 * self.m)\n",
    "            return cost + penalise_by\n",
    "        elif self.penalty == \"l2\":\n",
    "            for layer in range(1, self.layers):\n",
    "                penalise_by += np.sum(np.square(self.W[layer])) * self.lambd / (2 * self.m)\n",
    "            return cost + penalise_by\n",
    "        else:\n",
    "            return cost\n",
    "\n",
    "\n",
    "    def display_information(self, X, Y, epoch_no):\n",
    "        model_performance_metrics = calculate_model_performance(\n",
    "            np.argmax(Y, axis=0),\n",
    "            self.predict(X)\n",
    "        )\n",
    "        print(\"%s: %.10f - epoch %s    iteration %s - loss %.20f\" % (\n",
    "            self.metrics,\n",
    "            model_performance_metrics[self.metrics],\n",
    "            epoch_no,\n",
    "            self.no_of_iterations,\n",
    "            self.calculate_error(Y,\n",
    "                            self.get_A(X)[-1])\n",
    "        )\n",
    "              )\n",
    "\n",
    "    def get_dimensions_and_activations(self):\n",
    "        self.dimensions = []\n",
    "        self.activation_functions = []\n",
    "\n",
    "        self.dimensions.append(self.input_layer[0])\n",
    "        self.activation_functions.append(self.input_layer[1])\n",
    "\n",
    "        for dim, act_func in self.hidden_layer:\n",
    "            self.dimensions.append(dim)\n",
    "            self.activation_functions.append(act_func)\n",
    "\n",
    "        self.dimensions.append(self.output_layer)\n",
    "\n",
    "    @property\n",
    "    def weight_set_dimensions(self):\n",
    "        self.get_dimensions_and_activations()\n",
    "        a, b = itertools.tee(self.dimensions[::-1])\n",
    "        next(b, None)\n",
    "        weight_set_dimensions = list(zip(a, b))[::-1]\n",
    "        return weight_set_dimensions\n",
    "\n",
    "    def initialise_weights(self, layer=None):\n",
    "        self.W = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.B = np.empty_like(range(self.layers), dtype=object)\n",
    "        for layer, (y, x) in zip(range(1, self.layers), self.weight_set_dimensions):\n",
    "            np.random.seed(self.seed)\n",
    "            self.W[layer] = np.random.rand(y, x) / np.sqrt(self.dimensions[layer - 1])\n",
    "            self.B[layer] = np.random.rand(y, 1)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        Z = np.empty_like(range(self.layers), dtype=object)\n",
    "        A = np.empty_like(range(self.layers), dtype=object)\n",
    "        A[0] = X\n",
    "\n",
    "        for layer in range(1, self.layers):\n",
    "            # activation_function starts from 0 whereas layer starts from 1\n",
    "            active_function = self.activation_functions[layer - 1]\n",
    "            arg_to_pass_to_eval = \"(Z[layer])\"\n",
    "\n",
    "            Z[layer] = self.W[layer] @ A[layer - 1] + self.B[layer]\n",
    "            A[layer] = eval(active_function + arg_to_pass_to_eval)\n",
    "            \n",
    "            if self.dropout is True:\n",
    "                print(\"here\")\n",
    "                D = np.random.randn(A[layer].shape[0], A[layer].shape[1])\n",
    "                D = (D < self.keep_prob)\n",
    "                A[layer] = np.multiply(A[layer], D) / self.keep_prob\n",
    "        return A, Z\n",
    "\n",
    "    def backpropagation(self, Y, A, Z):\n",
    "        self.delta = np.empty_like(range(self.layers), dtype=object)\n",
    "\n",
    "        self.gradient_W = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.gradient_B = np.empty_like(range(self.layers), dtype=object)\n",
    "\n",
    "        self.delta[-1] = A[-1] - Y\n",
    "\n",
    "        # We substract 1 here as delta_final is calculated seperately above\n",
    "        for layer in reversed(range(1, self.layers - 1)):\n",
    "            # 1 is substracted from layer as activation_functions start indexing from 0\n",
    "            active_function = self.activation_functions[layer - 1]\n",
    "            arg_to_pass_to_eval = \"(Z[layer], prime=True)\"\n",
    "\n",
    "            self.delta[layer] = (\n",
    "                    self.W[layer + 1].T @ self.delta[layer + 1] *\n",
    "                    eval(active_function + arg_to_pass_to_eval)\n",
    "            )\n",
    "\n",
    "        for layer in range(1, self.layers):\n",
    "            self.gradient_W[layer] = (self.delta[layer] @ A[layer - 1].T) / self.m\n",
    "            self.gradient_B[layer] = np.sum(self.delta[layer], axis=1, keepdims=True) / self.m\n",
    "            \n",
    "            if self.penalty == \"l1\":\n",
    "                # https://towardsdatascience.com/only-numpy-implementing-different-combination-of-l1-norm-l2-norm-l1-regularization-and-14b01a9773b\n",
    "                self.gradient_W[layer] += np.where(self.W[layer] < 0, -1, 1) * (self.lambd / self.m)\n",
    "            elif self.penalty == \"l2\":\n",
    "                self.gradient_W[layer] += self.W[layer] * (self.lambd / self.m)\n",
    "            \n",
    "        self.update_weights()\n",
    "\n",
    "\n",
    "    def update_weights(self):\n",
    "        if self.optimizer == \"SGD\":\n",
    "            for layer in range(1, self.layers):\n",
    "                self.W[layer] -= self.alpha * self.gradient_W[layer]\n",
    "                self.B[layer] -= self.alpha * self.gradient_B[layer]\n",
    "\n",
    "        elif self.optimizer[\"method\"] == \"SGDM\":\n",
    "            for layer in range(1, self.layers):\n",
    "                beta = self.optimizer[\"beta\"]\n",
    "                self.v_dw[layer] = beta * self.v_dw[layer] + (1 - beta) * self.gradient_W[layer]\n",
    "                self.v_db[layer] = beta * self.v_db[layer] + (1 - beta) * self.gradient_B[layer]\n",
    "\n",
    "                self.W[layer] -= self.alpha * self.v_dw[layer]\n",
    "                self.B[layer] -= self.alpha * self.v_db[layer]\n",
    "\n",
    "        elif self.optimizer[\"method\"] == \"RMSP\":\n",
    "            for layer in range(1, self.layers):\n",
    "                beta = self.optimizer[\"beta\"]\n",
    "                self.s_dw[layer] = beta * self.s_dw[layer] + (1 - beta) * np.square(self.gradient_W[layer])\n",
    "                self.s_db[layer] = beta * self.s_db[layer] + (1 - beta) * np.square(self.gradient_B[layer])\n",
    "\n",
    "                w_rms_grad = self.gradient_W[layer] / (np.sqrt(self.s_dw[layer]) + self.EPSILON)\n",
    "                b_rms_grad = self.gradient_B[layer] / (np.sqrt(self.s_db[layer]) + self.EPSILON)\n",
    "\n",
    "                self.W[layer] -= self.alpha * w_rms_grad\n",
    "                self.B[layer] -= self.alpha * b_rms_grad\n",
    "\n",
    "        elif self.optimizer[\"method\"] == \"ADAM\":\n",
    "            # EWA: Exponential weighted average\n",
    "            # ToDo: Check if bias correction is necessary. The EWA will be inaccurate initially,\n",
    "            # but it shouldn't take many iterations to compute correct EWA.\n",
    "            for layer in range(1, self.layers):\n",
    "                beta1 = self.optimizer[\"beta1\"]\n",
    "                beta2 = self.optimizer[\"beta2\"]\n",
    "                self.v_dw[layer] = beta1 * self.v_dw[layer] + (1 - beta1) * self.gradient_W[layer]\n",
    "                self.v_db[layer] = beta1 * self.v_db[layer] + (1 - beta1) * self.gradient_B[layer]\n",
    "\n",
    "                self.s_dw[layer] = beta2 * self.s_dw[layer] + (1 - beta2) * np.square(self.gradient_W[layer])\n",
    "                self.s_db[layer] = beta2 * self.s_db[layer] + (1 - beta2) * np.square(self.gradient_B[layer])\n",
    "\n",
    "                v_dw_corrected = self.v_dw[layer] / (1 - beta1 ** self.no_of_iterations)\n",
    "                s_dw_corrected = self.s_dw[layer] / (1 - beta2 ** self.no_of_iterations)\n",
    "\n",
    "                v_db_corrected = self.v_db[layer] / (1 - beta1 ** self.no_of_iterations)\n",
    "                s_db_corrected = self.s_db[layer] / (1 - beta2 ** self.no_of_iterations)\n",
    "\n",
    "                self.W[layer] -= self.alpha * (v_dw_corrected / (np.sqrt(s_dw_corrected) + self.EPSILON))\n",
    "                self.B[layer] -= self.alpha * (v_db_corrected / (np.sqrt(s_db_corrected) + self.EPSILON))\n",
    "\n",
    "\n",
    "    def initialise_cache(self):\n",
    "        self.v_dw = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.v_db = np.empty_like(range(self.layers), dtype=object)\n",
    "    \n",
    "        self.s_dw = np.empty_like(range(self.layers), dtype=object)\n",
    "        self.s_db = np.empty_like(range(self.layers), dtype=object)\n",
    "\n",
    "        for layer, (y, x) in zip(range(1, self.layers), self.weight_set_dimensions):\n",
    "            self.v_dw[layer] = np.zeros((y, x))\n",
    "            self.v_db[layer] = np.zeros((y, 1))\n",
    "            \n",
    "            self.s_dw[layer] = np.zeros((y, x))\n",
    "            self.s_db[layer] = np.zeros((y, 1))\n",
    "\n",
    "\n",
    "    @timeit\n",
    "    def fit(self, X, Y):\n",
    "        self.m = X.shape[1] # where (no_of_features, no_of_training_examples)\n",
    "        self.initialise_weights()\n",
    "        self.initialise_cache()\n",
    "\n",
    "        # By default the method is SGD(Stochastic Gradient Descent) if one wishes to use\n",
    "        # the whole batch, simply pass the number of traning examples available as the\n",
    "        # batch size when instantiating the class\n",
    "        self.no_of_iterations = 0\n",
    "        shuffled = np.arange(self.m)\n",
    "        if self.verbose:\n",
    "            print(\"Initialising weights...\")\n",
    "            print(\"Starting the training...\")\n",
    "            print(\"Initial cost: %.10f\\n\" % self.calculate_error(Y, self.get_A(X)[-1]))\n",
    "        for epoch_no in range(1, self.epoch + 1):\n",
    "            np.random.shuffle(shuffled)\n",
    "            X_shuffled = X[:, shuffled]\n",
    "            Y_shuffled = Y[:, shuffled]\n",
    "            for i in range(0, self.m, self.mini_batch_size):\n",
    "                self.no_of_iterations += 1\n",
    "                X_mini_batch = X_shuffled[:, i: i + self.mini_batch_size]\n",
    "                Y_mini_batch = Y_shuffled[:, i: i + self.mini_batch_size]\n",
    "\n",
    "                A, Z = self.forwardpass(X_mini_batch)\n",
    "                self.backpropagation(Y_mini_batch, A, Z)\n",
    "                if self.no_of_iterations % 100 == 0 and self.verbose:\n",
    "                    self.display_information(X, Y, epoch_no)\n",
    "\n",
    "    def predict(\n",
    "            self,\n",
    "            X: np.ndarray,\n",
    "            return_prob_matrix=False\n",
    "    ):\n",
    "        \"\"\"Predict the output given the training data.\n",
    "\n",
    "            Returns the predicted values in two forms:\n",
    "\n",
    "            1.either by picking up the highest value along the columns for every row,\n",
    "                i.e. \"np.argmax(self.A[-1].T, axis=1)\"\n",
    "            2.or by returning a matrix that is in the shape of Y.T where each column\n",
    "                represents the probability of the instance belonging to that class.\n",
    "                Please note that every column in Y.T represents a class. To be able to\n",
    "                return the probability matrix, the final activation function must be\n",
    "                softmax!\n",
    "                i.e. \"array([0.9650488423, 0.0354737543, 0.0005225966])\"\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Training set in the shape of\n",
    "                (no_of_features, no_of_training examples).\n",
    "            return_prob_matrix (bool, optional): Returns the probability matrix if True.\n",
    "                Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray:\n",
    "\n",
    "            if return_prob_matrix is False, the output is in the shape of\n",
    "                (no_of_training_examples, 1)\n",
    "            if return_prob_matrix is True, the output is in the shape of\n",
    "                (no_of_training_examples, no_of_features)\n",
    "        \"\"\"\n",
    "        A, Z = self.forwardpass(X)\n",
    "        if return_prob_matrix:\n",
    "            np.set_printoptions(precision=10, suppress=True)\n",
    "            return A[-1].T\n",
    "        return np.argmax(A[-1].T, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with benchmark datasets\n",
    "\n",
    "## 1.Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "x = data.data[:,[0,2]]\n",
    "y = data.target\n",
    "\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 150)\n",
      "(3, 150)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising weights...\n",
      "Starting the training...\n",
      "Initial cost: 2.1722547973\n",
      "\n",
      "accuracy: 75.3333333328 - epoch 34    iteration 100 - loss 0.52207993300976174300\n",
      "accuracy: 93.9999999994 - epoch 67    iteration 200 - loss 0.23762698771784168672\n",
      "accuracy: 95.9999999994 - epoch 100    iteration 300 - loss 0.13258480561607385861\n",
      "accuracy: 95.9999999994 - epoch 134    iteration 400 - loss 0.11424061213710878793\n",
      "accuracy: 95.9999999994 - epoch 167    iteration 500 - loss 0.11451978669556735213\n",
      "accuracy: 95.9999999994 - epoch 200    iteration 600 - loss 0.10058079201569422589\n",
      "accuracy: 95.9999999994 - epoch 234    iteration 700 - loss 0.10516579997153779891\n",
      "accuracy: 95.9999999994 - epoch 267    iteration 800 - loss 0.09457311123667304364\n",
      "accuracy: 95.9999999994 - epoch 300    iteration 900 - loss 0.09244557838540501393\n",
      "accuracy: 95.9999999994 - epoch 334    iteration 1000 - loss 0.09129316663983459701\n",
      "accuracy: 96.6666666660 - epoch 367    iteration 1100 - loss 0.08790719854846917869\n",
      "accuracy: 95.3333333327 - epoch 400    iteration 1200 - loss 0.10335897528990094230\n",
      "accuracy: 96.6666666660 - epoch 434    iteration 1300 - loss 0.10007886490798169332\n",
      "accuracy: 96.6666666660 - epoch 467    iteration 1400 - loss 0.08497867208590581534\n",
      "accuracy: 96.6666666660 - epoch 500    iteration 1500 - loss 0.09103832437568681879\n",
      "accuracy: 97.3333333327 - epoch 534    iteration 1600 - loss 0.08411021425498003967\n",
      "accuracy: 96.6666666660 - epoch 567    iteration 1700 - loss 0.08690835892474636781\n",
      "accuracy: 95.9999999994 - epoch 600    iteration 1800 - loss 0.08293924700501584257\n",
      "accuracy: 97.3333333327 - epoch 634    iteration 1900 - loss 0.09614495608675674432\n",
      "accuracy: 96.6666666660 - epoch 667    iteration 2000 - loss 0.08799495232186582694\n",
      "accuracy: 97.3333333327 - epoch 700    iteration 2100 - loss 0.08237244653596144384\n",
      "accuracy: 95.9999999994 - epoch 734    iteration 2200 - loss 0.08266381892732506398\n",
      "accuracy: 95.9999999994 - epoch 767    iteration 2300 - loss 0.08219430820658775450\n",
      "accuracy: 97.3333333327 - epoch 800    iteration 2400 - loss 0.08082362209204863035\n",
      "accuracy: 95.9999999994 - epoch 834    iteration 2500 - loss 0.08068402249157935469\n",
      "accuracy: 97.3333333327 - epoch 867    iteration 2600 - loss 0.08082206690032935070\n",
      "accuracy: 96.6666666660 - epoch 900    iteration 2700 - loss 0.08407382178300801256\n",
      "accuracy: 95.3333333327 - epoch 934    iteration 2800 - loss 0.09866442502537997594\n",
      "accuracy: 95.9999999994 - epoch 967    iteration 2900 - loss 0.08755695985777833812\n",
      "accuracy: 97.3333333327 - epoch 1000    iteration 3000 - loss 0.08056234591559376101\n",
      "accuracy: 96.6666666660 - epoch 1034    iteration 3100 - loss 0.08171682191662918426\n",
      "accuracy: 95.9999999994 - epoch 1067    iteration 3200 - loss 0.08122337362572509822\n",
      "accuracy: 95.9999999994 - epoch 1100    iteration 3300 - loss 0.08599367545876174246\n",
      "accuracy: 97.3333333327 - epoch 1134    iteration 3400 - loss 0.07986673224248209968\n",
      "accuracy: 97.3333333327 - epoch 1167    iteration 3500 - loss 0.08022509325274841674\n",
      "accuracy: 96.6666666660 - epoch 1200    iteration 3600 - loss 0.08144458280334869460\n",
      "accuracy: 96.6666666660 - epoch 1234    iteration 3700 - loss 0.08104181196251465658\n",
      "accuracy: 95.9999999994 - epoch 1267    iteration 3800 - loss 0.08442166566933596850\n",
      "accuracy: 95.9999999994 - epoch 1300    iteration 3900 - loss 0.08108415655718359538\n",
      "accuracy: 96.6666666660 - epoch 1334    iteration 4000 - loss 0.07990476912832376877\n",
      "accuracy: 96.6666666660 - epoch 1367    iteration 4100 - loss 0.08015871277371461323\n",
      "accuracy: 95.9999999994 - epoch 1400    iteration 4200 - loss 0.08636279616136043058\n",
      "accuracy: 95.9999999994 - epoch 1434    iteration 4300 - loss 0.08066243827052689552\n",
      "accuracy: 97.3333333327 - epoch 1467    iteration 4400 - loss 0.07980826084627860861\n",
      "accuracy: 96.6666666660 - epoch 1500    iteration 4500 - loss 0.07988103382709148270\n",
      "accuracy: 97.3333333327 - epoch 1534    iteration 4600 - loss 0.07988635285682263443\n",
      "accuracy: 95.9999999994 - epoch 1567    iteration 4700 - loss 0.08063953292467315737\n",
      "accuracy: 96.6666666660 - epoch 1600    iteration 4800 - loss 0.08025759994191944413\n",
      "accuracy: 96.6666666660 - epoch 1634    iteration 4900 - loss 0.08158328350204567536\n",
      "accuracy: 95.9999999994 - epoch 1667    iteration 5000 - loss 0.08424983718042666769\n",
      "accuracy: 96.6666666660 - epoch 1700    iteration 5100 - loss 0.08032005776819092779\n",
      "accuracy: 96.6666666660 - epoch 1734    iteration 5200 - loss 0.08332642289603496566\n",
      "accuracy: 96.6666666660 - epoch 1767    iteration 5300 - loss 0.08216350130994509726\n",
      "accuracy: 96.6666666660 - epoch 1800    iteration 5400 - loss 0.07962143680371776278\n",
      "accuracy: 96.6666666660 - epoch 1834    iteration 5500 - loss 0.08059879250969283204\n",
      "accuracy: 96.6666666660 - epoch 1867    iteration 5600 - loss 0.08084747352521638730\n",
      "accuracy: 97.3333333327 - epoch 1900    iteration 5700 - loss 0.07987228405011875809\n",
      "accuracy: 95.3333333327 - epoch 1934    iteration 5800 - loss 0.09508503245831827211\n",
      "accuracy: 97.3333333327 - epoch 1967    iteration 5900 - loss 0.07960975953326579480\n",
      "accuracy: 95.9999999994 - epoch 2000    iteration 6000 - loss 0.08624122668617092979\n",
      "accuracy: 97.3333333327 - epoch 2034    iteration 6100 - loss 0.07983032855560753815\n",
      "accuracy: 95.9999999994 - epoch 2067    iteration 6200 - loss 0.08114696867199278074\n",
      "accuracy: 96.6666666660 - epoch 2100    iteration 6300 - loss 0.07962237646112053924\n",
      "accuracy: 96.6666666660 - epoch 2134    iteration 6400 - loss 0.08008142199117304627\n",
      "accuracy: 97.3333333327 - epoch 2167    iteration 6500 - loss 0.07979125396903737066\n",
      "accuracy: 96.6666666660 - epoch 2200    iteration 6600 - loss 0.08260944133921555022\n",
      "accuracy: 96.6666666660 - epoch 2234    iteration 6700 - loss 0.07967835175368614964\n",
      "accuracy: 96.6666666660 - epoch 2267    iteration 6800 - loss 0.08189714861603965279\n",
      "accuracy: 96.6666666660 - epoch 2300    iteration 6900 - loss 0.08117063822647641036\n",
      "accuracy: 96.6666666660 - epoch 2334    iteration 7000 - loss 0.07970817864499121774\n",
      "accuracy: 97.3333333327 - epoch 2367    iteration 7100 - loss 0.07962459883565654872\n",
      "accuracy: 96.6666666660 - epoch 2400    iteration 7200 - loss 0.08319543150298697176\n",
      "accuracy: 96.6666666660 - epoch 2434    iteration 7300 - loss 0.07964136815545765080\n",
      "accuracy: 96.6666666660 - epoch 2467    iteration 7400 - loss 0.07999487378559209938\n",
      "accuracy: 96.6666666660 - epoch 2500    iteration 7500 - loss 0.07964299674385381944\n",
      "accuracy: 97.3333333327 - epoch 2534    iteration 7600 - loss 0.07961264921075342227\n",
      "accuracy: 96.6666666660 - epoch 2567    iteration 7700 - loss 0.08038793375607412861\n",
      "accuracy: 96.6666666660 - epoch 2600    iteration 7800 - loss 0.07988186697118541957\n",
      "accuracy: 95.9999999994 - epoch 2634    iteration 7900 - loss 0.08106204951204844245\n",
      "accuracy: 95.9999999994 - epoch 2667    iteration 8000 - loss 0.08420332411247280491\n",
      "accuracy: 96.6666666660 - epoch 2700    iteration 8100 - loss 0.07995574998658624977\n",
      "accuracy: 95.9999999994 - epoch 2734    iteration 8200 - loss 0.08048515973770391241\n",
      "accuracy: 96.6666666660 - epoch 2767    iteration 8300 - loss 0.08686337384816660012\n",
      "accuracy: 95.3333333327 - epoch 2800    iteration 8400 - loss 0.09068468007862816316\n",
      "accuracy: 95.9999999994 - epoch 2834    iteration 8500 - loss 0.08867021963851484667\n",
      "accuracy: 95.9999999994 - epoch 2867    iteration 8600 - loss 0.08052302715050665971\n",
      "accuracy: 96.6666666660 - epoch 2900    iteration 8700 - loss 0.08117855853353107831\n",
      "accuracy: 97.3333333327 - epoch 2934    iteration 8800 - loss 0.07964958921724288321\n",
      "accuracy: 95.9999999994 - epoch 2967    iteration 8900 - loss 0.08048688054911125322\n",
      "accuracy: 95.9999999994 - epoch 3000    iteration 9000 - loss 0.08043277279894422982\n",
      "accuracy: 96.6666666660 - epoch 3034    iteration 9100 - loss 0.08522315337779605426\n",
      "accuracy: 96.6666666660 - epoch 3067    iteration 9200 - loss 0.08187704501209314145\n",
      "accuracy: 96.6666666660 - epoch 3100    iteration 9300 - loss 0.07980336983258944084\n",
      "accuracy: 95.9999999994 - epoch 3134    iteration 9400 - loss 0.08869859132227547738\n",
      "accuracy: 97.3333333327 - epoch 3167    iteration 9500 - loss 0.07963716573162687995\n",
      "accuracy: 95.9999999994 - epoch 3200    iteration 9600 - loss 0.08652993541484363393\n",
      "accuracy: 96.6666666660 - epoch 3234    iteration 9700 - loss 0.07967196155507891286\n",
      "accuracy: 96.6666666660 - epoch 3267    iteration 9800 - loss 0.08185072138691364130\n",
      "accuracy: 97.3333333327 - epoch 3300    iteration 9900 - loss 0.07961434083164076192\n",
      "accuracy: 96.6666666660 - epoch 3334    iteration 10000 - loss 0.08017498919365489973\n",
      "accuracy: 96.6666666660 - epoch 3367    iteration 10100 - loss 0.08194693147008678957\n",
      "accuracy: 96.6666666660 - epoch 3400    iteration 10200 - loss 0.08052374923643185189\n",
      "accuracy: 96.6666666660 - epoch 3434    iteration 10300 - loss 0.07967768372352429518\n",
      "accuracy: 96.6666666660 - epoch 3467    iteration 10400 - loss 0.08190517148370267397\n",
      "accuracy: 96.6666666660 - epoch 3500    iteration 10500 - loss 0.08082822557319596646\n",
      "accuracy: 96.6666666660 - epoch 3534    iteration 10600 - loss 0.07959091886531015347\n",
      "accuracy: 95.9999999994 - epoch 3567    iteration 10700 - loss 0.08113278404526788612\n",
      "accuracy: 97.3333333327 - epoch 3600    iteration 10800 - loss 0.08010329968972741688\n",
      "accuracy: 97.3333333327 - epoch 3634    iteration 10900 - loss 0.08008033785776894065\n",
      "accuracy: 96.6666666660 - epoch 3667    iteration 11000 - loss 0.08008515905115758116\n",
      "accuracy: 97.3333333327 - epoch 3700    iteration 11100 - loss 0.08887596939204905899\n",
      "accuracy: 97.3333333327 - epoch 3734    iteration 11200 - loss 0.07974912926148788883\n",
      "accuracy: 95.9999999994 - epoch 3767    iteration 11300 - loss 0.08077584768859980002\n",
      "accuracy: 96.6666666660 - epoch 3800    iteration 11400 - loss 0.08077282802085104796\n",
      "accuracy: 95.9999999994 - epoch 3834    iteration 11500 - loss 0.08369220826787948653\n",
      "accuracy: 96.6666666660 - epoch 3867    iteration 11600 - loss 0.08142368136124984723\n",
      "accuracy: 96.6666666660 - epoch 3900    iteration 11700 - loss 0.07980720241263873638\n",
      "accuracy: 95.9999999994 - epoch 3934    iteration 11800 - loss 0.08083278951644348731\n",
      "accuracy: 96.6666666660 - epoch 3967    iteration 11900 - loss 0.08199442109405491452\n",
      "accuracy: 95.9999999994 - epoch 4000    iteration 12000 - loss 0.08054043308869733575\n",
      "func:'fit' -- took: 6.2295 sec\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(2, 'relu'),\n",
    "    hidden_layer=[(4,'relu'),(4,'softmax')],\n",
    "    output_layer=3,\n",
    "    batch_size=64,\n",
    "    optimizer={\"method\": \"ADAM\", \"beta1\": 0.9, \"beta2\": 0.999},\n",
    "    penalty=\"l2\",\n",
    "    lambd=0.0001,\n",
    "    epoch=4000,\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calculate_model_performance(np.argmax(Y, axis=0),\n",
    "                           model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dict_all_models, results_average_dict, models = grid_search(\n",
    "    x,\n",
    "    y,\n",
    "    clf=NeuralNetwork,\n",
    "    lst_metrics=[\"F1\", \"accuracy\"],\n",
    "    sort_by = \"accuracy\",\n",
    "    n_folds=5,\n",
    "    dict_param_grid={\n",
    "        'batch_size': [8, 16, 32],\n",
    "        'input_layer': [(2, 'relu')],\n",
    "        'hidden_layer': [\n",
    "            [(4,'relu'), (4,'softmax')],\n",
    "            [(4,'sigmoid'),(4,'softmax')]\n",
    "        ],\n",
    "        'optimizer': [\n",
    "            {\n",
    "                \"method\": \"RMSP\",\n",
    "                \"beta\": 0.9\n",
    "            }\n",
    "        ],\n",
    "        'output_layer': [3],\n",
    "        'alpha': [0.001],\n",
    "        'verbose': [False],\n",
    "        'epoch': [1000]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\ma\\core.py:6461: MaskedArrayFutureWarning: In the future the default for ma.maximum.reduce will be axis=0, not the current None, to match np.maximum.reduce. Explicitly pass 0 or None to silence this warning.\n",
      "  return self.reduce(a)\n",
      "C:\\Anaconda3\\lib\\site-packages\\numpy\\ma\\core.py:6461: MaskedArrayFutureWarning: In the future the default for ma.minimum.reduce will be axis=0, not the current None, to match np.minimum.reduce. Explicitly pass 0 or None to silence this warning.\n",
      "  return self.reduce(a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x177fa6f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsUAAAHpCAYAAABqTUHIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x135f0550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "dt = data.data[:,[0,2]]\n",
    "x_min, x_max = dt[:, 0].min() - 1, dt[:, 0].max() + 1\n",
    "y_min, y_max = dt[:, 1].min() - 1, dt[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()].T) \n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx, yy, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "\n",
    "plt.scatter(dt[:, 0], dt[:, 1], c=y,s=20, edgecolor='k')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('petal length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Make Moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "x,y =make_moons(n_samples=1500, noise=.05)\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(X.shape[0], 'relu'),\n",
    "    hidden_layer=[(10,'relu'), (4,'softmax')],\n",
    "    output_layer=Y.shape[0],\n",
    "    batch_size=16,\n",
    "    optimizer=\n",
    "    {\n",
    "        \"method\": \"ADAM\",\n",
    "        \"beta1\": 0.9,\n",
    "        \"beta2\": 0.999\n",
    "    },\n",
    "    penalty = \"l2\",\n",
    "    lambd=0.001,\n",
    "    epoch=100,\n",
    "    alpha=0.1\n",
    ")\n",
    "\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Decision Boundaries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "dt = x\n",
    "x_min, x_max = dt[:, 0].min() - 0.5, dt[:, 0].max() + 0.5\n",
    "y_min, y_max = dt[:, 1].min() - 0.5, dt[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()].T) \n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx, yy, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "plt.scatter(dt[:, 0], dt[:, 1], c=y, s=20, edgecolor='k')\n",
    "plt.title('Decision Boundaries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Andrew NG Assignment 2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex2data2 = np.loadtxt(\"../ex2/data/ex2data2.txt\", delimiter=\",\")\n",
    "\n",
    "x = ex2data2[:, :-1]\n",
    "y = ex2data2[:, -1]\n",
    "\n",
    "X = x.T\n",
    "Y = one_hot_encode(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    input_layer=(X.shape[0], 'relu'),\n",
    "    hidden_layer=[(10,'relu'), (4,'softmax')],\n",
    "    output_layer=Y.shape[0],\n",
    "    batch_size=8,\n",
    "    optimizer={\"method\": \"ADAM\", \"beta1\": 0.9, \"beta2\": 0.999},\n",
    "    penalty = \"l2\",\n",
    "    lambd=0.05,\n",
    "    epoch=1500,\n",
    "    alpha=0.001\n",
    ")\n",
    "\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dict_all_models, results_average_dict, models = grid_search_stratified(\n",
    "    x,\n",
    "    y,\n",
    "    clf=NeuralNetwork,\n",
    "    metrics=[\"F1\", \"accuracy\"],\n",
    "    sort_by = \"accuracy\",\n",
    "    n_fold=6,\n",
    "    param_grid_dict={\n",
    "        'batch_size': [16, 32],\n",
    "        'input_layer': [(2, 'relu')],\n",
    "        'hidden_layer': [\n",
    "            [(4,'relu'), (4,'relu'), (4,'softmax')],\n",
    "            [(4,'sigmoid'),(4,'softmax')]\n",
    "        ],\n",
    "        'output_layer': [2],\n",
    "        'alpha': [2, 4],\n",
    "        'verbose': [False],\n",
    "        'epoch': [5000]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_average_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(models[\"model_5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Decision Boundaries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "X = ex2data2\n",
    "\n",
    "x1_min, x1_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3,\n",
    "x2_min, x2_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3,\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = model.predict(np.c_[xx1.ravel(), xx2.ravel()].T) \n",
    "\n",
    "negatives = ex2data2[ex2data2[:, -1] == 0]\n",
    "positives = ex2data2[ex2data2[:, -1] == 1]\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx1.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx1, xx2, Z,alpha=0.4)\n",
    "#plt.axis('off')\n",
    "plt.scatter(negatives[:, 0], negatives[:, 1],s=50, color='k')\n",
    "plt.scatter(positives[:, 0], positives[:, 1],s=50, color='r')\n",
    "plt.title('Decision Boundaries')\n",
    "\n",
    "plt.contour(xx1, xx2, Z, [0.5], linewidths=2, colors=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "data = loadmat('../ex3/data/ex3data1.mat')\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = data[\"X\"]\n",
    "y = data[\"y\"]\n",
    "y[y==10] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_test, dataset_train = split_data_as(x, y, train=0.9, test=0.1)\n",
    "X_train = dataset_train[:, :-1].T\n",
    "Y_train = one_hot_encode(dataset_train[:, -1]).T\n",
    "\n",
    "X_test = dataset_test[:, :-1].T\n",
    "Y_test = one_hot_encode(dataset_test[:, -1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = x.T\n",
    "# Y = one_hot_encode(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "sample = np.random.choice(data[\"X\"].shape[0], 20)\n",
    "ax.imshow(data[\"X\"][sample,1:].reshape(-1,20).T)\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354\n",
    "\n",
    "model = NeuralNetwork(\n",
    "    input_layer=(X_train.shape[0], 'relu'),\n",
    "    hidden_layer=[(200,'relu'),(100,'relu'),(4,'softmax')],\n",
    "    output_layer=Y_train.shape[0],\n",
    "    batch_size=64,\n",
    "    optimizer={\"method\": \"ADAM\", \"beta1\": 0.9, \"beta2\": 0.999},\n",
    "    penalty = \"l2\",\n",
    "    lambd=0.1,\n",
    "    epoch=500,\n",
    "    alpha=0.001\n",
    ")\n",
    "\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calculate_model_performance(np.argmax(Y_test, axis=0), model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    (X.shape[0], 'relu'),\n",
    "    [(200, 'relu'), (100, 'relu'), (50, 'relu'), (10,'softmax')],\n",
    "    Y.shape[0],\n",
    "    batch_size=50,\n",
    "    optimizer={\n",
    "        \"method\": \"RMSP\",\n",
    "        \"beta\": 0.9\n",
    "                },\n",
    "    epoch=100,\n",
    "    alpha=0.001\n",
    ")\n",
    "\n",
    "model.fit(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    (X.shape[0], 'relu'),\n",
    "    [(25,'relu'), (4,'softmax')],\n",
    "    Y.shape[0],\n",
    "    batch_size=50,\n",
    "    optimizer={\n",
    "        \"method\": \"SGDM\",\n",
    "        \"beta\": 0.9\n",
    "                },\n",
    "    epoch=1000,\n",
    "    alpha=6)\n",
    "\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dict_all_models, results_average_dict, models = grid_search(\n",
    "    x,\n",
    "    y,\n",
    "    clf=NeuralNetwork,\n",
    "    lst_metrics=[\"F1\", \"accuracy\"],\n",
    "    sort_by = \"F1\",\n",
    "    n_folds=10,\n",
    "    dict_param_grid={\n",
    "        'batch_size': [64, 128, 256],\n",
    "        'input_layer': [(x.shape[1], 'relu')],\n",
    "        'hidden_layer': [\n",
    "            [(50, 'relu'), (25, 'relu'), (10,'softmax')],\n",
    "            [(200, 'relu'), (100, 'relu'), (50, 'relu'), (10,'softmax')]\n",
    "        ],\n",
    "        'optimizer':[\n",
    "            {\n",
    "                \"method\": \"ADAM\",\n",
    "                \"beta1\": 0.9,\n",
    "                \"beta2\": 0.999\n",
    "            }\n",
    "        ],\n",
    "        'output_layer': [10],\n",
    "        'alpha': [0.0001, 0.001],\n",
    "        'verbose': [False],\n",
    "        'epoch': [250]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_average_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(models[\"model_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_miss_clasifications(model, digits_to_display):\n",
    "    count = 0\n",
    "    for index, (act, predicted) in enumerate(zip(np.argmax(Y,axis=0), model.predict(X))):\n",
    "        if act != predicted:\n",
    "            fig, ax = plt.subplots(figsize = (2,2))\n",
    "            ax.set_title(\"%s: act %s --- predicted %s\" %(index, act, predicted))\n",
    "            ax.imshow(X[:, index].reshape(-1,20).T)\n",
    "            ax.axis('off');\n",
    "            count += 1\n",
    "        if count == digits_to_display:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_miss_clasifications(model, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
